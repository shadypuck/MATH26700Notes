\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{2}

\begin{document}




\chapter{Characters}
\section{Defining Characters}
\begin{itemize}
    \item \marginnote{10/9:}Today, we talk about \textbf{characters}, arguably the most important idea in rep theory.
    \item As per usual, we begin by letting $G$ a finite group.
    \begin{itemize}
        \item We've been discussing finite dimensional representations of $G$ over $\C$.
        \item We've also already talked about irreps, and we know that it's enough to understand those because every rep is a sum of them.
    \end{itemize}
    \item Goal of characters: Understand the irreps $V_1,\dots,V_k$ of $G$.
    \begin{itemize}
        \item Recall the surprising fact about $k$: It is the number of conjugacy classes of $G$!
        \begin{itemize}
            \item We haven't yet proven this, but we will soon!
        \end{itemize}
        \item Game plan: Use characters to relate irreps to something that is counted by conjugacy classes.
    \end{itemize}
    \item Let $V=\C e_1\oplus\cdots\oplus\C e_n$ be a $G$-rep.
    \begin{itemize}
        \item Then there exists a homomorphism $\rho:g\mapsto A_g\in GL_n(\C)$.
    \end{itemize}
    \item Motivating question: What doesn't change when we change the basis of $V$?
    \begin{itemize}
        \item To isolate the "essence" of the $A_g$, we want to construct a function $f:GL_n(\C)\to\C$ such that $f(XAX^{-1})=f(A)$, where $X\in GL_n(\C)$.
    \end{itemize}
    \item Ideas.
    \begin{enumerate}
        \item The determinant is a great example of such a function, but it's kind of boring because this rank 1 representation doesn't characterize your product representation.
        \item Trace is the main example of such a function.
    \end{enumerate}
    \item Indeed, you can also take $\tr(A^k)$ for any $k$.
    \begin{itemize}
        \item Traces of powers are ubiquitous in physics and math because they contain the same information as the coefficients of the characteristic polynomial. In particular, we can express the determinant in terms of them.
    \end{itemize}
    \item In fact, we could also take any coefficient of the characteristic polynomial, but others would get complicated.
    \begin{itemize}
        \item Any characteristic polynomial coefficient can be expressed in terms of traces; this will be an exercise in PSet 3; it's not hard.
        \item For example, the second characteristic polynomial coefficient (sum of products of eigenvalues) can be written as follows.
        \begin{equation*}
            \sum\lambda_i\lambda_j = \frac{\tr(A)^2-\tr(A^2)}{2}
        \end{equation*}
    \end{itemize}
    \item So what do we have at this point?
    \begin{itemize}
        \item We can associate to $\rho$ a function $\chi_\rho:G\to\C$ defined by $\chi_\rho(g)=\tr(A_g)=\tr(\rho(g))$.
        \item This function is invariant under isomorphism.
        \item If we know $\tr(A)$, we know $\tr(A^2)$ since $A_g^2=A_{g^2}$. Thus, if we know all traces, we know all power traces.
        \item We form a ring of polynomials?? Equivalently, $\chi_\rho$ has a representation as a polynomial with coefficients in $\C$?
    \end{itemize}
    \item If $V$ is a $G$-rep, $\chi_V:G\to\C$ will be our notation for its character.
    \item Properties.
    \begin{enumerate}
        \item $\chi_V(xgx^{-1})=\chi_V(g)$ for any $x,g\in G$.
        \begin{itemize}
            \item Implication: $\chi_V$ is a \textbf{class function}.
            \item Let $\C[G]$ be the vector space of all functions from $G\to\C$. Its $\dim=|G|$.
            \begin{itemize}
                \item Recall that this notation is the same as that for the space of polynomials in $g_1,\dots,g_n$ with complex coefficients.
                \item But since $G$ is a group, a polynomial in products of $g_i$'s can be reduced to a polynomial in the $g_i$'s.
                \begin{itemize}
                    \item For example, if $G=S_3$, we know since $(23)(123)^2=(12)$ that
                    \begin{align*}
                        2+3(12)+i(123)+(-3+7i)(23)(123)^2 &= 2+[3+(-3+7i)](12)+i(123)\\
                        &= 2e+7i(12)+i(123)
                    \end{align*}
                \end{itemize}
                \item Such a polynomial is then easily mapped onto a complex-valued function by sending each $g_i$ to its coefficient $a_{g_i}$.
                \begin{itemize}
                    \item Continuing with the above example, the corresponding function in $\C[G]$ would be defined by
                    \begin{align*}
                        e &\mapsto 2&
                            (12) &\mapsto 7i&
                                (123) &\mapsto i\\
                        &&
                            (13) &\mapsto 0&
                                (132) &\mapsto 0\\
                        &&
                            (23) &\mapsto 0
                    \end{align*}
                \end{itemize}
                \item Thus, $\C[G]$ (as a space of polynomials) is canonically isomorphic to $\C[G]$ (as a space of complex-valued functions on $G$), so the notation is well chosen.
                \item What do we do for multiplication?? Because functions multiply pointwise but polynomials do not. It appears that we typically go with polynomial multiplication (but not always, potentially; see the claim from Lecture 6.1). \href{https://math.stackexchange.com/questions/2423220/thinking-about-the-group-algebra-kg-as-functions-on-g}{MSE} also supports polynomial multiplication. Perhaps $\C[G]$ for this function space with pointwise multiplication is just really misleading notation?
            \end{itemize}
            \item Inside this space, there is the subspace $\C_\text{cl}[G]$ of functions $f:G\to\C$ such that $f(xgx^{-1})=f(g)$ for all $x,g\in G$. These are functions from the sets of conjugacy classes, isomorphic to functions that are constant on conjugacy classes. $\dim\C_\text{cl}[G]$ is the number of conjugacy classes.
            \item Thus, for every $V$ a $G$-rep, we get a vector $\chi_V\in\C_\text{cl}[G]$. These class functions form a basis of the space; each $\chi_V$ for $V$ an irrep forms a linearly independent vector; the set is an \emph{orthogonal} basis. This is the reason for the original theorem holding true!
        \end{itemize}
        \item $\chi_{V_1\oplus V_2}=\chi_{V_1}+\chi_{V_2}$.
        \begin{itemize}
            \item Proof: It's basically tautological (not actually, but it's easy). Let $g\in G$. Compute $\chi_{V_1\oplus V_2}(g)$. We can compute a basis $e_1,\dots,e_{n+m}$ where the first $n$ vectors form a basis of $V_1$, and the next $m$ vectors are a basis of $V_2$. This gives us a block matrix from which we show that the trace of the matrix is the sum of traces.
            \begin{equation*}
                \chi_{V_1\oplus V_2}(g) = \tr
                \begin{bmatrix}
                    \rho_{V_1}(g) & 0\\
                    0 & \rho_{V_2}(g)\\
                \end{bmatrix}
                = \tr\rho_{V_1}(g)+\tr\rho_{V_2}(g)
                = \chi_{V_1}(g)+\chi_{V_2}(g)
            \end{equation*}
            \item Corollary:
            \begin{equation*}
                \chi_{V_1^{n_1}\oplus\cdots\oplus V_k^{n_k}} = n_1\chi_{V_1}+\cdots+n_k\chi_{V_k}
            \end{equation*}
        \end{itemize}
    \end{enumerate}
    \item We now pause for a fact that will be instrumental in proving the next property, which is a bit more involved.
    \begin{itemize}
        \item He will explain two ways to prove it; we can also just prove it on our own.
    \end{itemize}
    \item Fact: If $A$ is a matrix such that $A^n=1$, then $A$ is diagonalizable or "semi-simple."
    \begin{itemize}
        \item We can prove this with Jordan normal form.
        \item It's a slightly surprising statement.
        \item Obviously eigenvalues are roots of unity, but still needs some work.
        \item This proof is left as an exercise.
    \end{itemize}
    \item We now resume the list of properties.
    \begin{enumerate}[resume]
        \item $\chi_V(g)$ is a sum of roots of unity.
        \begin{itemize}
            \item Proof: We know that $g^{|G|}=e$. Thus, $A_g^{|G|}=1$. It follows by the fact above that $A_g$ is diagonalizable with eigenvalues $\lambda_1,\dots,\lambda_n$, each of which satisfies $\lambda_i^{|G|}=1$.
            \begin{itemize}
                \item Note: Eigenvalues can repeat in the list $\lambda_1,\dots,\lambda_n$, i.e., we are not asserting $n$ distinct eigenvalues here.
            \end{itemize}
            \item Therefore, since each $\lambda_i$ is, individually, a root of unity, we have that $\chi_V(g)=\tr A_g=\lambda_1+\cdots+\lambda_n$, as desired.
        \end{itemize}
        \item $\chi_{V^*}=\bar{\chi}_V$.
        \begin{itemize}
            \item This property begins to address how characters behave under other operations.
            \begin{itemize}
                \item Naturally, this is something specific for complex numbers, because the idea of "conjugates" doesn't exist everywhere.
            \end{itemize}
            \item Proof: Recall that $\rho_{V^*}(g)=(\rho_V(g)^{-1})^T$.
            \begin{itemize}
                \item If we know that $\rho_V(g)\sim\diag(\lambda_1,\dots,\lambda_n)$, then we know that $\rho_V^{-1}(g)^T\sim\diag(\lambda_1^{-1},\dots,\lambda_n^{-1})$.
                \item Thus, $\chi_{V^*}(g)=\lambda_1^{-1}+\cdots+\lambda_n^{-1}$.
                \item But since we're in the complex plane, $|\lambda_i|=1$ (equiv. $\lambda_i\bar{\lambda}_i=1$), so $\lambda_i^{-1}=1/\lambda_i=\bar{\lambda}_i$.
                \item This means that $\chi_{V^*}(g)=\bar{\lambda}_1+\cdots+\bar{\lambda}_n=\overline{\lambda_1+\cdots+\lambda_n}=\bar{\chi}_V(g)$.
            \end{itemize}
            \item Note: Every representation we have is \textbf{unitary} in certain bases, but unitary representations are not covered in this course.
        \end{itemize}
        \item $\chi_{V_1\otimes V_2}=\chi_{V_1}\cdot\chi_{V_2}$.
        \begin{itemize}
            \item Proof: We can use a basis or not use a basis.
            \item Let's use a basis for now.
            \begin{itemize}
                \item Let $g\in G$ be arbitrary. Then there exist bases $e_1,\dots,e_n$ of $V_1$ and $f_1,\dots,f_m$ of $V_2$ such that $\rho_{V_1}(g)$ and $\rho_{V_2}(g)$ are diagonal.
                \item It follows that $\rho_{V_1}(g)e_i=\lambda_ie_i$ ($i=1,\dots,n$) and $\rho_{V_2}(g)f_i=\mu_if_i$ ($i=1,\dots,m$).
                \item $V_1\otimes V_2$ thus has basis $e_i\otimes f_j$.
                \item But then it follows that $\rho_{V_1\otimes V_2}(g)e_i\otimes f_j=(\lambda_ie_i)\otimes(\mu_jf_j)=\lambda_i\mu_j(e_i\otimes f_j)$.
                \item Thus,
                \begin{equation*}
                    \tr(\rho_{V_1\otimes V_2}(g)) = \sum_{i,j=1}^{n,m}\lambda_i\mu_j
                    = (\lambda_1+\cdots+\lambda_n)(\mu_1+\cdots+\mu_m)
                    = \tr(\rho_{V_1}(g))\cdot\tr(\rho_{V_2}(g))
                \end{equation*}
            \end{itemize}
            \item Alternate approach.
            \begin{itemize}
                \item If we don't want to think of eigenvalues, think of tensor product of matrices, the Kronecker product.
                \item Essentially, if we adopt a basis such that our matrices are diagonal, then the block diagonal of the Kronecker product will be $\lambda_1\rho_{V_2}(g)+\cdots+\lambda_n\rho_{V_2}(g)$, the trace of which will be $\lambda_1(\mu_1+\cdots+\mu_m)+\cdots+\lambda_n(\mu_1+\cdots+\mu_m)$.
                \item We get trace is the product of traces once again!
            \end{itemize}
        \end{itemize}
    \end{enumerate}
    \item \textbf{Class function}: A function on a group $G$ that is constant on the conjugacy classes of $G$.
    \item Examples.
    \begin{enumerate}
        \item Let $A$ be an abelian group.
        \begin{itemize}
            \item Then $\chi:A\to\C^\times$.
            \item Implication: Character of a character is $\chi_\chi=\chi$.
            \begin{itemize}
                \item This is horribly repetitive but true.
            \end{itemize}
        \end{itemize}
        \item $G=S_3$.
        \begin{table}[h!]
            \centering
            \small
            \renewcommand{\arraystretch}{1.4}
            \begin{tabular}{c|c|c|c|}
                 & $e$ & \renewcommand{\arraystretch}{1}\begin{tabular}{@{}c@{}}$(12)$\\$(13)$\\$(23)$\end{tabular} & \renewcommand{\arraystretch}{1}\begin{tabular}{@{}c@{}}$(123)$\\$(132)$\end{tabular}\\
                \hline
                Trivial & 1 & 1 & 1\\ \hline
                Alternating & 1 & $-1$ & 1\\ \hline
                Standard & 2 & 0 & $-1$\\ \hline
            \end{tabular}
            \caption{Character table for $S_3$.}
            \label{tab:charTableS3}
        \end{table}
        \begin{itemize}
            \item The conjugacy classes of this group are $\{e\}$, $\{(12),(13),(23)\}$, and $\{(123),(132)\}$.
            \item We construct a \textbf{character table} to define all characters.
            \item Computing the characters for the trivial representation.
            \begin{itemize}
                \item We know that $\rho$ sends each $g$ to the matrix $(1)$, which has trace 1.
            \end{itemize}
            \item Computing the characters for the sign representation.
            \begin{itemize}
                \item $e$ and $(123)$ have sign 1 and thus get sent to the matrix $(1)$.
                \item $(12)$ has sign $-1$ and thus gets sent to the matrix $(-1)$.
            \end{itemize}
            \item Computing the characters for the standard representation.
            \begin{itemize}
                \item We can compute these traces via a thought experiment.
                \item Visualize a triangle in a plane.
                \item The $2\times 2$ identity matrix (the standard representation of $e\in G$) acts on it by doing nothing, and has trace 2.
                \item In \emph{some} basis, our matrix fixes one vector and inverts another, so matrix is
                \begin{equation*}
                    \begin{pmatrix}
                        1 & 0\\
                        0 & -1\\
                    \end{pmatrix}
                \end{equation*}
                and character is 0.
                \item Last one is rotation by $2\pi/3$, so
                \begin{equation*}
                    \begin{pmatrix}
                        \cos(2\pi/3) & \sin(2\pi/3)\\
                        -\sin(2\pi/3) & \cos(2\pi/3)\\
                    \end{pmatrix}
                \end{equation*}
                so character is $-1=2\cdot -1/2=2\cdot\cos(2\pi/3)$.
            \end{itemize}
            \item If $V$ is the standard representation, we can also compute the characters of $V^{\otimes 2}$ for instance. Indeed, by the product rule of characters, they will be the squares of the standard representation's characters, i.e., $(4,0,1)$.
            \item Similarly, since the permutational representation is the direct sum of the standard and trivial representations, we can add their characters to get its characters $(3,1,0)$.
        \end{itemize}
        \item A very general and very pretty example. Let $G\acts X$ a finite set.
        \begin{itemize}
            \item Assign the permutational representation.
            \item Let $X=\{x_1,\dots,x_n\}$. Think of these elements as the basis of a vector space; in particular, consider $V=\C e_{x_1}\oplus\cdots\oplus\C e_{x_n}$. Recall that $g(a_1e_{x_1}+\cdots+a_ne_{x_n})=a_1e_{gx_1}+\cdots+a_ne_{gx_n}$. The fact that this is a representation follows immediately from the properties of the group action.
            \item Computing the character $\chi_V$ of this $V$: Look at $g$ and write its matrix. In particular, the trace is the number of unmoved/fixed elements, sometimes denoted $\Fix(g)$.
            \item This gives us another way of computing $V_\text{perm}$ from above!
        \end{itemize}
    \end{enumerate}
    \item \textbf{Character table}: A table that lists the conjugacy classes across the top, the irreps down the left side, and at each point within it, the value of an irrep's character over that conjugacy class.
    \begin{itemize}
        \item The character table is a very nice matrix with very nice properties.
        \item It is almost orthogonal; not exactly, but very close.
        \begin{itemize}
            \item Rows aren't orthogonal, but columns are (take direct products)!
            \item It is full rank, though.
        \end{itemize}
    \end{itemize}
    \item The midterm: Take the character table and do fun things with it.
\end{itemize}



\section{Office Hours}
\begin{itemize}
    \item \marginnote{10/10:}Problem 1b:
    \begin{itemize}
        \item Canonically self-dual: $V\cong V^*$ canonically.
    \end{itemize}
    \item Mathematical methods of quantum mechanics: First few paragraphs of \emph{picture}.
    \item We should have everything we need to do most of the problem set at this point; maybe not all of 5, but maybe yes, too.
    \item Problem 3:
    \begin{itemize}
        \item There is some problem where it decomposes into trivial plus standard, but we still have to prove that standard is irreducible in this case!
        \item If you have any vector, you can produce out of this vector something else.
        \item If we take any vector and the group acts on it, we'll get a basis. If you hit a vector in the invariant subspace, it will just stay there; if you hit it and it goes everywhere, you get a basis.
        \item Now think about a vector when you permute its coordinates.
        \item Tomorrow in class, we will learn a quick way to do this problem.
    \end{itemize}
    \item Problem 5:
    \begin{itemize}
        \item For some problem, we need to use the fact that $A^n=1$ proves that $A=I$ in some sense.
        \item This is a hard problem!
        \item Show that eigenvalues sum to 1; we know that the eigenvalues are roots of unity! Thus, they have to both be 1!
        \item When the problem in group theory is harder, that's when you need to go to rep theory.
    \end{itemize}
\end{itemize}



\section{Characters are Orthonormal}
\begin{itemize}
    \item \marginnote{10/11:}Announcement: Zoom OH today.
    \item Recap: The big picture.
    \begin{itemize}
        \item Representations.
        \begin{itemize}
            \item We have representations, which are vector spaces on which a group acts.
            \item With these representations, we can do a bunch of operations we've discussed: $\oplus,\otimes,V^*,\Lambda^n,S^n$.
            \item We'll focus on the first 3 for now, though.
        \end{itemize}
        \item Class functions.
        \begin{itemize}
            \item We also have class functions: Functions $f:G\to\C$ such that for all $g,x\in G$, $f(gxg^{-1})=f(x)$.
            \item The space of class functions forms a ring, since you can add, multiply, and take the complex conjugate of these functions.
            \item Moreover, this ring is a vector space and it has dimension equal to the number of conjugacy classes of $G$.
        \end{itemize}
        \item The big idea: These two things (representations and class functions) are closely related!
        \begin{itemize}
            \item There is a map, called a \emph{character}, that pairs a representation to a class function.
            \item Indeed, $V\to\chi_V$.
            \item Under this map, operations of representations become operations of functions:
            \begin{align*}
                \oplus &\mapsto +&
                \otimes &\mapsto \cdot&
                V^* &\mapsto \bar{f}
            \end{align*}
            \item Additionally, $V_1,\dots,V_s$ become $\chi_{V_1},\dots,\chi_{V_s}$.
        \end{itemize}
    \end{itemize}
    \item Theorem we will prove over the next couple of lectures: Irreps become \emph{linearly independent} class functions, and all irreps form a basis of the space of class functions.
    \begin{itemize}
        \item This theorem is huge! It is our main takeaway for now.
        \item For the first part of the course, this is the main thing that we should remember.
    \end{itemize}
    \item How do we prove that multiple vectors are linearly independent?
    \begin{itemize}
        \item A strong condition would be to introduce an inner product and prove that the pairwise inner product of the vectors is zero.
    \end{itemize}
    \item \textbf{Orthonormal basis}: A basis for which $\inp{e_i,e_j}=\delta_{ij}$.
    \item Let's begin carrying out this plan by defining an inner product on $\C[G]$. Indeed, let $f_1,f_2$ be two functions on $G$ and take
    \begin{equation*}
        \inp{f_1,f_2} := \frac{1}{|G|}\sum_{g\in G}f_1(g)\overline{f_2(g)}
    \end{equation*}
    \item Motivation for this definition.
    \begin{itemize}
        \item Recall the \textbf{Hermitian inner product} on $\C^n$.
        \begin{itemize}
            \item We are essentially mapping $f_1,f_2$ to $(f_1(g_1),\dots,f_1(g_{|G|})),(f_2(g_1),\dots,f_2(g_{|G|}))\in\C^{|G|}$ and taking the Hermitian inner product there.
            \item Thus, we can see that all properties hold for both the Hermitian inner product on $\C^n$ and the one defined above on $\C[G]$.
            \item In other words, this kind of construction should inherit its status as a linear, positive definite bilinear form from the Hermitian inner product.
        \end{itemize}
        \item Note: The Hermitian product above is \textbf{$\bm{G}$-invariant}.
        \begin{itemize}
            \item This means that the functions on $G$ from $G\to\C$ in $\C[G]$ form a representation of $G$.
            \item In particular, if $\varphi:G\to\C$, then $g=\rho(g)$ moves it as follows: $g\cdot\varphi=\varphi^g$ where $\varphi^g(h):=\varphi(g^{-1}h)$. Thus, we have an action of $G$ on every $\varphi$!
            \item Such representations are isomorphic for finite groups??
        \end{itemize}
        \item If we have $\inp{f_1,f_2}$, we can ask if
        \begin{equation*}
            \inp{f_1,f_2} \stackrel{?}{=} \inp{f_1^g,f_2^g}
        \end{equation*}
        \begin{itemize}
            \item Left as an exercise that this \emph{is} true!
        \end{itemize}
    \end{itemize}
    \item \textbf{Hermitian inner product} (on $\C^n$): The inner product defined as follows for all $z,w\in\C^n$. \emph{Denoted by} $\bm{\inp{\ ,\ }}$. \emph{Given by}
    \begin{equation*}
        \inp{z,w} = \sum_{i=1}^nz_i\bar{w}_i
    \end{equation*}
    \begin{itemize}
        \item This inner product gives a complex number $\inp{v,w}\in\C$ with the following properties.
        \begin{enumerate}
            \item $\inp{a_1v_1+a_2v_2,w}=a_1\inp{v_1,w}+a_2\inp{v_2,w}$.
            \item $\inp{v,b_1w_1+b_2w_2}=\bar{b}_1\inp{v,w_1}+\bar{b}_2\inp{v,w_2}$.
            \item $\inp{v,v}\geq 0$, and $\inp{v,v}=0$ implies that $v=0$.
        \end{enumerate}
        \item Thus, if $v=(z_1,\dots,z_n)$ and $w=(w_1,\dots,w_n)$, then
        \begin{align*}
            \inp{v,w} &= \sum z_i\bar{w}_i&
            \inp{v,v} &= \sum|z_i|^2
        \end{align*}
    \end{itemize}
    \item We now begin tackling today's main theorem: If $V_1,V_2$ are irreps, then
    \begin{equation*}
        \inp{\chi_{V_1},\chi_{V_2}} =
        \begin{cases}
            0 & V_1\ncong V_2\\
            1 & V_1\cong V_2
        \end{cases}
    \end{equation*}
    \begin{itemize}
        \item We will prove this theorem in stages.
        \item The general outline of our approach is to deduce the equality step by step through the transitive property. Some of the equalities we'll eventually end up needing are easier to discuss on their own first, though, so we begin with some lemmas.
    \end{itemize}
    \item First off, recall the \textbf{space of invariants} from PSet 2.
    \item \textbf{Space of invariants} (of a representation $V$): The vector space defined as follows. \emph{Denoted by} $\bm{V^G}$. \emph{Given by}
    \begin{equation*}
        V^G = \{v\in V\mid gv=v\ \forall\ g\in G\}
    \end{equation*}
    \item Lemma 1: Let $G$ be a finite group, let $\rho:G\to GL(V)$ be a finite-dimensional representation of it, and let $p$ be defined as above. Then $p\in\Hom_G(V,V)$.
    \begin{proof}
        We can view $p$ as an element of $\Hom(V,V)$. This combined with the fact that for every $h\in G$,
        \begin{equation*}
            p(hv) = \frac{1}{|G|}\sum_{g\in G}(gh)v
            = \frac{1}{|G|}\sum_{gh\in G}(gh)v
            = \frac{1}{|G|}h\sum_{g\in G}gv
            = h(pv)
        \end{equation*}
        implies that $p\in\Hom_G(V,V)$. In more formal notation,
        \begin{align*}
            [p\circ\rho_V(h)](v) &= \frac{1}{|G|}\sum_{g\in G}[\rho_V(g)\circ\rho_V(h)](v)\\
            &= \frac{1}{|G|}\sum_{g\in G}[\rho_V(gh)](v)\\
            &= \frac{1}{|G|}\sum_{gh\in G}[\rho_V(gh)](v)\\
            &= \frac{1}{|G|}\sum_{hg\in G}[\rho_V(hg)](v)\\
            &= \frac{1}{|G|}\sum_{g\in G}[\rho_V(hg)](v)\\
            &= [\rho_V(h)]\left( \frac{1}{|G|}\sum_{g\in G}[\rho_V(g)](v) \right)\\
            &= [\rho_V(h)\circ p](v)
        \end{align*}
    \end{proof}
    \item Why do we need this result?? What does it do for the rest of the proof?
    \item Lemma 2: Let $G$ be a finite group, and let $\rho:G\to GL(V)$ be a finite-dimensional representation of it. Then the map $p$, defined as follows, is a projector from $V\to V^G$.
    \begin{equation*}
        p = \frac{1}{|G|}\sum_{g\in G}g = \frac{1}{|G|}\sum_{g\in G}\rho_V(g)
    \end{equation*}
    \begin{proof}
        To prove that $p$ is a projector, it will suffice to show that $p^2=p$. To prove that $p$ projects onto $V^G$, it will suffice to show that $\im(p)=V^G$. Let's begin.\par
        To show that $p^2=p$, we have
        \begin{equation*}
            p^2 = \left( \frac{1}{|G|}\sum_{g\in G}g \right)^2
            = \frac{1}{|G|^2}\sum_{g_1,g_2\in G}g_1g_2
            = \frac{|G|}{|G|^2}\sum_{g\in G}g
            = p
        \end{equation*}
        Note that since $G$ is not abelian (i.e., $g_1g_2\neq g_2g_1$ in all cases), the square of $\sum g$ is as above and cannot be reduced to a smaller sum with a 2 coefficient or something like that. Additionally, note that $\sum_{g_1,g_2\in G}g_1g_2=|G|\sum g$ since for each $g_i$, $g_i(g_1+\cdots+g_{|G|})=g_1+\cdots+g_{|G|}$.\par
        To show that $\im(p)=V^G$, we will use a bidirectional inclusion proof. To confirm that $\im(p)\subset V^G$, we have for any $h\in G$ that
        \begin{equation*}
            h\left( \frac{1}{|G|}\sum_{g\in G}gv \right) = \frac{1}{|G|}\sum_{hg\in G}hgv
            = \frac{1}{|G|}\sum_{g\in G}gv
        \end{equation*}
        from which it follows that
        \begin{equation*}
            p(v) = \frac{1}{|G|}\sum gv \in V^G
        \end{equation*}
        as desired. To confirm that $V^G\subset\im(p)$, let $v\in V^G$. Then $gv=v$. It follows that
        \begin{equation*}
            v = \frac{1}{|G|}\sum_{g\in G}v
            = \frac{1}{|G|}\sum_{g\in G}gv
            = p(v)
            \in \im(p)
        \end{equation*}
        as desired.
    \end{proof}
    \item You differentiated the first and second parts of the above proof by saying, "this is the algebraic way to prove it; we can also prove it nonalgebraically." Does this mean that $p^2=p$ somehow \emph{implies} $\im(p)=V^G$ here, or do we still need to prove that "nonalgebraically," as in \textcite{bib:FultonHarris}??
    \item Consequence of Lemma 2: There's a very easy way to construct invariant factors.
    \item We now prove one final lemma using what we have learned about $p$.
    \item Lemma 3: Let $G$ be a finite group, and let $\rho:G\to GL(V)$ be a finite-dimensional representation of it. Then $\dim V^G=(1/|G|)\sum_{g\in G}\chi_V(g)$.
    \begin{proof}
        Define $p$ as above. Then
        \begin{align*}
            \dim V^G &= \dim(\im(p))\tag*{Lemma 2}\\
            &= \tr(p)\tag*{PSet 1, Q5c}\\
            &= \tr(\frac{1}{|G|}\sum\rho_V(g))\\
            &= \frac{1}{|G|}\sum_{g\in G}\tr(\rho_V(g))\\
            &= \frac{1}{|G|}\sum_{g\in G}\chi_V(g)
        \end{align*}
        as desired.
    \end{proof}
    \item We can now prove the main result.
    \item Theorem: If $V,W$ are irreps, then
    \begin{equation*}
        \inp{\chi_V,\chi_W} =
        \begin{cases}
            0 & V\ncong W\\
            1 & V\cong W
        \end{cases}
    \end{equation*}
    \begin{proof}
        % \par\smallskip
        % We could derive this formula, but we'll just state it for now. Let's begin.\par\smallskip
        % Let $G$ be a finite group and let $V$ be a representation of $G$.
        % We want to be able to compute something about this representation. Suppose we want to understand the space of invariants from PSet 2. How do we compute the dimension of $V^G$? Close to what we discussed when we proved the complete reducibility theorem.\par

        % \begin{equation*}
        %     p = \sum_{g\in G}\rho_V(g)
        %     = \sum_{g\in G}\rho_V(hgh^{-1})
        %     = \sum_{g\in G}\rho_V(h)\circ\rho_V(g)\circ\rho_V(h^{-1})
        %     = \rho_V(h)\circ p\circ\rho_V(h^{-1})
        % \end{equation*}
        
        % \par
        % Let $V,W$ be two representations. Try to understand $\Hom_G(V,W)$. Recall that the dimension is 1 if $V\cong G$ and ?? otherwise. We have that
        % \begin{equation*}
        %     \dim(\Hom_G(V,W)) = \dim([\Hom_F(V,W)]^G)
        % \end{equation*}
        % This is almost a tautology, but it requires care, regardless.
        % We have
        % \begin{align*}
        %     \dim([\Hom_F(V,W)]^G) &= \dim((V^*\otimes W)^G)\\
        %     &= \frac{1}{|G|}\sum_{g\in G}\chi_{V^*\otimes W}(g)\\
        %     &= \frac{1}{|G|}\sum_{g\in G}\chi_{V^*}(g)\cdot\chi_W(g)\\
        %     &= \frac{1}{|G|}\sum_{g\in G}\overline{\chi_V(g)}\cdot\chi_W(g)\\
        %     &= \frac{1}{|G|}\sum_{g\in G}\chi_V(g)\cdot\overline{\chi_W(g)}\\
        %     &= \inp{\chi_V,\chi_W}
        % \end{align*}
        % Note that the third-to-last term is an integer, so we can rearrange it to the second-to-last term??\par
        % This implies immediately the theorem via \hyperref[lem:Schur]{Schur's Lemma}.


        We will work towards a formula for the inner product, using various results that we've proven up until now. Let's begin.
        \begin{align*}
            \inp{\chi_V,\chi_W} &= \frac{1}{|G|}\sum_{g\in G}\chi_V(g)\cdot\overline{\chi_W(g)}\tag*{Definition}\\
            &= \frac{1}{|G|}\sum_{g\in G}\chi_V(g)\cdot\chi_{W^*}(g)\tag*{Property 4}\\
            &= \frac{1}{|G|}\sum_{g\in G}\chi_{V\otimes W^*}(g)\tag*{Property 5}\\
            &= \dim[(V\otimes W^*)^G]\tag*{Lemma 3}\\
            &= \dim([\Hom_F(V,W)]^G)\tag*{Lecture 2.1}\\
            &= \dim[\Hom_G(V,W)]\tag*{PSet 2, Q4b}\\
            &=
            \begin{cases}
                \dim(\spn(I)) & V\cong W\\
                \dim(\spn(0)) & V\ncong W
            \end{cases}
            \tag*{Schur's Lemma}\\
            &=
            \begin{cases}
                0 & V\ncong W\\
                1 & V\cong W
            \end{cases}
        \end{align*}
    \end{proof}
    \item In the above proof, Rudenko first surveys the following special case. Why??
    \begin{itemize}
        \item Then if $V$ is irreducible and trivial, we have
        \begin{equation*}
            \frac{1}{|G|}\sum_{g\in G}\chi_V(g) = 0
        \end{equation*}
        which happens iff
        \begin{equation*}
            \inp{\chi_V,\chi_\text{triv}} = 0
        \end{equation*}
        whereas
        \begin{equation*}
            \inp{\chi_\text{triv},\chi_\text{triv}} = 1
        \end{equation*}
        This proves the theorem in a special case, but how do we go from here to all representations? We're very close!
    \end{itemize}
    \item Corollary: The number of irreps is less than or equal to the number of conjugacy classes.
    \begin{itemize}
        \item We'll leave it to next time to prove that equality holds.
    \end{itemize}
    \item Whenever we have a sec, we should try to form a mental picture the whole class function thing.
    \item Consequence of the theorem: We get an orthogonality relation.
    \begin{itemize}
        \item If $\chi_1,\chi_2$ are characters if irreps, then
        \begin{equation*}
            \sum_{g\in G}\chi_1(g)\overline{\chi_2(g)} =
            \begin{cases}
                0 & \chi_1\neq\chi_2\\
                |G| & \chi_1=\chi_2
            \end{cases}
        \end{equation*}
        \item This is related to the character table and IChem!!! Example:
        \begin{itemize}
            \item Recall Table \ref{tab:charTableS3}, the character table for $S_3$.
            \item Between the trivial and alternating representations, we have
            \begin{equation*}
                (1)(1)+(1)(-1)+(1)(-1)+(1)(-1)+(1)(1)+(1)(1) = 0
            \end{equation*}
            as expected. Note that we have a term for each element in $S_3$, so some products get repeated multiple times.
            \item For the standard representation, we have
            \begin{equation*}
                (2)(2)+(0)(0)+(0)(0)+(0)(0)+(-1)(-1)+(-1)(-1) = 6 = |S_3|
            \end{equation*}
            as expected.
        \end{itemize}
    \end{itemize}
    \item Theorem: Characters are equal iff their representations are isomorphic.
    \item Next time.
    \begin{itemize}
        \item Prove the theorem.
        \item Consequences.
        \item Implications for the character table.
    \end{itemize}
\end{itemize}



\section{Character Table Properties}
\begin{itemize}
    \item \marginnote{10/13:}Announcement: Midterm on November 10.
    \begin{itemize}
        \item Will mostly involve computing character tables; HW will be good prep.
    \end{itemize}
    \item Review of the general picture from the first part of the course.
    \begin{figure}[h!]
        \centering
        \begin{tikzpicture}
            \footnotesize
            \filldraw [draw=blx,fill=blz,thick] (-1.5,-1) -- ++(-2,0) -- ++(0,2) -- node[black,above]{Representations} ++(2,0) -- cycle;
            \filldraw [draw=blx,fill=blz,thick] (1.5,-1) -- ++(2,0) -- ++(0,2) -- node[black,above]{Characters} ++(-2,0) -- cycle;
    
            \node (V) at (-2.2,-0.1) {$V$};
            \node (X) at (2.8,-0.1) {$\chi_V^{}$};
            \draw [blz,line width=4pt] (V) -- (-1.5,-0.1);
            \draw [white,line width=4pt] (-1.501,-0.1)-- (1.501,-0.1);
            \draw [blz,line width=4pt] (1.5,-0.1) -- (X);
            \draw [bly,|->] (V) -- (X);
    
            \node [minimum width=1cm] (S1l) at (-2.5,-1.5) {$\oplus$};
            \node [minimum width=1cm] (S2l) at (-2.5,-2)   {$\otimes$};
            \node [minimum width=1cm] (S3l) at (-2.5,-2.5) {$V^*$};
            \node [minimum width=1cm] (S4l) at (-2.5,-3.2) {$\dim_\C\Hom_G(V_1,V_2)$};
            \node [minimum width=1cm] (S1r) at (2.5,-1.5)  {$+$};
            \node [minimum width=1cm] (S2r) at (2.5,-2)    {$\cdot$};
            \node [minimum width=1cm] (S3r) at (2.5,-2.5)  {$\bar{f}$};
            \node [minimum width=1cm] (S4r) at (2.5,-3.2)  {$\inp{\chi_{V_1},\chi_{V_2}}$};
            \draw [bly,->] (S1l) -- (S1r);
            \draw [bly,->] (S2l) -- (S2r);
            \draw [bly,->] (S3l) -- (S3r);
            \node [bly] at ($(S4l)!0.5!(S4r)$) {=};
        \end{tikzpicture}
        \caption{The stories of representation theory.}
        \label{fig:RepTheoryStory}
    \end{figure}
    \begin{itemize}
        \item Let $G$ be a finite group.
        \item First story: We study finite-dimensional representations of $G$ over $\C$; these are vector spaces, so we can direct sum, tensor multiply, and dualize them. We can also look at the morphisms between them.
        \item Second story: We study class functions $\C_\text{cl}[G]=\{f:G\to\C\mid f(gxg^{-1})=f(x)\}$; these are elements of a ring, so we can add, multiply, and conjugate them. We can also take the inner product of them.
        \item We can map between these two stories: Representations become characters, $\oplus\mapsto +$, $\otimes\mapsto\cdot$, and $V^*\mapsto\bar{f}$.
    \end{itemize}
    \item Theorem (from last time):
    \begin{equation*}
        \inp{\chi_{V_1},\chi_{V_2}} = \dim\Hom_G(V_1,V_2)
    \end{equation*}
    \item The story in Figure \ref{fig:RepTheoryStory} tells us stuff about representations.
    \begin{itemize}
        \item Let $V_1,\dots,V_k$ be irreps. Then the vectors $\chi_{V_1},\dots,\chi_{V_k}$ are orthonormal.
        \begin{itemize}
            \item We get this result with the Theorem above and \hyperref[lem:Schur]{Schur's Lemma}.
        \end{itemize}
        \item Next time, we'll prove that $\chi_{V_1},\dots,\chi_{V_k}$ spans $\C_\text{cl}[G]$, i.e., the number of irreps is the number of conjugacy classes.
        \item \emph{Cube thing??}
        \item This picture is remarkable because it's so simple.
    \end{itemize}
    \item We now look at some corollaries to last time's main theorem.
    \item Corollary 1: If $V,W$ are $G$-reps, then $\chi_V=\chi_W$ iff $V\cong W$.
    \begin{proof}
        Invoking complete reducibility, we have that $V=\bigoplus V_i^{n_i}$. Thus, to know $V$, it is enough to know the $n_i$'s. But
        \begin{equation*}
            \chi_V = \sum n_i\chi_{V_i}
        \end{equation*}
        where
        \begin{equation*}
            n_i = n_i\cdot 1 = n_i\inp{\chi_{V_i},\chi_{V_i}} = \inp{\chi_V,\chi_{V_i}}
        \end{equation*}
        Therefore, since the $\chi_{V_i}$ are linearly independent, the only way that $\chi_V=\chi_W$ is if the $n_i$'s match which would mean that $V\cong W$, and vice versa the only way that $V\cong W$ is if the $n_i$'s match which would mean that $\chi_V=\chi_W$.
    \end{proof}
    \item Corollary 2: Let $V$ be a $G$-rep. Then TFAE:
    \begin{enumerate}
        \item $V$ is irreducible.
        \item $\inp{\chi_V,\chi_V}=1$.
        \item $\sum_{g\in G}|\chi_V(g)|^2=|G|$.
    \end{enumerate}
    \begin{proof}
        ($1\Rightarrow 2$): We have that
        \begin{equation*}
            \inp{\chi_V,\chi_V} = \dim\Hom_G(V,V) = 1
        \end{equation*}
        as desired.\par
        ($2\Rightarrow 1$): Complete reducibility implies that $V\cong V_1^{n_1}\oplus\cdots\oplus V_k^{n_k}$, where the $V_i$'s are irreps. This combined with the hypothesis implies that
        \begin{equation*}
            1 = \inp{\chi_V,\chi_V}
            = \inp{\sum_{i=1}^kn_i\chi_{V_i},\sum_{i=1}^kn_i\chi_{V_i}}
            = \sum_{i=1}^kn_i^2
        \end{equation*}
        But if $\sum n_i^2=1$ where each $n_i\in\Z^+$, then $n_i=1$ for some $i$ and $n_j=0$ for $j\neq i$, from which it follows that $V\cong V_i$.\par
        We can interconvert between 2 and 3 using the definition of the inner product and the property of complex numbers that $zz^*=|z|^2$.
    \end{proof}
    \item We now build up to one final corollary.
    \item We've discussed all of these properties of irreps, but where do we even find them?
    \begin{itemize}
        \item We might be able to find some by inspection, but here's how we find all of them.
    \end{itemize}
    \item Review: The regular representation. Here are two different but isomorphic ways to think about it.
    \begin{itemize}
        \item Think of it as functions on $G$.
        \begin{itemize}
            \item Better for infinite groups.
        \end{itemize}
        \item Think of it as the permutational representation associated with the action $G\acts G$.
        \begin{itemize}
            \item Better for finite groups.
        \end{itemize}
        \item Why did we talk about this here??
    \end{itemize}
    \item Corollary 3: Consider the regular representation $V_R$. We have that
    \begin{equation*}
        \chi_{V_R}(g) =
        \begin{cases}
            0 & g\neq e\\
            |G| & g=e
        \end{cases}
    \end{equation*}
    \begin{proof}
        We can compute its character $\chi_{V_R}$ by considering the corresponding permutation matrices. Indeed, the action $\chi_{V_R}(g)$ of this character on $g$ is equal to the number of 1's on the diagonal in the permuttion matrix, which is equal to the number of fixed points of the permutation, i.e., the number of $i$'s such that $gg_i=g_i$. But in a group, $gg_i=g_i$ iff $g=e$, so this number of fixed points is
        \begin{equation*}
            \chi_{V_R}(g) = \Fix(g)
            =
            \begin{pmatrix}
                g_1 & \cdots & g_n\\
                gg_1 & \cdots & gg_n\\
            \end{pmatrix}
            =
            \begin{cases}
                0 & g\neq e\\
                |G| & g=e
            \end{cases}
        \end{equation*}
        as desired.
    \end{proof}
    \item What is the matrix thing?
    \begin{itemize}
        \item It is most likely a representation of the function $g_i\mapsto gg_i$, and it denotes that $\Fix(g)$ is equal to the number of columns that have the same entry top and bottom.
    \end{itemize}
    \item We now apply Corollaries 1-3 to the regular representation $V_R$ to obtain some important results.
    \begin{itemize}
        \item Let $V_i$ be an arbitrary irrep.
        \item By complete reducibility, $V_R=\bigoplus_{i=1}^kV_i^{n_i}$ for some set of $n_i$'s.
        \item Additionally,
        \begin{align*}
            n_i &= \inp{\chi_{V_R},\chi_{V_i}}\tag*{Corollary 1}\\
            &= \frac{1}{|G|}\sum_{g\in G}\chi_{V_R}(g)\overline{\chi_{V_i}(g)}\\
            &= \frac{1}{|G|}|G|\underbrace{\overline{\chi_{V_i}(e)}}_{\dim V_i}\tag*{Corollary 3}\\
            &= \dim V_i
        \end{align*}
        \item This implies three remarkable results, all worth remembering.
        \begin{align*}
            V_R &= \bigoplus_{i=1}^kV_i^{\dim V_i}&
            |G| &= \sum_{i=1}^k(\dim V_i)^2&
            &\text{\# irreps is finite}
        \end{align*}
        \begin{itemize}
            \item The first result follows directly by substituting $n_i=\dim V_i$ into complete reducibility.
            \item The second result follows because $|G|=\dim(V_R)=\dim(\bigoplus_{i=1}^kV_i^{\dim V_i})=\sum(\dim V_i)^2$.
            \item The third result follows because if there were infinitely many irreps, each with $\dim V_i\geq 1$, then $|G|=\sum_{i=1}^k(\dim V_i)^2=\infty$, contradicting the hypothesis that $|G|$ is finite.
        \end{itemize}
    \end{itemize}
    % \item Proof that the number of irreps equals the number of conjugacy classes.
    \item We want to investigate $S_4$, i.e., characterize all irreps of it.
    \begin{table}[h!]
        \centering
        \small
        \renewcommand{\arraystretch}{1.2}
        \begin{tabular}{c|c|c|c|c|c|}
             & 1 & 6 & 8 & 3 & 6\\
             & $e$ & $(12)$ & $(123)$ & $(12)(34)$ & $(1234)$\\ \hline
            Trivial & 1 & 1 & 1 & 1 & 1\\ \hline
            Sign & 1 & $-1$ & 1 & 1 & $-1$\\ \hline
            $V_\text{std}$ & 3 & 1 & 0 & $-1$ & $-1$\\ \hline
            $\text{Sign}\otimes V_\text{std}$ & 3 & $-1$ & 0 & $-1$ & 1\\ \hline
             & 2 & 0 & $-1$ & 2 & 0\\
        \end{tabular}
        \caption{Character table for $S_4$.}
        \label{tab:charTableS4}
    \end{table}
    \begin{itemize}
        \item We do so by constructing the character table, Table \ref{tab:charTableS4}.
        \item Initially, this seemed like a very hard problem.
        \begin{itemize}
            \item However, with all of our theory, it only takes a couple of minutes now!
        \end{itemize}
        \item We start by inputting the trivial, sign, and standard representations.
        \begin{itemize}
            \item The trivial is obviously $(1,1,1,1,1)$.
            \item The sign can be calculated to be $(1,-1,1,1,-1)$.
            \item The standard is found via $V_\text{std}=V_\text{perm}-V_\text{triv}=(4,2,1,0,0)-(1,1,1,1,1)=(3,1,0,-1,-1)$.
        \end{itemize}
        \item Note that
        \begin{gather*}
            \sum n_i^2 = \inp{\chi_{V_\text{perm}},\chi_{V_\text{perm}}}
                = \frac{1}{24}(1\cdot 4^2+6\cdot 2^2+8\cdot 1^2+0\cdot 0^2+0\cdot 0^2)
                = 2\\
            \inp{\chi_{V_\text{perm}},\chi_{V_\text{sign}}} = \frac{1}{24}(4-12+8)
                = 0\\
            \inp{\chi_{V_\text{perm}},\chi_{V_\text{triv}}} = 1
        \end{gather*}
        \begin{itemize}
            \item What is the point of these calculations??
        \end{itemize}
        \item Thus, we can derive representations without having any geometric notion of it using characters!
        \item To see any of these representations geometrically, look at actions on the tetrahedron in $\R^3$!
        \item All those computations above used the \textbf{first orthogonality relation}; here's the \textbf{second orthogonality relation}:
        \begin{equation*}
            \sum_\chi\chi(g_1)\overline{\chi(g_2)} =
            \begin{cases}
                0 & g_1\nsim g_2\\
                \frac{|G|}{|C_G(g)|} & g_1\sim g_2
            \end{cases}
        \end{equation*}
        where $\sim$ denotes conjugacy and $C_G(g)$ is the number of elements in the conjugacy class of $g$ (where we borrow centralizer notation).
        \begin{itemize}
            \item Prove the new one with $AB=1=BA$. This very simple thing leads to a very powerful statement about systems of equations that we will discuss later. How does this proof work??
            \item We can start doing this stuff in the new homework!
            \item We constructed the fifth representation using this.
        \end{itemize}
        \item Where does the fifth irrep come from?
        \begin{itemize}
            \item Going back to a miracle of group theory: Simple groups.
            \item If $f:S_n\to S_n$, we have lots of injective maps, lots of minor actions $S_n\to S_n$ sending $x\mapsto gxg^{-1}$.
            \item We have $\sign:S_n\to S_2$, $S_4\twoheadrightarrow S_3$ with kernel equal to $K_4$.
            \item We have the exotic $S_6\to S_6$.
            \item We have $S_5\hookrightarrow S_6$ that is also exotic.
            \item These are called \textbf{exceptional homomorphisms}.
            \item Since we have $S_4\to S_3$ and $\rho:S_3\to GL_n$, we have $S_4\to GL_n$ with a the same character table. Takeaway: This $(2,0,1)$ thing in the big character table comes from this map, geometrically.
            \item Takeaway: The geometry of the fifth irrep comes from $S_3$.
            \item What is going on here??
        \end{itemize}
    \end{itemize}
    \item Final announcements.
    \begin{itemize}
        \item Going forward, we'll mostly be following \textcite{bib:FultonHarris}.
        \item Then we'll get into associative algebra.
        \item OH next week will be Zoom, but we should still feel free to meet with him in-person by emailing him for an appointment.
    \end{itemize}
\end{itemize}



\section{S Chapter 2: Character Theory}
\emph{From \textcite{bib:Serre}.}
\subsection*{Section 2.1: The Character of a Representation}
\begin{itemize}
    \item \marginnote{11/2:}Definition of \textbf{trace}.
    \item \textbf{Character} (of $\rho$): The complex-valued function on $G$ defined as follows. \emph{Denoted by} $\bm{\chi_\rho}$. \emph{Given by}
    \begin{equation*}
        \chi_\rho(s) = \tr(\rho_s)
    \end{equation*}
    \item Notation: \textcite{bib:Serre} uses both $z^*$ and $\bar{z}$ to denote the complex conjugate of a $z\in\C$.
    \item Properties of the character as a function.
    \begin{proposition}\label{prp:charPropsFun}
        If $\chi$ is the character of a representation $\rho$ of degree $n$, then\dots
        \begin{enumerate}[label={\textup{(\roman*)}}]
            \item $\chi(1)=n$;
            \item $\chi(s^{-1})=\chi(s)^*$ for all $s\in G$;
            \item $\chi(tst^{-1})=\chi(s)$ for all $s\in G$.
        \end{enumerate}
        \begin{proof}
            \underline{(i)}:
            \begin{equation*}
                \chi(1) = \tr(\rho_1)
                = \tr(1)
                = n
            \end{equation*}
            \underline{(ii)}: Recall that there exists a basis with respect to which $\rho_s$ is unitary. Thus, its (not necessarily unique) eigenvalues $\lambda_1,\dots,\lambda_n$ are roots of unity. Consequently, $\lambda_i^{-1}=\lambda_i^*$ ($i=1,\dots,n$). Therefore,
            \begin{equation*}
                \chi(s^{-1}) = \tr(\rho_{s^{-1}})
                = \tr(\rho_s^{-1})
                = \sum\lambda_i^{-1}
                = \sum\lambda_i^*
                = \tr(\rho_s)^*
                = \chi(s)^*
            \end{equation*}
            \underline{(iii)}: This follows directly from the fact that the trace is invariant under choice of basis. Alternatively (and more explicitly), we may put $u=\rho_t\rho_s$ and $v=\rho_{t^{-1}}$ and use the formula $\tr(ab)=\tr(ba)$\footnote{See Theorem 10.4 of \textcite{bib:Axler}.} as follows.
            \begin{equation*}
                \chi(tst^{-1}) = \tr(\rho_{tst^{-1}})
                = \tr(\rho_t\rho_s\rho_{t^{-1}})
                = \tr(uv)
                = \tr(vu)
                = \tr(\rho_{t^{-1}}\rho_t\rho_s)
                % = \tr(\rho_{t^{-1}t}\rho_s)
                % = \tr(\rho_e\rho_s)
                % = \tr(I\rho_s)
                = \tr(\rho_s)
                = \chi(s)
            \end{equation*}
        \end{proof}
    \end{proposition}
    \item \textbf{Class function}: A function $f$ on $G$ satisfying identity (iii) in Proposition \ref{prp:charPropsFun}.
    \item Properties of the character with respect to representations.
    \begin{proposition}\label{prp:charPropsSumProd}
        Let $\rho^1:G\to GL(V_1)$ and $\rho^2:G\to GL(V_2)$ be two linear representations of $G$, and let $\chi_1,\chi_2$ be their characters. Then\dots
        \begin{enumerate}[label={\textup{(\roman*)}}]
            \item The character $\chi$ of the direct sum representation $V_1\oplus V_2$ is equal to $\chi_1+\chi_2$;
            \item The character $\psi$ of the tensor product representation $V_1\otimes V_2$ is equal to $\chi_1\cdot\chi_2$.
        \end{enumerate}
        \begin{proof}
            As in class.
        \end{proof}
    \end{proposition}
    \item \textcite{bib:Serre} defines the characters of the symmetric square and alternating square representations.
    \stepcounter{proposition}
\end{itemize}


\subsection*{Section 2.2: Schur's Lemma; Basic Applications}
\begin{itemize}
    \item \textcite{bib:Serre} states and proves Schur's Lemma.
    \begin{proposition}[Schur's Lemma]
        Let $\rho^1:G\to GL(V_1)$ and $\rho^2:G\to GL(V_2)$ be two irreducible representations of $G$, and let $f$ be a linear mapping of $V_1$ into $V_2$ such that $\rho_s^2\circ f=f\circ\rho_s^1$ for all $s\in G$. Then\dots
        \begin{enumerate}[label={\textup{(\roman*)}}]
            \item If $\rho^1$ and $\rho^2$ are not isomorphic, then $f=0$;
            \item If $V_1=V_2$ and $\rho^1=\rho^2$, then $f$ is a \textbf{homothety}.
        \end{enumerate}
        \begin{proof}
            This proof is identical to the one given in \textcite{bib:FultonHarris}. However, I am including it again here because it includes a few key details that \textcite{bib:FultonHarris} leave out. Let's begin.\par\smallskip
            \underline{(i)}:
            % We divide into two cases ($f=0$ and $f\neq 0$). The case $f=0$ is trivial. Now suppose $f\neq 0$.
            To prove the claim, it will suffice to prove the contrapositive. Suppose $f\neq 0$. To prove that $\rho^1\cong\rho^2$, it will suffice to show that $f$ is an isomorphism of $G$-representations. Since $f$ is a morphism of $G$-representations by hypothesis, all that is left is to show that it is an isomorphism of vector spaces. To do so, we will demonstrate that $\ker(f)=0$ and $\im(f)=V_2$, one claim at a time. Let's begin.\par
            Let $W_1=\ker(f)$. Then for any $s\in G$ and $x\in W_1$,
            \begin{equation*}
                f(\rho_s^1(x)) = [f\circ\rho_s^1](x)
                = [\rho_s^2\circ f](x)
                = \rho_s^2(f(x))
                = \rho_s^2(0)
                = 0
            \end{equation*}
            so $\rho_s^1(x)\in W_1$. It follows that $W_1$ is stable under $G$. But since $V_1$ is irreducible, this means that $W_1=V_1,0$. This combined with the fact that $f\neq 0$ (hence $W_1=\ker(f)\neq V_1$) implies that $W_1=0$.\par
            Let $W_2=\im(f)$. Then for any $s\in G$ and $f(x)\in W_2$,
            \begin{equation*}
                \rho_s^2(f(x)) = [\rho_s^2\circ f](x)
                = [f\circ\rho_s^1](x)
                = f(\rho_s^1(x))
            \end{equation*}
            so $\rho_s^2(fx)\in W_2$. It follows that $W_2$ is stable under $G$. But since $V_2$ is irreducible, this means that $W_2=V_2,0$. This combined with the fact that $f\neq 0$ (hence $W_2=\im(f)\neq 0$) implies that $W_2=V_2$.\par\smallskip
            \underline{(ii)}: Since $f$ is an operator on a finite-dimensional, nonzero, complex vector space, it has\footnote{See Theorem 5.5 of \textcite{bib:Axler}.} an eigenvalue $\lambda$. To prove that $f$ is a homothety, it will suffice to show that $f=\lambda I$, which we will do by demonstrating that $f-\lambda I=0$ using part (i). Indeed, to use part (i) in this manner, we need only show that $f-\lambda I$ \emph{does} satisfy $\rho_s^2\circ(f-\lambda I)=(f-\lambda I)\circ\rho_s^1$ but \emph{is not} an isomorphism of vector spaces. For the first claim, we have since $\rho^1=\rho^2$ and $V_1=V_2$ that
            \begin{equation*}
                \rho_s^2\circ(f-\lambda I) = \rho_s^2\circ f-\rho_s^2\circ\lambda I
                = f\circ\rho_s^1-\lambda I\circ\rho_s^2
                = f\circ\rho_s^1-\lambda I\circ\rho_s^1
                = (f-\lambda I)\circ\rho_s^1
            \end{equation*}
            For the second claim, we know that the eigenvector corresponding to $\lambda$ is in $\ker(f-\lambda I)$, so $f-\lambda I$ has a nontrivial kernel and thus cannot be an isomorphism.
        \end{proof}
    \end{proposition}
    \item \textbf{Homothety}: A scalar multiple of the identity operator. \emph{Given by}
    \begin{equation*}
        \lambda I
    \end{equation*}
    for some $\lambda\in\C$.
    \item \marginnote{11/8:}Another condition for telling if two representations are isomorphic.
    \begin{corollary}
        Let $h$ be a linear mapping of $V_1$ into $V_2$ and put
        \begin{equation*}
            h^0 = \frac{1}{g}\sum_{t\in G}(\rho_t^2)^{-1}h\rho_t^1
        \end{equation*}
        with $g=|G|$. Then\dots
        \begin{enumerate}[label={\textup{(\roman*)}}]
            \item If $\rho^1$ and $\rho^2$ are not isomorphic, we have $h^0=0$;
            \item If $V_1=V_2$ and $\rho^1=\rho^2$, $h^0$ is a homothety of ratio $(1/n)\tr(h)$ with $n=\dim(V_1)$.
        \end{enumerate}
        \begin{proof}
            Given.
        \end{proof}
    \end{corollary}
    \item The above corollary in matrix form:
    \begin{corollary}\label{cly:repIsoMat1}
        Let $(r_{i_1j_1}(t)):=\rho_t^1$, $(r_{i_2j_2}(t)):=\rho_t^2$, $(x_{i_2i_1}):=h$, and
        \begin{equation*}
            x_{i_2i_1}^0 := h^0
            = \frac{1}{g}\sum_{t,j_1,j_2}r_{i_2j_2}(t^{-1})x_{j_2j_1}r_{j_1i_1}(t)
        \end{equation*}
        Then if $(r_{i_1j_1}(t))$ and $(r_{i_2j_2}(t))$ are not isomorphic, we have
        \begin{equation*}
            \frac{1}{g}\sum_{t,j_1,j_2}r_{i_2j_2}(t^{-1})r_{j_1i_1}(t) = 0
        \end{equation*}
        for arbitrary $i_1,i_2,j_1,j_2$.
        \begin{proof}
            Described.
        \end{proof}
    \end{corollary}
    \begin{corollary}\label{cly:repIsoMat2}
        Under the same definitions as before, if $(r_{i_1j_1}(t))$ and $(r_{i_2j_2}(t))$ are isomorphic, we have
        \begin{equation*}
            \frac{1}{g}\sum_{t\in G}r_{i_2j_2}(t^{-1})r_{j_1i_1}(t) = \frac{1}{n}\delta_{i_2i_1}\delta_{j_2j_1}
        \end{equation*}
        \begin{proof}
            Described.
        \end{proof}
    \end{corollary}
    \item Note that under the notation
    \begin{equation*}
        \inp{\phi,\psi} = \frac{1}{g}\sum_{t\in G}\phi(t^{-1})\psi(t)
        = \frac{1}{g}\sum_{t\in G}\phi(t)\psi(t^{-1})
    \end{equation*}
    we have\dots
    \begin{itemize}
        \item $\inp{\phi,\psi}=\inp{\psi,\phi}$;
        \item $\inp{\phi,\psi}$ is linear in $\phi$ and $\psi$;
        \item Corollary \ref{cly:repIsoMat1} becomes $\inp{r_{i_2j_2},r_{j_1i_1}}=0$;
        \item Corollary \ref{cly:repIsoMat2} becomes $\inp{r_{i_2j_2},r_{j_1i_1}}=(1/n)\delta_{i_2i_1}\delta_{j_2j_1}$.
    \end{itemize}
    \item Furthermore, note that if we choose the $(r_{ij}(t))$ to be unitary, then $r_{ij}(t^{-1})=r_{ji}(t)^*$ and Corollaries \ref{cly:repIsoMat1}, \ref{cly:repIsoMat2} are \textbf{orthogonality relations} for the scalar product $(\phi,\psi)$ defined below.
\end{itemize}


\subsection*{Section 2.3: Orthogonality Relations for Characters}
\begin{itemize}
    \item \textbf{Scalar product}: A binary operation $(x\mid y)$, where $x,y$ are elements of some vector space $V$, that is linear in $x$, semilinear in $y$, and satisfies $(x\mid x)>0$ for all $x\neq 0$.
    \item Define a scalar product on the space of complex valued functions on $G$ by
    \begin{equation*}
        (\phi\mid\psi) = \frac{1}{g}\sum_{t\in G}\phi(t)\psi(t)^*
    \end{equation*}
    \item Defining $\tilde{\psi}(t):=\psi(t^{-1})^*$, we have that
    \begin{equation*}
        (\phi\mid\psi) = \frac{1}{g}\sum_{t\in G}\phi(t)\tilde{\psi}(t^{-1})
        = \inp{\phi,\tilde{\psi}}
    \end{equation*}
    \begin{itemize}
        \item Special case: By Proposition \ref{prp:charPropsFun}, $\tilde{\chi}=\chi$, so
        \begin{equation*}
            (\phi\mid\chi) = \inp{\phi,\chi}
        \end{equation*}
    \end{itemize}
    \item We now prove a theorem analogous to the main theorem from Wednesday's lecture.
    \setcounter{theorem}{2}
    \begin{theorem}\label{trm:charOrthonormal}
        \leavevmode
        \begin{enumerate}[label={\textup{(\roman*)}}]
            \item If $\chi$ is the character of an irreducible representation, we have $(\chi\mid\chi)=1$ (i.e., $\chi$ is "of norm 1").
            \item If $\chi$ and $\chi'$ are the characters of two nonisomorphic irreducible representations, we have $(\chi\mid\chi')=0$ (i.e., $\chi,\chi'$ are orthogonal).
        \end{enumerate}
        \begin{proof}
            Extremely clean argument using Corollaries \ref{cly:repIsoMat1}-\ref{cly:repIsoMat2}.
        \end{proof}
    \end{theorem}
    \item \textbf{Irreducible character}: A character of an irreducible representation.
    \item With this definition in hand, we can see that the irreducible characters form an orthonormal system.
    \item With the previous result in hand, we can use characters to count how many times an irreducible representation occurs within the direct sum decomposition of a representation.
    \begin{theorem}
        Let $V$ be a linear representation of $G$, with character $\phi$, and suppose $V$ decomposes into a sum of irreducible representations via
        \begin{equation*}
            V = W_1\oplus\cdots\oplus W_k
        \end{equation*}
        Then if $W$ is an irreducible representation with character $\chi$, the number of $W_i$ isomorphic to $W$ is equal to the scalar product $(\phi\mid\chi)=\inp{\phi,\chi}$.
        \begin{proof}
            Let $\chi_i$ be the character of $W_i$. Then by Proposition \ref{prp:charPropsSumProd}, we have
            \begin{equation*}
                \phi = \chi_1+\cdots+\chi_k
            \end{equation*}
            Now recall from Theorem \ref{trm:charOrthonormal} that $(\chi_i\mid\chi)$ is 1 or 0 depending on whether $W_i$ is or is not isomorphic to $W$, respectively. Thus, in the sum
            \begin{equation*}
                (\phi\mid\chi) = (\chi_1\mid\chi)+\cdots+(\chi_k\mid\chi)
            \end{equation*}
            all terms for which $\chi_i\neq\chi$ go to zero and all terms for which $\chi_i=\chi$ go to 1, leaving a sum that adds 1 for every occurrence of $W$, as desired.
        \end{proof}
    \end{theorem}
    \item We now explore some results that immediately follow from this result.
    \item There is a uniqueness in the decomposition of a representation into irreducible representations.
    \setcounter{corollary}{0}
    \begin{corollary}
        The number of $W_i$ isomorphic to $W$ does not depend on the chosen decomposition.
    \end{corollary}
    \item Matching Corollary 1 from Friday's class.
    \begin{corollary}
        Two representations with the same character are isomorphic.
    \end{corollary}
    \item Note on this result.
    \begin{itemize}
        \item This is what reduces the study of representations to the study of characters.
    \end{itemize}
    \item \textcite{bib:Serre} reiterates the formula
    \begin{equation*}
        (\phi\mid\phi) = \sum_{i=1}^hm_i^2
    \end{equation*}
    where $\phi$ is the character of $V=W_1^{m_1}\oplus\cdots\oplus W_h^{m_h}$.
    \item An \textbf{irreducibility criterion} based on Corollary 2 from Friday's class.
    \begin{theorem}\label{trm:irrepCriterion}
        If $\phi$ is the character of a representation $V$, $(\phi\mid\phi)$ is a positive integer and we have $(\phi\mid\phi)=1$ if and only if $V$ is irreducible.
        \begin{proof}
            See class.
        \end{proof}
    \end{theorem}
\end{itemize}


\subsection*{Section 2.4: Decomposition of the Regular Representation}
\begin{itemize}
    \item Notation.
    \begin{itemize}
        \item $\chi_1,\dots,\chi_h$: The irreducible characters of $G$.
        \item $n_1,\dots,n_k$: The degrees of $\chi_1,\dots,\chi_h$; $n_i=\chi_i(1)$.
        \item $\rho:G\to GL(R)$: The regular representation of $G$.
    \end{itemize}
    \item Matching Corollary 3 from Friday's class: Calculating $\rho_s$ for all $s\in G$.
    \begin{proposition}\label{prp:regRepChar}
        The character $r_G$ of the regular representation is given by the formulas
        \begin{align*}
            r_G(1) &= g&
            r_G(s) &= 0
        \end{align*}
        where we assume $s\neq 1$ in the right equation above.
        \begin{proof}
            If $s\neq e$, then $st\neq t$ for all $t\in G$. Thus, the diagonal terms of $\rho_s$ are all 0 in this case, so $\tr(\rho_s)=0$.\par
            If $s=e$, then $st=t$ for all $t\in G$. Thus, the diagonal terms of $\rho_s$ are all 1 in this case, so $\tr(\rho_s)=\tr(1)=\dim(R)=g$.
        \end{proof}
    \end{proposition}
    \item Matching the leftmost consequence of Corollary 3 from Friday's class.
    \setcounter{corollary}{0}
    \begin{corollary}\label{cly:regularDsum}
        Every irreducible representation $W_i$ is contained in the regular representation with multiplicity equal to its degree $n_i$
    \end{corollary}
    \item Matching the middle consequence of Corollary 3 from Friday's class (i) and the orthogonality of the first column in a character table with every other column, a special case of the second orthogonality criterion (ii).
    \begin{corollary}\label{cly:columnOrtho}
        \leavevmode
        \begin{enumerate}[label={\textup{(\roman*)}},ref={\thecorollary\roman*}]
            \item \label{cly:columnOrthoi}The degrees $n_i$ satisfy the relation
            \begin{equation*}
                \sum_{i=1}^hn_i^2 = g
            \end{equation*}
            \item \label{cly:columnOrthoii}If $s\in G$ is different from 1, we have
            \begin{equation*}
                \sum_{i=1}^hn_i\chi_i(s) = 0
            \end{equation*}
        \end{enumerate}
        \begin{proof}
            Corollary \ref{cly:regularDsum} says that $r_G(s)=\sum n_i\chi_i(s)$ for all $s\in G$. Taking $s=1$ gives (i) and $s\neq 1$ gives (ii).
        \end{proof}
    \end{corollary}
\end{itemize}



\section{FH Chapter 2: Characters}
\setcounter{FHchapter}{2}
\emph{From \textcite{bib:FultonHarris}.}
\subsection*{Section 2.1: Characters}
\begin{itemize}
    \item The example from last time suggests that "knowing all the eigenvalues of each element of $G$ [that is, each $\rho(g)$] should suffice to describe the representation" \parencite[12]{bib:FultonHarris}.
    \begin{itemize}
        \item We formalize this notion via \textbf{character theory}.
        \item In particular, note that we do not actually need to specify all eigenvalues of all elements of $G$; rather, we can opt to specify their sums, since knowing the $\sum\lambda_i^k$ is equivalent to knowing the $\{\lambda_i\}$ of $g$. This motivates the definition of the character as the trace!
    \end{itemize}
    \item Definition of a \textbf{character} and \textbf{class function}.
    \item Proposition 2.1: Properties of the character as discussed in class.
    \item Example 2.5: References the character of the permutation representation and $\Fix(g)$.
    \item An alternate way of deriving the characters of the standard representation.
    \begin{itemize}
        \item Calculate for the permutation representation $(3,1,0)$, which is easy.
        \item Note that $V_\text{perm}=V_{(2,1)}\oplus V_{(3)}$, so $(3,1,0)=(x,y,z)+(1,1,1)$, meaning that the character of the standard representation is $(2,0,-1)$, as desired.
    \end{itemize}
    \item Using characters to decompose arbitrary representations $W$ of $S_3$.
    \begin{itemize}
        \item We take it for granted that $W=V_{(3)}^a\oplus V_{(2,1)}^b\oplus V_{(1,1,1)}^c$.
        \item But then $\chi_W=a\chi_{V_{(3)}}+b\chi_{V_{(2,1)}}+c\chi_{V_{(1,1,1)}}$. Moreover, since the three characters are linearly independent, we can solve the system of equations for $a,b,c$.
        \item Takeaway: "$W$ is determined up to isomorphism by its character $\chi_W$" \parencite[14]{bib:FultonHarris}.
    \end{itemize}
    \item There are some great exercises throughout this section that I could come back to later for more practice!
\end{itemize}


\subsection*{Section 2.2: The First Projection Formula and its Consequences}
\begin{itemize}
    \item \textcite{bib:FultonHarris} identify a different goal than either Rudenko or \textcite{bib:Serre} when proving the orthonormality of the irreducible characters, but an interesting one nonetheless! Let's begin.
    \item We wish to construct an \emph{explicit} formula for a certain projection operator. The operator of interest is the one that projects the vectors in a representation $V$ onto the subspace $V_\text{triv}^m\leq V$ consisting of the direct sum of the trivial representations in the decomposition.
    \item Observe that the subspace $V_\text{triv}^m\leq V$ consists of all vectors $v\in V$ such that $gv=v$ for all $g\in G$. We call this subspace "$V^G$."
    \item Call our desired projection operator $\varphi$. What properties should $\varphi$ have?
    \begin{itemize}
        \item If it is to map between representations $V$ and $V^G$, it should be a morphism of $G$-representations.
        \begin{itemize}
            \item How is this related to Lemma 1??
        \end{itemize}
        \item It should satisfy $\varphi^2=\varphi$ and $\range\varphi=V^G$.
    \end{itemize}
    \item Now suppose we want to find $m$, the number of copies of the trivial representation appearing in the decomposition of $V$.
    \begin{itemize}
        \item Consider the matrix of $\varphi$ in a basis of $V$ such that the first $m$ vectors lie in $V^G$ and the rest of the vectors lie in the complement of $V^G$.
        \item This block diagonal matrix will be the $m\times m$ identity in the upper left and the zero matrix in the bottom right.
        \item Thus, $m=\tr(\varphi)$.
        \item Expanding, we actually have
        \begin{equation*}
            m = \tr(\varphi)
            = \frac{1}{|G|}\sum_{g\in G}\tr(g)
            = \frac{1}{|G|}\sum_{g\in G}\chi_V(g)
        \end{equation*}
        \item Note that both $V_\text{triv}$ and $V=V_\text{triv}^n\oplus\cdots$ make the equality true; it's just that if we want the following consequence, we \emph{need} $V$ in there.
    \end{itemize}
    \item Important consequence of the above: For an irreducible representation $V$ other than the trivial one, $\sum_{g\in G}\chi_V(g)=0$.
    \begin{itemize}
        \item Example: In Table \ref{tab:charTableS3}, notice how $1(1)+3(-1)+2(1)=0$ and $1(2)+3(0)+2(-1)=0$.
    \end{itemize}
    \item "If $V$ is irreducible, then by Schur's Lemma, $\dim[\Hom(V,W)^G]$ is the multiplicity of $V$ in $W$; similarly, if $W$ is irreducible, $\dim[\Hom(V,W)^G]$ is the multiplicity of $W$ in $V$, and in the case where both $V$ and $W$ are irreducible, we have
    \begin{equation*}
        \dim\Hom_G(V,W) =
        \begin{cases}
            1 & V\cong W\\
            0 & V\ncong W
        \end{cases}
    \end{equation*}
    " \textcite[16]{bib:FultonHarris}.
    \item \textcite{bib:FultonHarris} finally arrives at the theorem from Wednesday's lecture.
    \setcounter{FHlemma}{12}
    \item This theorem gives us a lower bound on the number of conjugacy classes of $G$.
    \begin{FHcorollary}\label{cly:lowerBoundConjCl}
        The number of irreducible representations of $G$ is less than or equal to the number of conjugacy classes.
        \begin{proof}
            We have that
            \begin{align*}
                \spn(\chi_1,\dots,\chi_k) &\leq \C_\text{cl}[G]\\
                \dim[\spn(\chi_1,\dots,\chi_k)] &\leq \dim(\C_\text{cl}[G])\\
                \text{\# irreducible representations} &\leq \text{\# conjugacy classes}
            \end{align*}
            as desired.
        \end{proof}
    \end{FHcorollary}
    \item Corollary 1 from Friday's class.
    \begin{FHcorollary}
        Any representation is determined by its character.
    \end{FHcorollary}
    \item Corollary 2 from Friday's class.
    \begin{FHcorollary}
        A representation $V$ is irreducible iff $(\chi_V,\chi_V)=1$.
    \end{FHcorollary}
    \item A result from the proof of Corollary 1 from Friday's class.
    \begin{FHcorollary}
        The multiplicity $a_i$ of $V_i$ in $V$ is the inner product of $\chi_V$ with $\chi_{V_i}$, i.e.,
        \begin{equation*}
            a_i = (\chi_V,\chi_{V_i})
        \end{equation*}
    \end{FHcorollary}
    \item With respect to the regular representation, \textcite{bib:FultonHarris} states that
    \begin{equation*}
        \chi_R(g) =
        \begin{cases}
            0 & g\neq e\\
            |G| & g=e
        \end{cases}
    \end{equation*}
    \item Matching the leftmost consequence of Corollary 3 from Friday's class.
    \begin{FHcorollary}
        Any irreducible representation $V$ of $G$ appears in the regular representation $\dim V$ times.
    \end{FHcorollary}
    \item Discusses the middle and rightmost consequences of Corollary 3 from Friday's class.
    \item Discusses Corollary \ref{cly:columnOrthoii} from Section 2.4 of \textcite{bib:Serre}.
    \item Note: These two formulas, copied below for convenience, amount to the \textbf{Fourier inversion formula} for finite groups.
    \begin{align*}
        |G| &= \sum_i\dim(V_i)^2&
        0 &= \sum_i(\dim V_i)\cdot\chi_{V_i}(g\neq e)
    \end{align*}
    \begin{itemize}
        \item Note: If all but one of the characters is known, they given a formula for the unknown character.
    \end{itemize}
    \item Matching the second orthogonality relation from Friday's class.
\end{itemize}


\subsection*{Section 2.3: Examples --- \texorpdfstring{$\bm{S_4}$}{TEXT} and \texorpdfstring{$\bm{A_4}$}{TEXT}}
\begin{itemize}
    \item Construction of Table \ref{tab:charTableS4}.
    \begin{enumerate}
        \item List the conjugacy classes in $S_4$ and the number of elements in each across the top of the table.
        \begin{itemize}
            \item Since this is a symmetric group $S_d$, the conjugacy classes correspond to the \textbf{partitions} of $d$ via cycle lengths.
            \item Thus, our conjugacy classes are\dots
            \begin{itemize}
                \item $\{e\}$: $4=1+1+1+1$. Number of elements: 1.
                \item $\{(xx)\}$: $4=2+1+1$. Number of elements: 6.
                \item $\{(xxx)\}$: $4=3+1$. Number of elements: 8.
                \item $\{(xx)(xx)\}$: $4=2+2$. Number of elements: 3.
                \item $\{(xxxx)\}$: $4=4$. Number of elements: 6.
            \end{itemize}
        \end{itemize}
        \item Start by listing the trivial, alternating, and standard representations.
        \begin{itemize}
            \item The character of the trivial is
            \begin{equation*}
                (1,1,1,1,1)
            \end{equation*}
            by default.
            \item The character of the alternating is
            \begin{equation*}
                ((-1)^e,(-1)^{(xx)},(-1)^{(xxx)},(-1)^{(xx)(xx)},(-1)^{(xxxx)}) = (1,-1,1,1,-1)
            \end{equation*}
            by the definition of the representation.
            \item The character of the standard representation is
            \begin{equation*}
                \chi_{\C^4}-\chi_{(5)} = (4,2,1,0,0)-(1,1,1,1,1)
                = (3,1,0,-1,-1)
            \end{equation*}
            by the fact that the regular representation always decomposes into the sum of the trivial and standard.
            \begin{itemize}
                \item We can double check that this representation is irreducible via the irreducibility criterion $|\chi_V|=\sqrt{(\chi_V,\chi_V)}=1$.
            \end{itemize}
        \end{itemize}
        \item Figure out how many more representations irreducible representations there are. In this case there are two more.
        \begin{itemize}
            \item We can figure this out by combining two previously used facts.
            \item First, we know that the sum of the squares of the dimensions must equal $|S_4|=24$ (middle consequence of Corollary 3 from Friday's class). Since we're only at $1+1+9=11$ so far, we still have $24-11=13$ to go. But how is this 13 allocated? To answer this question, we need the second fact.
            \item Second, by Corollary \ref{cly:lowerBoundConjCl}, the number of irreps is less than or equal to the number of conjugacy classes, so we have at most two irreps to go. In fact, since 13 is not the square of any natural number but $13=2^2+3^2$, we must have \emph{exactly} two irreps to go, of dimensions 2 and 3.
        \end{itemize}
        \item The tensor product of an irrep and a one-dimensional representation is irreducible, so $\text{Sign}\otimes V_\text{std}=(3,-1,0,-1,1)$ is one of them.
        \begin{itemize}
            \item Additional check: $|\text{Sign}\otimes V_\text{std}|=1$.
            \item Additional check: It is not a scalar multiple \emph{or} linear combination of any of the first three.
        \end{itemize}
        \item Use the second orthogonality relation to solve for each $\chi(g)$ for the final irrep.
    \end{enumerate}
    \item \textbf{Partition} (of $d$): An expression of $d$ as a sum of positive integers $a_1,\dots,a_k$.
    \item What is an "involution of trace 2" and how is it related to the quotient group \parencite[19]{bib:FultonHarris}??
    \begin{itemize}
        \item This is related to the closing comments of Friday's class.
    \end{itemize}
    \item Description of $A_4$.
\end{itemize}




\end{document}