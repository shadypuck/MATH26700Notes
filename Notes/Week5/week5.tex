\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{4}

\begin{document}




\chapter{???}
\section{Wedderburn-Artin Theory}
\begin{itemize}
    \item \marginnote{10/23:}Share notes with Rudenko at the end of the course!
    \item Today: Wedderburn-Artin theory.
    \begin{itemize}
        \item Noncommutative algebra.
        \item Noncommutative is a big part of math, partially because of its relation to QMech and partially because of its use in math, itself.
        \item There is a textbook: \textcite{bib:Lang}. It's a hard, grad-level textbook but very cleanly written. Not a bad book to have in our mind as we start to encounter category theory.
    \end{itemize}
    \item So here's what were talking about.
    \begin{itemize}
        \item Our main object is $A$, an \textbf{associative algebra} over a field $F$.
    \end{itemize}
    \item Left vs. right algebras.
    \begin{itemize}
        \item When $A$ is not commutative, we have to specify which we are dealing with.
        \item Let $A$ be an algebra over $F$.
        \item Recall left-modules and right-modules.
        \begin{itemize}
            \item In a left module, you can multiply $A\times M\to M$ where $(ab)m=a(bm)$.
            \item In a right module, $(ab)m=b(am)$. More simply, $m(ab)=(ma)b$.
            \item With modules, we get submodules, quotient modules, homomorphisms of modules, etc.
        \end{itemize}
        \item Let $I\subset A$ be a left-submodule. Thus, it is a subspace of $A$ such that for all $a\in A$, $aI\subset I$, i.e., a left ideal.
        \item In a right-submodule $I\subset A$, we have that for all $b\in A$, $Ib\subset I$, i.e., a right ideal.
        \item In a two-sided ideal $I\subset A$, we have for all $a,b\in I$ that $aI\subset I$ and $Ib\subset I$.
        \item Example: The matrix algebra is the prototypical noncommutative algebra. Consider $M_{2\times 2}(\C)$.
        \begin{itemize}
            \item Pick $v=(1,0)$.
            \item Look at ideal $I=\{X\in M_{2\times 2}\mid Xv=0\}$. This is called the \textbf{annihilator}, and it is a left ideal. Explicitly, this ideal is the subset of all matrices of the form
            \begin{equation*}
                \begin{pmatrix}
                    0 & a\\
                    0 & b\\
                \end{pmatrix}
            \end{equation*}
            for $a,b\in\C$.
            \item An example of a right ideal is all those such that $vX=0$, i.e., all matrices of the form
            \begin{equation*}
                \begin{pmatrix}
                    0 & 0\\
                    a & b\\
                \end{pmatrix}
            \end{equation*}
            \item There are \emph{no} two-sided ideals herein, save the trivial one.
        \end{itemize}
    \end{itemize}
    \item \textbf{Simple} (algebra): An algebra for which there are no nontrivial two-sided ideals.
    \item Every time you go more abstract, it's more boring because you have less things to play with, but we can derive more general rules.
    \begin{itemize}
        \item We'll only stay so abstract for 2-3 lectures.
    \end{itemize}
    \item We want to convert left-algebras to right-algebras.
    \begin{itemize}
        \item To do so, we can construct \textbf{opposite algebras}.
    \end{itemize}
    \item \textbf{Opposite algebra} (of $A$): The algebra with the same vector space structure as $A$, but with the reversed multiplication such that $a*b$ in this space yields $b*a$ in $A$. \emph{Denoted by} $\bm{A^\textbf{op}}$.
    \begin{itemize}
        \item Left ideals of $A$ become right ideals of $A^\text{op}$ and vice versa. Two-sided ideals stay the same. 
        \item In category theory, left-modules over $A$ are equivalent to right-modules over $A^\text{op}$.
        \item Opposite algebras are briefly defined on \textcite[308]{bib:FultonHarris} and are not defined anywhere else in any of the other sources.
    \end{itemize}
    \item Example: Consider $M_{n\times n}(F)^\text{op}$.
    \begin{itemize}
        \item Claim: This algebra equals regular $M_{n\times n}(F)$.
        \item The map between these spaces is $A\mapsto A^T$.
        \item There are other maps, such as conjugation and then transpose.
        \item Being isomorphic to your opposite is a strange and interesting property!
    \end{itemize}
    \item Example: $\C[G]^\text{op}\cong\C[G]$.
    \begin{itemize}
        \item Left as an exercise to find the map.
    \end{itemize}
    \item Let $M,N$ be modules. We now investigate some properties of $\Hom_A(M,N)$, a nice abelian group.
    \begin{itemize}
        \item Explicitly, it's
        \begin{equation*}
            \Hom_A(M,N) = \{f:M\to N\text{ linear}\mid f(am)=af(m)\ \forall\ a\in A\}
        \end{equation*}
        \item We have that
        \begin{equation*}
            \Hom_A(M_1\oplus M_2,N) \cong \Hom_A(M_1,N)\oplus\Hom_A(M_2,N)
        \end{equation*}
        \begin{itemize}
            \item Prove by looking at what happens to vectors of the form $(M_1,0)$ and $(0,M_2)$.
        \end{itemize}
        \item Similarly,
        \begin{equation*}
            \Hom_A(M,N_1\oplus N_2) \cong \Hom_A(M,N_1)\oplus\Hom_A(M,N_2)
        \end{equation*}
    \end{itemize}
    \item What if we have $\Hom(M_1\oplus\cdots\oplus M_n,N_1\oplus\cdots\oplus N_m)$?
    \begin{itemize}
        \item Then we have by induction from the previous cases that
        \begin{equation*}
            \Hom(M_1\oplus\cdots\oplus M_n,N_1\oplus\cdots\oplus N_m) = \bigoplus_{\substack{i=1,\dots,n\\j=1,\dots,m}}\Hom(M_i,N_j)
        \end{equation*}
        \item Let $\varphi_{ij}\in\Hom(M_i,N_j)$.
        \item At this point, it's very natural to write matrices
        \begin{equation*}
            \begin{bNiceMatrix}[first-row,first-col]
                 & {\color{white}n} & n & {\color{white}n}\\
                 & \Block{3-3}{\varphi_{ji}} & & \\
                m & & & \\
                 & & & \\
            \end{bNiceMatrix}
            \begin{pmatrix}
                m_1\\
                \vdots\\
                m_n\\
            \end{pmatrix}
            =
            \begin{pmatrix}
                \varphi_{11}(m_1)+\cdots+\varphi_{1n}(m_n)\\
                \vdots\\
                \\
            \end{pmatrix}
            =
            \begin{pmatrix}
                (\varphi(m))\\
                \vdots\\
                \\
            \end{pmatrix}
        \end{equation*}
        \begin{itemize}
            \item Is it $\phi_{ji}$ or $\phi_{ij}$?? \textcite[642]{bib:Lang} seems to back the latter.
        \end{itemize}
        \item To make this make sense for ourselves, write out the $2\times 2$ case from $M_1\oplus M_2\to M_1\oplus M_2$.
        \begin{equation*}
            \begin{pmatrix}
                \varphi_{11} & \varphi_{21}\\
                \varphi_{12} & \varphi_{22}\\
            \end{pmatrix}
            \begin{pmatrix}
                m_1\\
                m_2\\
            \end{pmatrix}
            =
            \begin{pNiceMatrix}
                \Block{2-2}{} & \\
                {\color{white}\varphi_{12}} & {\color{white}\varphi_{12}}\\
            \end{pNiceMatrix}
        \end{equation*}
        \item Matrices made out of maps can seem really confusing when you first start, but in time, it will make sense.
    \end{itemize}
    \item Recall the result from last time about division algebras.
    \item The main object we need to understand is a \textbf{semisimple algebra}.
    \item \textbf{Semisimple} (module): A module that satisfies any of the conditions in the following theorem.
    \begin{itemize}
        \item Note that we proved something analogous to condition 3 early on! This was the complements theorem.
        \item This is equivalent for infinite-dimensional algebras; we need \textbf{Zorn's lemma} regarding maximal ideals/the axiom of choice here, though.
    \end{itemize}
    \item Theorem: Let $A$ be an algebra over $F$, and let $M$ be a left-module. Then TFAE.
    \begin{enumerate}
        \item $M=\bigoplus_{i\in I}S_i$, where each $S_i$ is a simple module and $I$ is an \textbf{indexing set}, not a simple module/ideal.
        \item $M=\sum_{i\in I}S_i$, where the sum is \emph{not} direct.
        \item For all submodules $N\subset M$, there exists $N'$ such that $M=N\oplus N'$.
    \end{enumerate}
    \begin{proof}
        This proof only applies for the case that $M$ is finite dimensional; the theorem is more general than that, but we are not interested in the more general case.\par\smallskip
        ($1\Rightarrow 2$): Very clear; all direct sums are sums.\par\smallskip
        ($2\Rightarrow 1$): Consider the maximal subset $J\subset I$ (by inclusion, not by indices) of our indexing set such that
        \begin{equation*}
            \sum_{i\in J}S_i = \bigoplus_{i\in J}S_i
        \end{equation*}
        In other words, $J$ induces the highest-dimension sum of submodules that is a direct sum. Note that we can still find a singleton $J$ in the direct-sum-of-one-thing case, so we're starting from a good base case.\par
        Claim: $\bigoplus_{i\in J}S_i=M$. Suppose not. Then there exists $m\in M$ such that $m\notin\bigoplus_{i\in J}S_i$ and $m=s_{i_1}+\cdots+s_{i_k}$ where each $s_{i_j}\in S_{i_j}$. If all $s_{i_1},\dots,s_{i_k}\in\bigoplus_{i\in J}S_i$, then we have arrived at a contradiction and we are done. If not, then there exists some $s_{i_t}$ such that $s_{i_t}\notin\bigoplus_{i\in J}S_i$. Now consider $S_{i_t}\cap(\bigoplus_{i\in J}S_{i})$. This will be a submodule of $S_{i_t}$. But since $S_{i_t}$ is simple by hypothesis, this means that $S_{i_t}\cap(\bigoplus_{i\in J}S_{i})$ either equals $S_{i_t}$ or $0$. However, we know that it can't equal $S_{i_t}$ because above, we found $s_{i_t}\in S_{i_t}$ such that $s_{i_t}\notin\bigoplus_{i\in J}S_i$. Thus, $S_{i_t}\cap(\bigoplus_{i\in J}S_{i})=0$. But this means that $S_{i_t}+\bigoplus_{i\in J}S_i$ is a direct sum, which contradicts the choice of $J$ as maximal.\par\smallskip
        $(1\Rightarrow 3$): Let's take a submodule $N\subset M$. By 1, $M=\bigoplus_{i\in I}S_i$. Let's look a tall subsets $J$ such that
        \begin{equation*}
            N+\sum_{j\in J}S_j = N\oplus\left( \sum S_j \right)
        \end{equation*}
        Look at the maximal one by inclusion. Then once again, by the same proof strategy as above,
        \begin{equation*}
            N\oplus\underbrace{\left( \sum S_j \right)}_{N'} = M
        \end{equation*}\par
        $(3\Rightarrow 1)$: We use what we've learned about representations. Let $M=N_1\oplus N_2$. Then $N_2$, if nonsimple, has subsets $N_2\oplus N_3$. We can continue on and on. Because dimensions finitely decrease, we'll eventually have to arrive at a sum $N_1\oplus\cdots\oplus N_m$ of simples.
    \end{proof}
    \item Now, we have 3 definitions of semisimple modules.
    \item Corollary: If $A$ is an algebra, $M$ is a semisimple module, and $N\subset M$ is a submodule, then\dots
    \begin{enumerate}
        \item $N$ is semisimple.
        \begin{proof}
            Let $L$ be a submodule of $N$. We need to find a complement of $L$ inside $N$. We can find $L'\subset M$ such that $L\oplus L'=M$. Then $L'\cap N\subset N$ is the complement of $L$ in $N$. Why? Because of the following.\par
            Claim: $(L'\cap N)\bigoplus L=N$. Not intersecting: $L'\cap N\cap L\subset L'\cap L=0$. Summing to the whole thing: Let $n\in N$ be arbitrary. Then since $n\in M$, there exists $\ell,\ell'\in L,L'$ such that $n=\ell+\ell'$. But since $n,\ell\in N$, we must have $\ell'\in N$ as well. Therefore, $\ell'\in L'\cap N$.
        \end{proof}
        \item $M/N$ is semisimple.
    \end{enumerate}
    \item Takeaway: Submodules and quotient modules of semisimple modules are semisimple modules.
    \item \textcite{bib:Lang} has a write-up of the proof from today's class.
    \begin{itemize}
        \item Funnily enough, it is the only textbook that does! \textcite{bib:FultonHarris} doesn't have it; not even \textcite{bib:Etingof} has it!
    \end{itemize}
\end{itemize}



\section{Semisimple Algebras}
\begin{itemize}
    \item \marginnote{10/25:}More associative algebra today; we'll wrap it up next time.
    \item Review.
    \begin{itemize}
        \item Let $A$ be a finite dimensional associative algebra over a field $F$.
        \item We want to understand when this algebra is very close to a \emph{group algebra}.
        \begin{itemize}
            \item Recall that $A=F[G]=\{a_{g_1}g_1+\cdots+a_{g_n}g_n\mid a_i\in F\}$ is the group algebra of $G$ a finite group.
        \end{itemize}
        \item Recall left modules.
        \begin{itemize}
            \item These are very similar to representations.
            \item Indeed, if we have a left module $M$, then we have a multiplication map $\rho:A\times M\to M$ with properties such as associativity, etc.
        \end{itemize}
        \item Recall right modules.
        \begin{itemize}
            \item In a group representation, left modules over $A$ are essentially the same thing as right modules over $A^\text{op}$.
            \item Because there is a bijection between left modules over $A$ and right modules over $A^\text{op}$, we sometimes have the case where the thing doesn't change??
        \end{itemize}
        \item All of the above motivated the definition of \emph{semisimple}: If $A$ is a finite dimensional algebra and $M$ is a finite-dimensional module, then $M$ is \emph{semisimple} if it satisfies any one of three conditions from last time's theorem.
        \begin{itemize}
            \item Note: When we describe a module as "finite-dimensional," we mean this in the sense of a vector space, i.e., literally finite-dimensional as opposed to finitely generated or anything like that.
            \item Note: "Last time's theorem" refers to the semisimplicity conditions one, which is a part of Wedderburn-Artin theory but is \emph{not} the \textbf{Wedderburn-Artin \emph{theorem}}. We'll get to this theorem eventually, but that's still in the future.
        \end{itemize}
    \end{itemize}
    \item Theorem (Maschke's theorem): Let $G$ be a finite group and let $F$ be a field. Suppose $(|G|,\chr F)=1$, i.e., they are coprime. Then every finite-dimensional left module over $F[G]$ is semisimple.
    \begin{proof}
        We've already basically done this proof as part of last time's theorem. Here's a refresher, though.\par
        Let $M$ be an arbitrary finite-dimensional left module over $F[G]$. Then there exists a map $F[G]\to\End_{F[G]}(M)$, or $G\to GL(M)$. Thus, $M$ is a $G$-representation, which satisfies condition (3) from last time's theorem because of the complements theorem, stated as Theorem \ref{trm:complements} from \textcite{bib:Serre} for instance.
    \end{proof}
    \item Takeaway: The proof actually works for any field under this condition.
    \begin{itemize}
        \item Rudenko will reprove Maschke's theorem tomorrow a different way.
    \end{itemize}
    \item In an algebra, we have a multiplication map $\cdot:A\times A\to A$.
    \begin{itemize}
        \item If we take the perspective that this map defines an action of the left $A$ on the right one, we see that $A$ has the structure of a left $A$-module.
        \item Vice versa for right-modules.
    \end{itemize}
    \item \textbf{Semisimple} (algebra): An algebra for which every finite-dimensional $A$-module is semisimple. \emph{Also known as} \textbf{semi-simple}.
    \item Theorem: Let $A$ be a finite-dimensional associative algebra. Then TFAE.
    \begin{enumerate}
        \item $A$ is a semisimple algebra.
        \item $A$ is semisimple as a left-module over $A$. Equivalently, as an $A$-module, $A\cong S_1^{n_1}\oplus\cdots\oplus S_k^{n_k}$.
        \item (Wedderburn-Artin theorem) $A\cong M_{n_1}(D_1)\oplus\cdots\oplus M_{n_k}(D_k)$, where the $D_1,\dots,D_k$ are division algebras. Note that the isomorphism is an isomorphism of algebras.
    \end{enumerate}
    \item We will prove this theorem in just a moment, but there are a few preliminary comments to be made first.
    \item Let's look at the algebra $\HH$.
    \begin{itemize}
        \item We can create matrices of quaternions, and we can add and multiply these matrices just fine.
        \item However, the determinant is weirder: Is it $ad-bc$ or $ad-cb$?
        \begin{itemize}
            \item There is a theory of determinants of noncommutative fields called \textbf{algebraic $\bm{k}$-theory}, but we will not get into that.
        \end{itemize}
    \end{itemize}
    \item Example: Proving (3) for $\C[G]$.
    \begin{itemize}
        \item We have $\C[G]$. There are not many division algebras over complex numbers; only one, in fact: Complex numbers.
        \item Let $V_1,\dots,V_k$ be the irreps. Then we want to show that
        \begin{equation*}
            \C[G] \cong M_{d_1}(\C)\oplus\cdots\oplus M_{d_k}(\C)
        \end{equation*}
        where $d_i=\deg V_i$.
        \item Note: Matrices give us a nice way to compute otherwise complicated elements of $\C[G]$.
        \item Proof: Define a map $F:\C[G]\to M_{d_1}(\C)\oplus\cdots\oplus M_{d_k}(\C)$ by
        \begin{equation*}
            x \mapsto (\rho_{V_1}(x),\dots,\rho_{V_k}(x))
        \end{equation*}
        \begin{itemize}
            \item $F$ is injective: $F(x)=0$ implies that $\rho_{V_i}(x)=0$ ($i=1,\dots,k$), so $xV_i=0$ ($i=1,\dots,k$). In particular, this means that $x=x\cdot 1=0$.
            \item $F$ is surjective: $F$ is injective and $\dim(\C[G])=\sum d_i^2=\dim[M_{d_1}(\C)\oplus\cdots\oplus M_{d_k}(\C)]$.
            \item $F$ is a homomorphism of algebras: Left as an exercise.
        \end{itemize}
        \item Note: Remember this theorem very well because it allows you to treat group rings very easily.
        \item Tomorrow, we'll bring characters into this picture.
    \end{itemize}
    \item We now state a lemma that will be used to prove $2\Rightarrow 3$.
    \item Lemma: Let $\End_A(A)$ denote the set of $A$-module endomorphisms of $A$. Then
    \begin{equation*}
        \End_A(A) \cong A^\text{op}
    \end{equation*}
    as algebras.
    \begin{proof}
        To prove the claim, it will suffice to construct an $A$-algebra isomorphism $F:\End_A(A)\to A^\text{op}$. Define $F$ by
        \begin{equation*}
            F(f) := f(1)
        \end{equation*}
        for all $f\in\End_A(A)$. It should be fairly clear that
        \begin{align*}
            F(f+g) &= F(f)+F(g)&
            F(1) &= 1
        \end{align*}
        Proving that $F(f\circ g)=F(f)*F(g)$ is slightly more involved, but can be done as follows.
        \begin{equation*}
            F(f\circ g) = [f\circ g](1)
            = f(g(1))
            = f(g(1)\cdot 1)
            = g(1)\cdot f(1)
            = F(g)\cdot F(f)
            = F(f)*F(g)
        \end{equation*}
        Lastly, by plugging $f=a=aI$ and $g=f$ into the above, we can recover
        \begin{equation*}
            F(af) = a*F(f)
        \end{equation*}
        Thus, $F$ is an $A$-algebra \emph{homomorphism}. To prove that it is an \emph{isomorphism}, consider the inverse map $G:x\mapsto[a\mapsto ax]$. We can show that $F\circ G=1_{A^\text{op}}$ and $G\circ F=1_{\End_A(A)}$, thus completing the proof.
    \end{proof}
    \item We now prove the above theorem, which we restate for simplicity.
    \item Theorem: Let $A$ be a finite-dimensional associative algebra over $F$. Then TFAE.
    \begin{enumerate}
        \item $A$ is a semisimple algebra.
        \item $A$ is semisimple as a left-module over $A$. Equivalently, as an $A$-module, $A\cong S_1^{n_1}\oplus\cdots\oplus S_k^{n_k}$.
        \item (Wedderburn-Artin theorem) $A\cong M_{n_1}(D_1)\oplus\cdots\oplus M_{n_k}(D_k)$, where the $D_1,\dots,D_k$ are division algebras. Note that the isomorphism is an isomorphism of algebras.
    \end{enumerate}
    \begin{proof}
        One line; very simple, but a little weird conceptually.\par
        $(2\Rightarrow 1)$: To prove that $A$ is a semisimple algebra, it will suffice to show that every finite-dimensional $A$-module over $F$ is semisimple. Let $M=Fe_1+\cdots+Fe_n$ be an arbitrary finite-dimensional $A$-module. To show that it's semisimple, it will suffice to demonstrate that it's equal to the direct sum of simple modules. Define a map $A^n\to M$ by
        \begin{equation*}
            (a_1,\dots,a_n) \mapsto a_1e_1+\cdots+a_ne_n
        \end{equation*}
        This should (fairly clearly) be a surjective homomorphism of left-modules (how do we know that all $a_i\in F$??). Moreover, since $A=S_1\oplus\cdots\oplus S_k$ is semisimple as a left $A$-module by hypothesis, we have that $A^n=S_1^n\oplus\cdots\oplus S_k^n$ (how does this imply that $M$ is equal to the direct sum of simple modules??).\par
        $(3\Rightarrow 2)$: Work it out in the HW!\par
        $(2\Rightarrow 3)$: Let's take $A=S_1^{n_1}\oplus\cdots\oplus S_k^{n_k}$ a left $A$-module where each $S_i$ is simple. Then by the lemma,
        \begin{equation*}
            A^\text{op} \cong \End_A(A)
            = \Hom_A(A,A)
            = \Hom_A(S_1^{n_1}\oplus\cdots\oplus S_k^{n_k},S_1^{n_1}\oplus\cdots\oplus S_k^{n_k})
            = \bigoplus_{i,j=1}^k\Hom_A(S_i^{n_i},S_j^{n_j})
        \end{equation*}
        By Schur's lemma for associative algebras,
        \begin{equation*}
            \Hom_A(S_i,S_j) =
            \begin{cases}
                0 & i\neq j\\
                D_i & i=j
            \end{cases}
        \end{equation*}
        where each $D_i$ is a division algebra. Thus, continuing from the above,
        \begin{equation*}
            A^\text{op} \cong \bigoplus_{i=1}^k\Hom_A(S_i^{n_i},S_i^{n_i})
            = \bigoplus_{i=1}^kM_{n_i}(\Hom_A(S_i,S_i))
            = \bigoplus_{i=1}^kM_{n_i}(D_i)
        \end{equation*}
    \end{proof}
    \item Consequence: It follows because the $D_i$'s are division algebras that
    \begin{equation*}
        A \cong \bigoplus_{i=1}^kM_{n_i}(D_i^\text{op})
    \end{equation*}
    \begin{itemize}
        \item What was the point of this??
    \end{itemize}
    \item Note from last time that we forgot to discuss: A quotient module of a semisimple module is semisimple. Proving this will be in the next HW.
    \item \textbf{Radical} (of $A$): The finite dimensional $A$-algebra defined as follows. \emph{Also known as} \textbf{Jacobson ideal}, \textbf{Jacobson radical}. %Added during the 10/27 class.
    \emph{Denoted by} $\bm{\Rad(A)}$. \emph{Given by}
    \begin{equation*}
        \Rad(A) = \{a\in A\mid aS=0\text{ for any simple module }S\} \subset A
    \end{equation*}
    \begin{itemize}
        \item Immediate fact: $\Rad(A)$ is a two-sided ideal.
        \item This is because\dots
        \begin{itemize}
            \item $x\in A$ and $a\in\Rad(A)$ $\Longrightarrow$ $(xa)S=x(aS)=x(0)=0$ $\Longrightarrow$ $xa\in\Rad(A)$;
            \item $x\in A$ and $a\in\Rad(A)$ $\Longrightarrow$ $(ax)S=a(xS)=0$ $\Longrightarrow$ $ax\in\Rad(A)$.
        \end{itemize}
        \item Note that $xS$ is simple in the above line because a scaled simple module is still simple.
    \end{itemize}
    \item Theorem: $A$ is semisimple iff $\Rad(A)=0$.
    \begin{itemize}
        \item This will be explained next time.
        \item In other words, if there are problematic elements, the algebra is not semisimple.
        \item Quotienting algebras by two-sided ideals gives algebras, so if $A$ is not semisimple, we know that $A/\Rad(A)$ \emph{is} semisimple!
    \end{itemize}
    \item This week: A brief primer on noncommutative algebra that is probably worth studying for the midterm.
    \item Next week: Number theoretic stuff, integer elements, groups, etc.
    \item Most people/books don't treat the finite-dimensional case here (so it's not written up anywhere) because they view it as too restrictive; instead, they prefer to use the \textbf{Artinian} condition.
\end{itemize}




\end{document}