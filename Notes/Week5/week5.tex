\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{4}

\begin{document}




\chapter{Associative Algebras}
\section{Wedderburn-Artin Theory}
\begin{itemize}
    \item \marginnote{10/23:}Share notes with Rudenko at the end of the course!
    \item Today: Wedderburn-Artin theory.
    \begin{itemize}
        \item Noncommutative algebra.
        \item Noncommutative is a big part of math, partially because of its relation to QMech and partially because of its use in math, itself.
        \item There is a textbook: \textcite{bib:Lang}. It's a hard, grad-level textbook but very cleanly written. Not a bad book to have in our mind as we start to encounter category theory.
    \end{itemize}
    \item So here's what were talking about.
    \begin{itemize}
        \item Our main object is $A$, an \textbf{associative algebra} over a field $F$.
    \end{itemize}
    \item Left vs. right algebras.
    \begin{itemize}
        \item When $A$ is not commutative, we have to specify which we are dealing with.
        \item Let $A$ be an algebra over $F$.
        \item Recall left-modules and right-modules.
        \begin{itemize}
            \item In a left module, you can multiply $A\times M\to M$ where $(ab)m=a(bm)$.
            \item In a right module, $(ab)m=b(am)$. More simply, $m(ab)=(ma)b$.
            \item With modules, we get submodules, quotient modules, homomorphisms of modules, etc.
        \end{itemize}
        \item Let $I\subset A$ be a left-submodule. Thus, it is a subspace of $A$ such that for all $a\in A$, $aI\subset I$, i.e., a left ideal.
        \item In a right-submodule $I\subset A$, we have that for all $b\in A$, $Ib\subset I$, i.e., a right ideal.
        \item In a two-sided ideal $I\subset A$, we have for all $a,b\in I$ that $aI\subset I$ and $Ib\subset I$.
        \item Example: The matrix algebra is the prototypical noncommutative algebra. Consider $M_{2\times 2}(\C)$.
        \begin{itemize}
            \item Pick $v=(1,0)$.
            \item Look at ideal $I=\{X\in M_{2\times 2}\mid Xv=0\}$. This is called the \textbf{annihilator}, and it is a left ideal. Explicitly, this ideal is the subset of all matrices of the form
            \begin{equation*}
                \begin{pmatrix}
                    0 & a\\
                    0 & b\\
                \end{pmatrix}
            \end{equation*}
            for $a,b\in\C$.
            \item An example of a right ideal is all those such that $vX=0$, i.e., all matrices of the form
            \begin{equation*}
                \begin{pmatrix}
                    0 & 0\\
                    a & b\\
                \end{pmatrix}
            \end{equation*}
            \begin{itemize}
                \item Note that we are treating $v$ as a row vector here.
            \end{itemize}
            \item There are \emph{no} two-sided ideals herein, save the trivial one.
        \end{itemize}
    \end{itemize}
    \item \textbf{Simple} (algebra): An algebra for which there are no nontrivial two-sided ideals.
    \item Every time you go more abstract, it's more boring because you have less things to play with, but we can derive more general rules.
    \begin{itemize}
        \item We'll only stay so abstract for 2-3 lectures.
    \end{itemize}
    \item We want to convert left-algebras to right-algebras.
    \begin{itemize}
        \item To do so, we can construct \textbf{opposite algebras}.
    \end{itemize}
    \item \textbf{Opposite algebra} (of $A$): The algebra with the same vector space structure as $A$, but with the reversed multiplication such that $a*b$ in this space yields $b*a$ in $A$. \emph{Denoted by} $\bm{A^\textbf{op}}$.
    \begin{itemize}
        \item Left ideals of $A$ become right ideals of $A^\text{op}$ and vice versa. Two-sided ideals stay the same. 
        \item In category theory, left-modules over $A$ are equivalent to right-modules over $A^\text{op}$.
        \item Opposite algebras are briefly defined on \textcite[308]{bib:FultonHarris} and are not defined anywhere else in any of the other sources.
    \end{itemize}
    \item Example: Consider $M_{n\times n}(F)^\text{op}$.
    \begin{itemize}
        \item Claim: This algebra equals regular $M_{n\times n}(F)$.
        \item The map between these spaces is $A\mapsto A^T$.
        \item There are other maps, such as conjugation and then transpose.
        \item Being isomorphic to your opposite is a strange and interesting property!
    \end{itemize}
    \item Example: $\C[G]^\text{op}\cong\C[G]$.
    \begin{itemize}
        \item Left as an exercise to find the map.
    \end{itemize}
    \item Let $M,N$ be modules. We now investigate some properties of $\Hom_A(M,N)$, a nice abelian group.
    \begin{itemize}
        \item Explicitly, it's
        \begin{equation*}
            \Hom_A(M,N) = \{f:M\to N\text{ linear}\mid f(am)=af(m)\ \forall\ a\in A\}
        \end{equation*}
        \item We have that
        \begin{equation*}
            \Hom_A(M_1\oplus M_2,N) \cong \Hom_A(M_1,N)\oplus\Hom_A(M_2,N)
        \end{equation*}
        \begin{itemize}
            \item Prove by looking at what happens to vectors of the form $(M_1,0)$ and $(0,M_2)$.
        \end{itemize}
        \item Similarly,
        \begin{equation*}
            \Hom_A(M,N_1\oplus N_2) \cong \Hom_A(M,N_1)\oplus\Hom_A(M,N_2)
        \end{equation*}
    \end{itemize}
    \item What if we have $\Hom(M_1\oplus\cdots\oplus M_n,N_1\oplus\cdots\oplus N_m)$?
    \begin{itemize}
        \item Then we have by induction from the previous cases that
        \begin{equation*}
            \Hom(M_1\oplus\cdots\oplus M_n,N_1\oplus\cdots\oplus N_m) = \bigoplus_{\substack{i=1,\dots,n\\j=1,\dots,m}}\Hom(M_i,N_j)
        \end{equation*}
        \item Let $\varphi_{ij}\in\Hom(M_i,N_j)$.
        \item At this point, it's very natural to write matrices
        \begin{equation*}
            \begin{bNiceMatrix}[first-row,first-col]
                 & {\color{white}n} & n & {\color{white}n}\\
                 & \Block{3-3}{\varphi_{ji}} & & \\
                m & & & \\
                 & & & \\
            \end{bNiceMatrix}
            \begin{pmatrix}
                m_1\\
                \vdots\\
                m_n\\
            \end{pmatrix}
            =
            \begin{pmatrix}
                \varphi_{11}(m_1)+\cdots+\varphi_{1n}(m_n)\\
                \vdots\\
                \\
            \end{pmatrix}
            =
            \begin{pmatrix}
                (\varphi(m))\\
                \vdots\\
                \\
            \end{pmatrix}
        \end{equation*}
        \begin{itemize}
            \item Is it $\phi_{ji}$ or $\phi_{ij}$?? \textcite[642]{bib:Lang} seems to back the latter.
        \end{itemize}
        \item To make this make sense for ourselves, write out the $2\times 2$ case from $M_1\oplus M_2\to M_1\oplus M_2$.
        \begin{equation*}
            \begin{pmatrix}
                \varphi_{11} & \varphi_{21}\\
                \varphi_{12} & \varphi_{22}\\
            \end{pmatrix}
            \begin{pmatrix}
                m_1\\
                m_2\\
            \end{pmatrix}
            =
            \begin{pNiceMatrix}
                \Block{2-2}{} & \\
                {\color{white}\varphi_{12}} & {\color{white}\varphi_{12}}\\
            \end{pNiceMatrix}
        \end{equation*}
        \item Matrices made out of maps can seem really confusing when you first start, but in time, it will make sense.
    \end{itemize}
    \item Recall the result from last time about division algebras.
    \item The main object we need to understand is a \textbf{semisimple algebra}.
    \item \textbf{Semisimple} (module): A module that satisfies any of the conditions in the following theorem.
    \begin{itemize}
        \item Note that we proved something analogous to condition 3 early on! This was the complements theorem.
        \item There is an equivalent for infinite-dimensional algebras; we need \textbf{Zorn's lemma} regarding maximal ideals/the axiom of choice here, though.
    \end{itemize}
    \item Theorem: Let $A$ be an algebra over $F$, and let $M$ be a left-module. Then TFAE.
    \begin{enumerate}
        \item $M=\bigoplus_{i\in I}S_i$, where each $S_i$ is a simple module and $I$ is an \textbf{indexing set}, not a simple module/ideal.
        \item $M=\sum_{i\in I}S_i$, where the sum is \emph{not} direct.
        \item For all submodules $N\subset M$, there exists $N'$ such that $M=N\oplus N'$.
    \end{enumerate}
    \begin{proof}
        This proof only applies for the case that $M$ is finite dimensional; the theorem is more general than that, but we are not interested in the more general case.\par\smallskip
        ($1\Rightarrow 2$): Very clear; all direct sums are sums.\par\smallskip
        ($2\Rightarrow 1$): Consider the maximal subset $J\subset I$ (by inclusion, not by indices) of our indexing set such that
        \begin{equation*}
            \sum_{i\in J}S_i = \bigoplus_{i\in J}S_i
        \end{equation*}
        In other words, $J$ induces the highest-dimension sum of submodules that is a direct sum. Note that we can still find a singleton $J$ in the direct-sum-of-one-thing case, so we're starting from a good base case.\par
        Claim: $\bigoplus_{i\in J}S_i=M$. Suppose not. Then there exists $m\in M$ such that $m\notin\bigoplus_{i\in J}S_i$ and $m=s_{i_1}+\cdots+s_{i_k}$ where each $s_{i_j}\in S_{i_j}$. If all $s_{i_1},\dots,s_{i_k}\in\bigoplus_{i\in J}S_i$, then we have arrived at a contradiction and we are done. If not, then there exists some $s_{i_t}$ such that $s_{i_t}\notin\bigoplus_{i\in J}S_i$. Now consider $S_{i_t}\cap(\bigoplus_{i\in J}S_{i})$. This will be a submodule of $S_{i_t}$. But since $S_{i_t}$ is simple by hypothesis, this means that $S_{i_t}\cap(\bigoplus_{i\in J}S_{i})$ either equals $S_{i_t}$ or $0$. However, we know that it can't equal $S_{i_t}$ because above, we found $s_{i_t}\in S_{i_t}$ such that $s_{i_t}\notin\bigoplus_{i\in J}S_i$. Thus, $S_{i_t}\cap(\bigoplus_{i\in J}S_{i})=0$. But this means that $S_{i_t}+\bigoplus_{i\in J}S_i$ is a direct sum, which contradicts the choice of $J$ as maximal.\par\smallskip
        $(1\Rightarrow 3$): Let's take a submodule $N\subset M$. By 1, $M=\bigoplus_{i\in I}S_i$. Let's look at all subsets $J$ such that
        \begin{equation*}
            N+\sum_{j\in J}S_j = N\oplus\left( \sum S_j \right)
        \end{equation*}
        Look at the maximal one by inclusion. Then once again, by the same proof strategy as above,
        \begin{equation*}
            N\oplus\underbrace{\left( \sum S_j \right)}_{N'} = M
        \end{equation*}\par
        $(3\Rightarrow 1)$: We use what we've learned about representations. Let $M=N_1\oplus N_2$. Then $N_2$, if nonsimple, has subsets $N_2\oplus N_3$. We can continue on and on. Because dimensions finitely decrease, we'll eventually have to arrive at a sum $N_1\oplus\cdots\oplus N_m$ of simples.
    \end{proof}
    \item Now, we have 3 definitions of semisimple modules.
    \item Corollary: If $A$ is an algebra, $M$ is a semisimple module, and $N\subset M$ is a submodule, then\dots
    \begin{enumerate}
        \item $N$ is semisimple.
        \begin{proof}
            Let $L$ be a submodule of $N$. We need to find a complement of $L$ inside $N$. We can find $L'\subset M$ such that $L\oplus L'=M$. Then $L'\cap N\subset N$ is the complement of $L$ in $N$. Why? Because of the following.\par
            Claim: $(L'\cap N)\bigoplus L=N$. Not intersecting: $L'\cap N\cap L\subset L'\cap L=0$. Summing to the whole thing: Let $n\in N$ be arbitrary. Then since $n\in M$, there exists $\ell,\ell'\in L,L'$ such that $n=\ell+\ell'$. But since $n,\ell\in N$, we must have $\ell'\in N$ as well. Therefore, $\ell'\in L'\cap N$.
        \end{proof}
        \item $M/N$ is semisimple.
    \end{enumerate}
    \item Takeaway: Submodules and quotient modules of semisimple modules are semisimple modules.
    \item \textcite{bib:Lang} has a write-up of the proof from today's class.
    \begin{itemize}
        \item Funnily enough, it is the only textbook that does! \textcite{bib:FultonHarris} doesn't have it; not even \textcite{bib:Etingof} has it!
    \end{itemize}
\end{itemize}



\section{Semisimple Algebras}
\begin{itemize}
    \item \marginnote{10/25:}More associative algebra today; we'll wrap it up next time.
    \item Review.
    \begin{itemize}
        \item Let $A$ be a finite dimensional associative algebra over a field $F$.
        \item We want to understand when this algebra is very close to a \emph{group algebra}.
        \begin{itemize}
            \item Recall that $A=F[G]=\{a_{g_1}g_1+\cdots+a_{g_n}g_n\mid a_i\in F\}$ is the group algebra of $G$ a finite group.
        \end{itemize}
        \item Recall left modules.
        \begin{itemize}
            \item These are very similar to representations.
            \item Indeed, if we have a left module $M$, then we have a multiplication map $\rho:A\times M\to M$ with properties such as associativity, etc.
        \end{itemize}
        \item Recall right modules.
        \begin{itemize}
            \item In a group representation, left modules over $A$ are essentially the same thing as right modules over $A^\text{op}$.
            \item Because there is a bijection between left modules over $A$ and right modules over $A^\text{op}$, we sometimes have the case where $A$ doesn't change, i.e., $A\cong A^\text{op}$.
        \end{itemize}
        \item All of the above motivated the definition of \emph{semisimple}: If $A$ is a finite dimensional algebra and $M$ is a finite-dimensional module, then $M$ is \emph{semisimple} if it satisfies any one of three conditions from last time's theorem.
        \begin{itemize}
            \item Note: When we describe a module as "finite-dimensional," we mean this in the sense of a vector space, i.e., literally finite-dimensional as opposed to finitely generated or anything like that.
            \item Note: "Last time's theorem" refers to the semisimplicity conditions one, which is a part of Wedderburn-Artin theory but is \emph{not} the \textbf{Wedderburn-Artin \emph{theorem}}. We'll get to this theorem eventually, but that's still in the future.
        \end{itemize}
    \end{itemize}
    \item Theorem (Maschke's theorem): Let $G$ be a finite group and let $F$ be a field. Suppose $(|G|,\chr F)=1$, i.e., they are coprime. Then every finite-dimensional left module over $F[G]$ is semisimple.
    \begin{proof}
        We've already basically done this proof as part of last time's theorem. Here's a refresher, though.\par
        Let $M$ be an arbitrary finite-dimensional left module over $F[G]$. Then there exists a map $F[G]\to\End_{F[G]}(M)$ (left multiplication; the action of elements of this ring on elements of $M$), or $G\to GL(M)$. Thus, $M$ is a $G$-representation, which satisfies condition (3) from last time's theorem because of the complements theorem, stated as Theorem \ref{trm:complements} from \textcite{bib:Serre} for instance.
    \end{proof}
    \item Takeaway: The proof actually works for any field under this condition.
    \begin{itemize}
        \item Rudenko will reprove Maschke's theorem tomorrow a different way.
    \end{itemize}
    \item In an algebra, we have a multiplication map $\cdot:A\times A\to A$.
    \begin{itemize}
        \item If we take the perspective that this map defines an action of the left $A$ on the right one, we see that $A$ has the structure of a left $A$-module.
        \item Vice versa for right-modules.
    \end{itemize}
    \item \textbf{Semisimple} (algebra): An algebra for which every finite-dimensional $A$-module is semisimple. \emph{Also known as} \textbf{semi-simple}.
    \item Theorem: Let $A$ be a finite-dimensional associative algebra. Then TFAE.
    \begin{enumerate}
        \item $A$ is a semisimple algebra.
        \item $A$ is semisimple as a left-module over $A$. Equivalently, as an $A$-module, $A\cong S_1^{n_1}\oplus\cdots\oplus S_k^{n_k}$.
        \item (Wedderburn-Artin theorem) $A\cong M_{n_1}(D_1)\oplus\cdots\oplus M_{n_k}(D_k)$, where the $D_1,\dots,D_k$ are division algebras. Note that the isomorphism is an isomorphism of algebras.
    \end{enumerate}
    \item We will prove this theorem in just a moment, but there are a few preliminary comments to be made first.
    \item Let's look at the algebra $\HH$.
    \begin{itemize}
        \item We can create matrices of quaternions, and we can add and multiply these matrices just fine.
        \item However, the determinant is weirder: Is it $ad-bc$ or $ad-cb$?
        \begin{itemize}
            \item There is a theory of determinants of noncommutative fields called \textbf{algebraic $\bm{k}$-theory}, but we will not get into that.
        \end{itemize}
    \end{itemize}
    \item Example: Proving (3) for $\C[G]$.
    \begin{itemize}
        \item We have $\C[G]$. There are not many division algebras over complex numbers; only one, in fact: Complex numbers.
        \item Let $V_1,\dots,V_k$ be the irreps. Then we want to show that
        \begin{equation*}
            \C[G] \cong M_{d_1}(\C)\oplus\cdots\oplus M_{d_k}(\C)
        \end{equation*}
        where $d_i=\deg V_i$.
        \item Note: Matrices give us a nice way to compute otherwise complicated elements of $\C[G]$.
        \item Proof: Define a map $F:\C[G]\to M_{d_1}(\C)\oplus\cdots\oplus M_{d_k}(\C)$ by
        \begin{equation*}
            x \mapsto (\rho_{V_1}(x),\dots,\rho_{V_k}(x))
        \end{equation*}
        \begin{itemize}
            \item $F$ is injective: $F(x)=0$ implies that $\rho_{V_i}(x)=0$ ($i=1,\dots,k$), so $xV_i=0$ ($i=1,\dots,k$). In particular, this means that $x=x\cdot 1=0$.
            \item $F$ is surjective: $F$ is injective and $\dim(\C[G])=\sum d_i^2=\dim[M_{d_1}(\C)\oplus\cdots\oplus M_{d_k}(\C)]$.
            \item $F$ is a homomorphism of algebras: Left as an exercise.
        \end{itemize}
        \item Note: Remember this theorem very well because it allows you to treat group rings very easily.
        \item Tomorrow, we'll bring characters into this picture.
    \end{itemize}
    \item We now state a lemma that will be used to prove $2\Rightarrow 3$.
    \item Lemma: Let $\End_A(A)$ denote the set of $A$-module endomorphisms of $A$. Then
    \begin{equation*}
        \End_A(A) \cong A^\text{op}
    \end{equation*}
    as algebras.
    \begin{proof}
        To prove the claim, it will suffice to construct an $A$-algebra isomorphism $F:\End_A(A)\to A^\text{op}$. Define $F$ by
        \begin{equation*}
            F(f) := f(1)
        \end{equation*}
        for all $f\in\End_A(A)$. It should be fairly clear that
        \begin{align*}
            F(f+g) &= F(f)+F(g)&
            F(1) &= 1
        \end{align*}
        Proving that $F(f\circ g)=F(f)*F(g)$ is slightly more involved, but can be done as follows.
        \begin{equation*}
            F(f\circ g) = [f\circ g](1)
            = f(g(1))
            = f(g(1)\cdot 1)
            = g(1)\cdot f(1)
            = F(g)\cdot F(f)
            = F(f)*F(g)
        \end{equation*}
        Lastly, by plugging $f=a=aI$ and $g=f$ into the above, we can recover
        \begin{equation*}
            F(af) = a*F(f)
        \end{equation*}
        Thus, $F$ is an $A$-algebra \emph{homomorphism}. To prove that it is an \emph{isomorphism}, consider the inverse map $G:x\mapsto[a\mapsto ax]$. We can show that $F\circ G=1_{A^\text{op}}$ and $G\circ F=1_{\End_A(A)}$, thus completing the proof.
    \end{proof}
    \item We now prove the above theorem, which we restate for simplicity.
    \item Theorem: Let $A$ be a finite-dimensional associative algebra over $F$. Then TFAE.
    \begin{enumerate}
        \item $A$ is a semisimple algebra.
        \item $A$ is semisimple as a left-module over $A$. Equivalently, as an $A$-module, $A\cong S_1^{n_1}\oplus\cdots\oplus S_k^{n_k}$.
        \item (Wedderburn-Artin theorem) $A\cong M_{n_1}(D_1)\oplus\cdots\oplus M_{n_k}(D_k)$, where the $D_1,\dots,D_k$ are division algebras. Note that the isomorphism is an isomorphism of algebras.
    \end{enumerate}
    \begin{proof}
        One line; very simple, but a little weird conceptually.\par
        $(2\Rightarrow 1)$: To prove that $A$ is a semisimple algebra, it will suffice to show that every finite-dimensional $A$-module is semisimple. Let $M=Ae_1+\cdots+Ae_n$ be an arbitrary finite-dimensional $A$-module. To show that it's semisimple, it will suffice to demonstrate that it's equal to the direct sum of simple modules. Define a map $A^n\to M$ by
        \begin{equation*}
            (a_1,\dots,a_n) \mapsto a_1e_1+\cdots+a_ne_n
        \end{equation*}
        This should (fairly clearly) be a surjective homomorphism of left $A$-modules. Moreover, since $A=S_1\oplus\cdots\oplus S_k$ is semisimple as a left $A$-module by hypothesis, we have that $A^n=S_1^n\oplus\cdots\oplus S_k^n$. Since the map defined above is also injective, it follows that
        \begin{equation*}
            M \cong A^n = S_1^n\oplus\cdots\oplus S_k^n
        \end{equation*}
        as desired.\par
        $(3\Rightarrow 2)$: Work it out in the HW!\par
        $(2\Rightarrow 3)$: Let's take $A=S_1^{n_1}\oplus\cdots\oplus S_k^{n_k}$ a left $A$-module where each $S_i$ is simple. Then by the lemma,
        \begin{equation*}
            A^\text{op} \cong \End_A(A)
            = \Hom_A(A,A)
            = \Hom_A(S_1^{n_1}\oplus\cdots\oplus S_k^{n_k},S_1^{n_1}\oplus\cdots\oplus S_k^{n_k})
            = \bigoplus_{i,j=1}^k\Hom_A(S_i^{n_i},S_j^{n_j})
        \end{equation*}
        By Schur's lemma for associative algebras,
        \begin{equation*}
            \Hom_A(S_i,S_j) =
            \begin{cases}
                0 & i\neq j\\
                D_i & i=j
            \end{cases}
        \end{equation*}
        where each $D_i$ is a division algebra. Thus, continuing from the above,
        \begin{equation*}
            A^\text{op} \cong \bigoplus_{i=1}^k\Hom_A(S_i^{n_i},S_i^{n_i})
            = \bigoplus_{i=1}^kM_{n_i}(\Hom_A(S_i,S_i))
            = \bigoplus_{i=1}^kM_{n_i}(D_i)
        \end{equation*}
        Note that $\Hom_A(S_i^{n_i},S_i^{n_i})=M_{n_i}(\Hom_A(S_i,S_i))$ because of the thing about homomorphisms of direct sums of modules equaling matrices of homomorphisms. This was discussed on Monday. We don't include any $\Hom_A(S_i,S_j)$ because all of these are equal to zero; indeed, it appears that these matrices will be strictly diagonal.
    \end{proof}
    \item Consequence: It follows because the $D_i$'s are division algebras that
    \begin{equation*}
        A \cong \bigoplus_{i=1}^kM_{n_i}(D_i^\text{op})
    \end{equation*}
    \begin{itemize}
        \item What was the point of this??
    \end{itemize}
    \item Note from last time that we forgot to discuss: A quotient module of a semisimple module is semisimple. Proving this will be in the next HW.
    \item \textbf{Radical} (of $A$): The finite dimensional $A$-algebra defined as follows. \emph{Also known as} \textbf{Jacobson ideal}, \textbf{Jacobson radical}. %Added during the 10/27 class.
    \emph{Denoted by} $\bm{\Rad(A)}$. \emph{Given by}
    \begin{equation*}
        \Rad(A) = \{a\in A\mid aS=0\text{ for any simple module }S\} \subset A
    \end{equation*}
    \begin{itemize}
        \item Immediate fact: $\Rad(A)$ is a two-sided ideal.
        \item This is because\dots
        \begin{itemize}
            \item $x\in A$ and $a\in\Rad(A)$ $\Longrightarrow$ $(xa)S=x(aS)=x(0)=0$ $\Longrightarrow$ $xa\in\Rad(A)$;
            \item $x\in A$ and $a\in\Rad(A)$ $\Longrightarrow$ $(ax)S=a(xS)=0$ $\Longrightarrow$ $ax\in\Rad(A)$.
        \end{itemize}
        \item Note that $xS$ is simple in the above line because a scaled simple module is still simple.
    \end{itemize}
    \item Theorem: $A$ is semisimple iff $\Rad(A)=0$.
    \begin{itemize}
        \item This will be explained next time.
        \item In other words, if there are problematic elements, the algebra is not semisimple.
        \item Quotienting algebras by two-sided ideals gives algebras, so if $A$ is not semisimple, we know that $A/\Rad(A)$ \emph{is} semisimple!
    \end{itemize}
    \item This week: A brief primer on noncommutative algebra that is probably worth studying for the midterm.
    \item Next week: Number theoretic stuff, integer elements, groups, etc.
    \item Most people/books don't treat the finite-dimensional case here (so it's not written up anywhere) because they view it as too restrictive; instead, they prefer to use the \textbf{Artinian} condition.
\end{itemize}



\section{The Jacobson Radical}
\begin{itemize}
    \item \marginnote{10/27:}Review.
    \begin{itemize}
        \item Let $A$ be an associative algebra over a field $F$. $\dim_FA=\infty$ (??).
        \item $A$ is semisimple if every left $A$-module is a sum of simple $A$-modules.
        \item Theorem: $A$ is semisimple iff ${}_AA$ is semisimple iff $A\cong M_{n_1\times n_1}(D_1)\oplus\cdots\oplus M_{n_k\times n_k}(D_k)$, where the $D_i$ are division algebras.
        \begin{itemize}
            \item Note: ${}_AA$ denotes $A$ as a left $A$-module.
        \end{itemize}
        \item The simplest semisimple algebra is a matrix algebra.
    \end{itemize}
    \item Example: A HW problem solution (PSet 4, Q4b).
    \begin{itemize}
        \item Let $A=M_{n\times n}(F)$. Then $\dim(A)=n^2$.
        \item One linear representation of $A$ that's particularly nice is $S=F^n$, i.e., the set of all column vectors of length $n$ with entries in $F$.
        \begin{itemize}
            \item This representation $\rho:A\to GL(S)$ can simply be defined by $\rho(X)=X$.
            \item Alternatively, this can be thought of as the map from $A\times S\to S$ sending $(X,v)\mapsto Xv$.
            \item This is a simple representation! Using permutation matrices, for instance, we can see that no subspace is fixed under \emph{every} $X\in M_{n\times n}(F)$.
        \end{itemize}
        \item The HW problem was to show that $F^n$ is the only simple module over the matrix algebra.
        \item Sidebar: To prove that $A$ is semisimple, we can show that ${}_AM_{n\times n}(F)\cong\bigoplus^nS=S^n$.
        \begin{itemize}
            \item To do so, use the module isomorphism $(v_1\mid\cdots\mid v_n)\mapsto v_1\oplus\cdots\oplus v_n$.
        \end{itemize}
        \item From here, we can deduce that if $T$ is a simple module, we can construct a homomorphism $A^N=(S^n)^N\twoheadrightarrow T$?? It follows that $S\cong T$? What is this??
    \end{itemize}
    \item Takeaways.
    \begin{itemize}
        \item There is a unique simple module over the matrix algebra, i.e., the columns of the matrix.
        \item The dimension of every module over a matrix algebra will be a multiple of $n$. Why?
        \begin{itemize}
            \item $M_{n,n}(F)$ is semisimple. Thus, any (finite-dimensional) $M_{n,n}(F)$-module $M$ is semisimple. Consequently, $M=\bigoplus_{i\in I}S_i$. But since every $S_i=F^n$, we have $M=(F^n)^{|I|}$ with $\dim(M)=n\cdot|I|$, as desired.
            \item Think $n\times 1$ matrices (column vectors; what we just discussed), $n\times 2$ matrices, $n\times 3$ matrices, on and on.
        \end{itemize}
    \end{itemize}
    \item Moving on.
    \item We want something more complete about an algebra.
    \item Recall the radical of $A$.
    \item Main theorem: $A$ is semisimple iff $\Rad(A)=0$.
    \item Facts.
    \begin{enumerate}
        \item $\Rad(A)$ is a two-sided ideal.
        \begin{itemize}
            \item Prove directly by multiplying on both left and right, as at the end of Wednesday's class.
        \end{itemize}
        \item $\Rad(A)=\bigcap L$ where $L$ is a maximal left ideal.
    \end{enumerate}
    \item \textbf{Maximal} (left ideal of $A$): A left ideal $L$ for which there exists no left ideal $L'$ such that $L\subsetneq L'\subsetneq A$.
    \begin{itemize}
        \item Ideals are subspaces. Maximal means biggest by inclusion, but not necessarily equal to the whole thing.
    \end{itemize}
    \item We now prove Fact 2.
    \begin{proof}
        We first establish some facts. Then we do a bidirectional inclusion proof.\par\smallskip
        If $L$ is a left ideal, then $A/L$ is a left $A$-module. If we now assert that $L$ is a \emph{maximal} left ideal, then $A/L$ is a \emph{simple} left $A$-module. This is because of the following correspondence theorem, a very general fact that's easy to show: Essentially, if you have some modules $M,N$ such that $N\leq M$, then the modules in between $N\subsetneq M$ are in bijection with $M/N$. This bijection is defined in the forward direction by quotienting modules in between $N\subsetneq M$, and in the reverse direction by taking the preimage of the quotient projection. Thus, maximal left ideals $L$ have nothing in between them and $A$, so $A/L$ is in bijection with nothing! Moreover, \emph{every} simple module is obtained this way.\par
        If $S$ is an arbitrary simple module containing $v_0\neq 0$, then we may define $f:A\to S$ sending $a\mapsto av_0$. Note that $0\subsetneq\im(f)\subseteq S$. But since $S$ is simple, we must have $\im(f)=S$ so $f$ must be surjective. It follows that $S\cong A/L$ for some maximal left ideal $L$ of $A$.\footnote{This seems redundant; perhaps Rudenko meant to say $L=\ker(f)$ in addition to the other stuff??}\par\smallskip
        If $x\in\Rad(A)$ and $L$ is a maximal left ideal of $A$, then $x(A/L)=0$ (since $A/L$ is simple). It follows that $xA\subseteq L$. It follows since $x\in xA$ that $x\in L$. Thus, $\Rad(A)\subset\bigcap L$.\par
        % \begin{itemize}
        %     \item Let $x\in\bigcap L$.
        %     \item Let $S$ be a simple $A$-module.
        %     \item Let $L$ be the maximal left ideal of $A$ such that $S\cong A/L$.
        %     \item WTS: $xS=0$.
        %     \item Let $0\neq v_0\in S$ be arbitrary.
        %     \item Define $f:A\to S$ by $a\mapsto av_0$.
        %     \item $S$ is simple: $f$ is surjective.
        %     \item Implies: $S\cong A/\ker(f)$.
        %     \item Implies: $\ker(f)=L$.
        %     \item $x\in\bigcap L$: $x\in L$.
        %     \item Implies $x\in\ker(f)$.
        %     \item Implies $xv_0=0$.
        %     \item $v_0$ is arbitrary: $xS=0$.
        % \end{itemize}
        Now, to show the other inclusion, let $x\in\bigcap L$. Let $S$ be an arbitrary simple module over $A$. We know that $S\cong A/L$ for some maximal ideal $L$. To demonstrate that $xS=0$, it will suffice to confirm that $xv_0=0$ for all $v_0\in S$. Let $0\neq v_0\in S$ be arbitrary. Define $f:A\to S$ by $a\mapsto av_0$. Since $v_0$ is nonzero and hence $\im(f)$ is nontrivial, the fact that $S$ is simple must mean that $\im(f)=S$ and hence $f$ is surjective. Thus, $A/\ker(f)\cong S$. Consequently, $\ker(f)=L$. It follows since $x\in\bigcap L$ and hence $x\in L$ that $x\in\ker(f)$. But then $xv_0=0$, as desired.
    \end{proof}
    \item Thus, the radical has the equivalent descriptions
    \begin{equation*}
        \Rad(A) = \{a\in A\mid aS=0\text{ for any simple module }S\}
        = \bigcap L
    \end{equation*}
    \item Theorem: $A$ (finite-dimensional) is semisimple iff $\Rad(A)=0$.
    \begin{proof}
        We will prove both directions independently here. Let's begin.\par
        ($\Rightarrow$): Suppose $A$ is semisimple. Then $A=S_1\oplus\cdots\oplus S_N$. It follows in particular that $1=s_1+\cdots+s_N$ for some $s_i\in S_i$ ($i=1,\dots,n$). Now let $a\in\Rad(A)$ be arbitrary; we hope to show that $a=0$. Fortunately, we can do this as follows via
        \begin{equation*}
            a = a\cdot 1
            = as_1+\cdots+as_N
            = 0+\cdots+0
            = 0
        \end{equation*}
        Just to be super clear, $as_i=0$ because $a\in\Rad(A)$ implies $aS=0$ for all simple modules $S$, including $S_i$ of which $s_i$ is an element and is thus annihilated by $a$.\par
        % The fact that $A$ is finite dimensional implies that there exists a collection $L_1,\dots,L_n$ of maximal ideals such that $\bigcap L=\bigcap^nL_i$. It's more efficient to just take finitely many! Since we're finite dimensional, what we can do is drop dimensions from $\dim L_1$ to $\dim L_1\cap L_2$ to $\dim L_1\cap L_2\cap L_3$, so since we're eventually gonna hit zero, we're eventually going to have to stop. Thus, $\Rad(A)=L_1\cap\cdots\cap L_n$. One line to finish. Take ${}_AA$ as a left $A$-module (denoted with left subscript $A$). We can map ${}_AA$ to $A/L_1\oplus\cdots\oplus A/L_n$. Call this map $f$. Then
        % \begin{equation*}
        %     \ker(f) = \bigcap_{i=1}^nL_i = \Rad(A) = 0
        % \end{equation*}
        % But then $f$ is injective, so ${}_AA$ is a submodule of a semisimple module, so therefore ${}_AA$ is semisimple itself. Help on this whole direction of the argument??
        ($\Leftarrow$): Suppose $\Rad(A)=0$. Then $\bigcap L=0$. This combined with the fact that $A$ is finite dimensional implies that there exists a finite collection $L_1,\dots,L_n$ of maximal ideals such that $\bigcap L=\bigcap^nL_i$. (In particular, $n\leq\dim A$. Essentially, since we're finite dimensional, what we can do is drop dimensions from $\dim L_1$ to $\dim L_1\cap L_2$ to $\dim L_1\cap L_2\cap L_3$, so since we're eventually going to hit zero, we're eventually going to have to stop. In other words, choose $L_1$, then choose $L_2$ such that $\dim L_1\cap L_2<\dim L_1$, then choose $L_3$ such that $\dim L_1\cap L_2\cap L_3<\dim L_1\cap L_2$, and continue in this fashion until we have $\dim L_1\cap\cdots\cap L_n=0$; because the sequence $\dim L_1\cap\cdots\cap L_i$ is strictly decreasing and the initial value is finite, the sequence must eventually terminate.) Thus, $\Rad(A)=L_1\cap\cdots\cap L_n$. One line to finish. View $A$ as a left $A$-module (denote it ${}_AA$ with left subscript $A$). Define $f:{}_AA\to A/L_1\oplus\cdots\oplus A/L_n$ by $f(a)=(\pi_1(a),\dots,\pi_n(a))$, where $\pi_i(a):A\to A/L_i$ denotes the projection function $a\mapsto a+L_i$. Then
        \begin{equation*}
            \ker(f) = \bigcap_{i=1}^nL_i = \Rad(A) = 0
        \end{equation*}
        But then $f$ is injective. This combined with the fact that $A/L_1\oplus\cdots\oplus A/L_n$ is semisimple by definition means that ${}_AA$ is isomorphic to a submodule of a semisimple module. Thus, since the only submodules of a semisimple module are mix-and-match combinations of the semisimple module's constituent simple modules, ${}_AA$ is semisimple itself. Therefore, by the semisimple algebra conditions from Wednesday's class, $A$ is semisimple.
    \end{proof}
    \item \textbf{Artinian} (ring): A ring for which every decreasing sequence of ideals has to stabilize.
    \item Let $S_1,S_2$ be simple modules, and let $M$ be some module. We get $\Hom_G(S_2,S_1)$, $\Ext^1(S_2,S_1)$, $\Ext^2(S_2,S_1)$, \dots. This gets very complicated very quickly, and you actually need homological algebra to keep track of everything.
    \begin{itemize}
        \item Point??
    \end{itemize}
    \item New HW problem: $A=\F_p[G]$ ($p$ a prime) is never semisimple. This is called \textbf{modular representation theory}, it's in our book (where??), and it's hard.
    \item A very concrete criterion for semisimplicity.
    \begin{itemize}
        \item Let $F=\C$ and let $A$ be finite dimensional with $\dim_FA=n$.
        \item Define a scalar product in $A$ by
        \begin{equation*}
            \inp{x,y} = \tr(L_xL_y)
        \end{equation*}
        \begin{itemize}
            \item $L_x:A\to A$ is the map that sends $a\mapsto xa$.
            \item This is a symmetric map; it's got a lot of nice properties actually.
            \item Note: $\tr(L_xL_y)$ is colloquially known as $\tr(xy)$.
        \end{itemize}
        \item Theorem: Let $A$ be a finite-dimensional algebra over $\C$. Then $A$ is semisimple iff $\tr(x^2)$ is \textbf{nondegenerate}, which means that if $\tr(xa)=0$ for any $x$, then $a=0$. We've probably seen this in the context of vector spaces like $V\otimes V\to\C$ or $V\cong V^*$. What is this??
        \item Something about $|G|^{|G|}$. What is this??
    \end{itemize}
    \item \textbf{Nondegenerate} (finite-dimensional bilinear form): A bilinear form $f(x,a)$ such that if $f(x,a)=0$ for any $x$, then $a=0$.
    \item Next week: Number theoretic group theory and then representation theory of symmetric groups.
\end{itemize}



\section{L Chapter XVII: Semisimplicity}
\emph{From \textcite{bib:Lang}.}
\begin{itemize}
    \item \marginnote{11/10:}Sections 1-2 cover a lot of the stuff discussed during Monday's class.
    \item Rewrite proof of theorem from Monday's class!
    \item \marginnote{12/25:}Perhaps the reason that we talked about matrices of functions right before semisimplicity is that homomorphisms of semisimple modules, in particular, have this decomposable structure.
    \item The Wedderburn-Artin theorem is not obviously covered here.
    \item The radical is talked about in Section 6.
\end{itemize}




\end{document}