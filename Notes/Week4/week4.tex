\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{3}

\begin{document}




\chapter{Properties of Characters}
\section{Representation Ring; Character Basis}
\begin{itemize}
    \item \marginnote{10/16:}Announcements.
    \begin{itemize}
        \item Reminder: Midterm 11/10.
        \item OH this week in-person at normal times.
        \item PSet 3 should be fun.
    \end{itemize}
    \item Today: Finish proving some character things.
    \item Recall: The main picture.
    \begin{itemize}
        \item Rudenko redraws Figure \ref{fig:RepTheoryStory}.
        \item We have a finite group $G$ and we are studying finite-dimensional $G$-reps over $\C$.
        \item $\C_\text{cl}[G]$ is a ring.
        \item The map\dots
        \begin{itemize}
            \item Respects addition;
            \item Sends tensor multiplication to (pointwise) functional multiplication;
            \item Sends duality to conjugation;
            \item Respects a kind of inner product, whether it be either side of $\dim_\C\Hom_G(V,W)=\inp{f_1,f_2}$.
        \end{itemize}
    \end{itemize}
    \item Today, we will see that $\C_\text{cl}[G]\cong\C^k$, where $k$ is the number of conjugacy classes.
    \begin{itemize}
        \item In other words, we will see that the number of irreps is also exactly equal to $k$, that there is a bijection $\{V_i\}\to\{\chi_i\}$, and that the $\chi_1,\dots,\chi_k$ form an orthonormal basis of $\C_\text{cl}[G]$.
    \end{itemize}
    \item Visualizing the vector space $\C_\text{cl}[G]$.
    \begin{figure}[h!]
        \centering
        \begin{tikzpicture}[scale=1.2]
            \footnotesize
            \draw [->] (-0.3,0) -- (2.4,0);
            \draw [->] (0,-0.3) -- (0,2.4);
    
            \foreach \x in {0,0.5,...,2} {
                \foreach \y in {0,0.5,...,2} {
                    \fill [bly] (\x,\y) circle (1.5pt);
                }
            }
    
            \draw [blx,ultra thick,-stealth] (0,0) -- node[black,below]{$\chi_1^{}$} (0.5,0);
            \draw [blx,ultra thick,-stealth] (0,0) -- node[black,left]{$\chi_2^{}$} (0,0.5);
        \end{tikzpicture}
        \caption{Visualizing the space of class functions on $G$.}
        \label{fig:classFuncSpace}
    \end{figure}
    \begin{itemize}
        \item It's a "cone" emanating from the origin with only lattice points.
        \begin{itemize}
            \item If $\dim\C_\text{cl}[G]=2$, the vector space consists of all the blue points in Figure \ref{fig:classFuncSpace}.
        \end{itemize}
        \item Why is it only lattice points instead of a continuous function space?
        \begin{itemize}
            \item The restrictions on coefficients are inherited from the restrictions on what kinds of spaces you can build of the form $V_1^{n_1}\oplus V_2^{n_2}$.
            \item Indeed, if it were continuous, that would imply that there is some meaning to the point $0.3\chi_1+2.5\chi_2$, i.e., there is a space $V_1^{0.3}\oplus V_2^{2.5}$. But of course, we cannot define such a space!
        \end{itemize}
        \item Why is it only \emph{nonnegative} integer coefficients and not \emph{all} integer coefficients?
        \begin{itemize}
            \item We don't have subtraction to get us to a full ring.
            \item Additionally, we can only scale and linearly combine the $\chi_i$'s with nonnegative integer coefficients because, as said above, those are the types of reducible rep decompositions we have.
        \end{itemize}
    \end{itemize}
    \item Let $[V]$ denote the \textbf{isomorphism class} of the representation $V$.
    \item \textbf{Isomorphism class} (of $V$): The set of all vector spaces $W$ that are isomorphic to $V$ as representations.
    \item This allows us to define the \textbf{representation ring}.
    \item \textbf{Representation ring} (of $G$): The ring $(R,+,\cdot)$, where $R$ is the free abelian group generated by all isomorphism classes of the representations of $G$, quotiented by the span of all linear combinations of the form $[V\oplus W]-[V]-[W]$; $+$ is well-defined via the construction of $R$, which yields $[V]+[W]=[V\oplus W]$ for all $[V],[W]$ in the ring; and $\cdot$ is defined by $[V]\cdot[W]=[V\otimes W]$. \emph{Denoted by} $\bm{R(G)}$.
    \begin{itemize}
        \item Basis: $[V_1],\dots,[V_k]$.
        \item Thus, structurally,
        \begin{equation*}
            R(G) \cong \Z^k
        \end{equation*}
        \item Elements are of the form $[V_1]+2[V_2]-3[V_3]$.
        \item Multiplication is slightly complicated because $V_i\otimes V_j=\bigoplus V_k^{n_{ijk}}$; it follows that
        \begin{equation*}
            [V_i]\cdot[V_j] = \sum n_{ijk}[V_k]
        \end{equation*}
    \end{itemize}
    \item Alternative construction of $R(G)$: Take the subring of the class ring $\C_\text{cl}[G]$ that is generated by the characters.
    \begin{itemize}
        \item To do so, define a map $R(G)\to\C^k$ where the image is linear combinations of characters $\chi_i$ with $\Z$-class.
        \item Clarify this construction??
    \end{itemize}
    \item \textbf{Virtual representation}: An element of $R(G)$.
    \begin{itemize}
        % \item We can also call elements of the group \textbf{virtual} representations; they're not, because we don't have subtraction, but we're close.
        \item We need this term because some elements of $R(G)$ --- like $-[V]$, for instance --- may not correspond to an actual representation.
        \item Indeed, note that $-[V]$ is \emph{not} $V^*$; it is just some thing that when you add it to $[V]$, you get the zero representation.
    \end{itemize}
    \item Example: Let $G=\Z/2\Z=\{e,x\}$.
    \begin{itemize}
        \item Then $R(G)=\Z^2=\Z e\oplus\Z x$ has basis $[1],[-1]$ (corresponding to the trivial and alternating representations) where we define
        \begin{align*}
            [1]^2 &= [1]&
            [1][-1] &= [-1]&
            [-1]^2 &= [1]
        \end{align*}
    \end{itemize}
    \item One reason people like this $R(G)$ is as follows.
    \begin{itemize}
        \item Initially, understanding this group is not easy because even to get started, you have to find all your characters.
        \item But, we know that
        \begin{equation*}
            R(G)\otimes_\Z\C \cong \C_\text{cl}[G]
        \end{equation*}
        \begin{itemize}
            \item So we have a ring that's hard to understand, but if we do something called an \textbf{extension of scalars} (shown above) we get an easy ring!
            \item Why?? Clarify this construction.
        \end{itemize}
        \item This is interesting because we can look at the intermediate objects. For example, could we describe $R(G)\otimes\R$ or $R(G)\otimes\Q$. Interestingly, \textbf{Artin's theorem} describes $R(G)\otimes\Q$ completely.
        \item If we try to understand $R(S_n)$, this is still hard work, but if we take $\bigoplus_{n\geq 0}R(S_n)$, we obtain an object that is remarkably, surprisingly simple. That's where we're going. This is why rep theory of finite groups is simultaneously very hard and very simple.
    \end{itemize}
    \item Lemma: Let $G$ be a finite group, let $f$ be a complex-valued\footnote{This "complex-valued" hypothesis was not stated in class, but I have to imagine it's true. Is it??} class function, and let $V$ be a $G$-rep. Then the linear map
    \begin{equation*}
        F = \sum_{g\in G}f(g)\cdot g:V\to V
    \end{equation*}
    is a morphism of $G$-representations, that is, $F\in\Hom_G(V,V)$.
    \begin{proof}
        To prove that $F\in\Hom_G(V,V)$, it will suffice to show that $xF=Fx$ for every $x\in G$. Let $x\in G$ be arbitrary. Then
        \begin{align*}
            F(xv) &= \sum_{g\in G}f(g)gxv
            \intertext{Since $\rho$ is a group homomorphism, the functions $\rho(g)\in GL(V)$ act just like the elements $g\in G$. \emph{This} is what justifies us to basically move everything around all willy-nilly. Thus, continuing from the above, we have}
            &= \sum_{g\in G}f(g)(xx^{-1})gxv\\
            &= \sum_{g\in G}f(g)x(x^{-1}gx)v\\
            \intertext{Since $x=\rho(x)$ is in the general \emph{linear} group, i.e., is a \emph{linear} map, we can factor it out of the sum of functions to get}
            &= x\left( \sum_{g\in G}f(g)x^{-1}gx \right)v\\
            \intertext{Since $f$ is a class function by hypothesis, we have $f(g)=f(x^{-1}gx)$, so}
            &= x\left( \sum_{g\in G}f(x^{-1}gx)x^{-1}gxv \right)\\
            &= x\sum_{g\in G}f(g)gv\\
            &= x(Fv)
        \end{align*}
        as desired.
    \end{proof}
    \item Recall that previously, we had $(1/|G|)\sum_{g\in G}g:V\to V^G$.
    \begin{itemize}
        \item He will put something about this being a class function on the midterm?? Review how to prove that this is a class function!
    \end{itemize}
    \item Another comment: A slightly refined question.
    \begin{itemize}
        \item Suppose you have a class function $f$ and an irrep $V$.
        \item Then we know that $F=\sum f(g)g:V\to V$ is a $G$-morphism, so it is a \textbf{homothety} by Schur's lemma.
        \item So let's find $\lambda$.
        \item Thinking a big more carefully, we know that $F$ above is
        \begin{equation*}
            \sum_{g\in G}f(g)\rho_V(g)
            = \lambda I_{d_V}
        \end{equation*}
        where $d_V$ denotes the \textbf{degree} of $V$.
        \item Now, we will compute $\lambda$ using the trace. Take the trace of both sides. Then
        \begin{align*}
            \tr(\sum_{g\in G}f(g)\rho_V(g))
            &= \tr(\lambda I_{d_V})\\
            \sum f(g)\tr(\rho_V(g)) &= \lambda d_V\\
            \sum f(g)\chi_V(g) &= \lambda d_V\\
            \lambda &= \frac{|G|}{d_V}\frac{1}{|G|}\sum_{g\in G}f(g)\overline{\chi_{V^*}(g)}\\
            &= \frac{|G|}{d_V}\inp{f,\chi_{V^*}}
        \end{align*}
    \end{itemize}
    \item \textbf{Homothety}: A map $F:V\to V$ for which there exists $\lambda\in\C$ such that $Fv=\lambda v$ for all $v\in V$.
    \begin{itemize}
        \item It just means that we're scaling.
    \end{itemize}
    \item \textbf{Degree} (of $V$): The dimension of $V$ as a vector space. \emph{Denoted by} $\bm{d_V}$. \emph{Given by}
    \begin{equation*}
        d_V = \dim V
    \end{equation*}
    \item Now, we can prove the theorem to which we've been building up the whole time.
    \item Theorem: Let $G$ be a finite group. Then the number of irreps up to isomorphism is equal to the number of conjugacy classes.
    \begin{proof}
        % Thus, since the $e_g$ are all linearly independent, demonstrating that $F(e_e)=0$ will illustrate that each $f(g)=0$. By the Lemma, $F:V_\text{reg}\to V_\text{reg}$ is a morphism of $G$-representations. Additionally, by complete reducibility, $V_\text{reg}\cong V_1^{d_{V_1}}\oplus\cdots\oplus V_s^{d_{V_s}}$. Thus, if we restrict $F$ to any irrep
        
        % It follows since $\inp{f,\chi_{V_i}}=0$ ($i=1,\dots,s$) that $\inp{f,\chi_{V_i^*}}=0$ ($i=1,\dots,s$). Consequently, by the comment above, $\sum f(g)g$ acts by zero on any irrep by the preceding lemma. Thus, since the regular representation $V_\text{reg}\cong V_1^{d_{V_1}}\oplus\cdots\oplus V_s^{d_{V_s}}$, $\sum f(g)g$ acts by zero on $V_\text{reg}$. Thus, since $\C[G]=\bigoplus_{g\in G}\C e_g$, apply $\sum f(g)g$ to $e_e$; it will go to $\sum f(g)e_g=0$ which means since each $e_g$ is linearly independent that $f=0$, a contradiction.


        Let $k$ be the number of conjugacy classes of $G$, and let $\chi_1,\dots,\chi_s$ be the characters of the irreps. By the theorem from last Wednesday's class, it follows that $\chi_1,\dots,\chi_s$ are orthonormal vectors in $\C_\text{cl}[G]$. Thus, by the corollary to the aforementioned theorem, $s\leq k$.\par
        Now, suppose for the sake of contradiction that $s<k$. Then there exists a nonzero $f\in\C_\text{cl}[G]$ such that $\inp{f,\chi_{V_i}}=0$ ($i=1,\dots,s$). By Gram-Schmidt, we can choose $f$ to be another \emph{orthonormal} vector in the list, extending it to $\chi_1,\dots,\chi_s,f$. We will now build up to proving that $f(g)=0$ for all $g\in G$ (i.e., $f=0$), which we will do by using the above lemma to construct a linear independence argument as follows. The first step is to let $V_i$ be an arbitrary irrep of $G$. Then by the above comment, $F:V_i\to V_i$ may be evaluated on any $v\in V_i$ as follows.
        \begin{equation*}
            F(v) = \lambda Iv
            = \frac{|G|}{d_{V_i}}\inp{f,\chi_{V_i^*}}\cdot v
            = \frac{|G|}{d_{V_i}}\overline{\inp{f,\chi_{V_i}}}\cdot v
            = \frac{|G|}{d_{V_i}}\overline{0}\cdot v
            = 0
        \end{equation*}
        It follows that $F=0$ on \emph{any} representation since by complete reducibility, they're all direct sums of irreps. In particular, $F:V_\text{reg}\to V_\text{reg}$ is the zero operator, where $V_\text{reg}\cong V_1^{d_{V_1}}\oplus\cdots\oplus V_s^{d_{V_s}}$ is the regular representation. Thus, for example, $F(e_e)=0$. But we also know that
        \begin{equation*}
            F(e_e) = \sum_{g\in G}f(g)\cdot ge_e
            = \sum_{g\in G}f(g)\cdot e_g
        \end{equation*}
        Consequently, by transitivity, we have that
        \begin{equation*}
            0 = \sum_{g\in G}f(g)\cdot e_g
        \end{equation*}
        But since the $e_g$ are all linearly independent by the definition of the regular representation, we have that each $f(g)=0$, as desired. This means that $f=0$, contradicting our original supposition.
    \end{proof}
    \item That is the end of this story.
    \item Here's one consequence of the above theorem.
    \begin{itemize}
        \item We now know that the space of class functions has an orthonormal basis $\chi_{V_1^*},\dots,\chi_{V_k^*}$.
        \item If we denote the conjugacy classes of $G$ by $C_1,\dots,C_k$, then another obvious basis of $\C_\text{cl}[G]$ is $\delta_{C_1},\dots,\delta_{C_k}$ defined by
        \begin{equation*}
            \delta_{C_i}(g) =
            \begin{cases}
                1 & g\in C_i\\
                0 & g\notin C_i
            \end{cases}
        \end{equation*}
        \item This new basis is orthogonal: We have
        \begin{equation*}
            \inp{\delta_{C_i},\delta_{C_j}} = \frac{1}{|G|}\sum_{g\in G}\delta_{C_i}(g)\delta_{C_j}(g) =
            \begin{cases}
                0 & i\neq j\\
                \frac{|C_i|}{|G|} & i=j
            \end{cases}
        \end{equation*}
        \begin{itemize}
            \item Justifying this computation: If $i\neq j$, then at least one of $\delta_{C_i},\delta_{C_j}$ will be zero; if $i=j$, then they're both nonzero and equal to 1 for all $|C_i|$ elements $g\in C_i$.
        \end{itemize}
        \item What is the change of basis matrix between $\{\delta_{C_i}\}$ and $\{\chi_{V_i^*}\}$? It's the character table.
        \begin{itemize}
            \item The orthogonality condition for characters then just comes from the fact that we're going from one orthogonal basis to another.
            \item What are the exact bases we change between??
        \end{itemize}
    \end{itemize}
\end{itemize}



\section{Office Hours}
\begin{itemize}
    \item \marginnote{10/17:}\textbf{Transitive} (group action): A group action for which the \textbf{orbit} of $x$ is equal to $X$ for any $x\in X$.
    \item \textbf{Orbit} (of $x\in X$): The set of $g\cdot x$ for all $g\in G$.
    \item \textbf{Diagonal action} (of $G$ on $X\times X$): The action defined as follows. \emph{Given by}
    \begin{equation*}
        g\cdot(x_1,x_2) = (g\cdot x_1,g\cdot x_2)
    \end{equation*}
    \item Check \textcite{bib:Etingof} for some things??
\end{itemize}



\section{Orthogonality Results}
\begin{itemize}
    \item \marginnote{10/18:}Announcements.
    \begin{itemize}
        \item Goal: Finish our discussion of the orthogonality of characters, projection functions, etc.
        \item Friday: Frobenius determinant.
        \item Next week: Group algebras, associative algebras, etc.; another perspective on representations.
        \item After next week: A more advanced part of representation theory related to group theory.
    \end{itemize}
    \item Describing Figure \ref{fig:RepTheoryStory} from a different perspective.
    \begin{itemize}
        \item Let $G$ be a finite group, and let $k$ denote the number of conjugacy classes and the number of irreps. Let $C_1,\dots,C_k$ be the conjugacy classes and $V_1,\dots,V_k$ be the irreps.
        \item There is no natural/canonical bijection between the two sets. For a simple group, there is often a canonical way, and this is where things get interesting.
        \begin{itemize}
            \item Example: Symmetric group induces canonical bijection, as we'll see later.
        \end{itemize}
        \item $\C_\text{cl}[G]=\C^k$ is a vector space of class functions and a ring.
        \item We have the Hermitian inner product
        \begin{equation*}
            \inp{f_1,f_2} = \frac{1}{|G|}\sum_{g\in G}f_1(g)\overline{f_2(g)}
        \end{equation*}
        \item Recall that $\chi_{V_1},\dots,\chi_{V_k}$ is an orthonormal basis such that
        \begin{equation*}
            \inp{\chi_{V_i},\chi_{V_j}} = \delta_{ij}
        \end{equation*}
        \item We have another basis $\delta_{C_1},\dots,\delta_{C_k}$ defined by
        \begin{equation*}
            \delta_{C_i}(g) =
            \begin{cases}
                0 & g\notin C_i\\
                1 & g\in C_i
            \end{cases}
        \end{equation*}
        that is orthogonal but not orthonormal:
        \begin{equation*}
            \inp{\delta_{C_i},\delta_{C_j}} =
            \begin{cases}
                0 & C_i\neq C_j\\
                \frac{|C_i|}{|G|} & C_i=C_j
            \end{cases}
        \end{equation*}
        \item How do we relate the two bases?
        \item To begin, fix $C_i$. Then
        \begin{equation*}
            \delta_{C_j}(g) = \sum_{V_i}\lambda_i\chi_{V_i}(g)
        \end{equation*}
        \item $\lambda_i$ can be computed immediately using the inner product since the characters are orthonormal:
        \begin{equation*}
            \lambda_i = \inp{\delta_{C_j},\chi_{V_i}}
            = \frac{1}{|G|}\sum_{g\in G}\delta_{C_j}(g)\overline{\chi_{V_i}(g)}
            = \frac{|C_j|\bar{\chi}_{V_i}(g)}{|G|}
        \end{equation*}
        \begin{itemize}
            \item You took $\lambda_i = \inp{\delta_{C_j},\bar{\chi}_{V_i}}$; which one is correct??
        \end{itemize}
        \item But then
        \begin{equation*}
            \delta_{C_j}(g) = \frac{|C_j|}{|G|}\left( \sum_{V_i}\bar{\chi}_{V_i}(C_j)\chi_{V_i}(g) \right)
        \end{equation*}
        \item It follows that we have two bases of $\C_\text{cl}[G]$. These are given by
        \begin{align*}
            \frac{|G|}{|C_j|}\delta_{C_j}&&
            \chi_{V_i^*}
        \end{align*}
        where $i,j=1,\dots,k$.
        \item How do we convert between these two very natural bases of our space of functions? The change of basis matrix from left to right is the character table.
        \item Obviously, we have to do some scaling and take some duals, but it's not that bad and it fits the character table really well.
        \item This gives us some properties of the character table such as orthogonality.
        \item For example, \textbf{orthogonal} matrices convert between orthogonal bases; in the complex domain, such a matrix is \textbf{unitary}, i.e., for the character table $U$, $U\bar{U}^T=E$.
    \end{itemize}
    \item Orthogonality relations that you can derive.
    \begin{enumerate}
        \item We can show that
        \begin{equation*}
            \sum_{g\in G}\chi_1(g)\overline{\chi_2(g)} =
            \begin{cases}
                0 & \chi_1\neq\chi_2\\
                |G| & \chi_1=\chi_2
            \end{cases}
        \end{equation*}
        \begin{itemize}
            \item Use the unitary condition.
        \end{itemize}
        \item We can show that
        \begin{equation*}
            \sum_{i=1}^k\chi_i(g_1)\overline{\chi_i(g_2)} =
            \begin{cases}
                0 & g_1\neq g_2\\
                \frac{|G|}{|C(g_1)|} & g_1\sim g_2
            \end{cases}
        \end{equation*}
        \begin{itemize}
            \item We literally just take the identity defining $\delta_{C_j}(g)$.
        \end{itemize}
    \end{enumerate}
    \item \textbf{Isotypical component}: A representation that is equal to the direct sum of isomorphic irreducible representations. \emph{Also known as} \textbf{isotypic component}.
    \begin{itemize}
        \item Illustrative example: For $V=V_1^{n_1}\oplus\cdots\oplus V_k^{n_k}$, each $V_i^{n_i}$ is an isotypical component.
    \end{itemize}
    \item Examples.
    \begin{enumerate}
        \item Let $G\acts\C^2$ by $\rho(g)=E_2$. Thus, we can say that $\C^2=V_1\oplus V_1$, but we can't say this in any unique, canonical way, i.e., we can choose infinitely many $V_1$'s and have the statement still be true, where $V_1$ is the trivial rep.
        \item We have $V_1^{n_1}=V^G=\{v\in V\mid gv=v\ \forall\ g\in G\}$. Look at what's invariant under the symmetry group, i.e., define
        \begin{equation*}
            P = \frac{1}{|G|}\sum g
        \end{equation*}
        \begin{itemize}
            \item All \textbf{invariant functions} come from averaging over the group!
            \item Then $P^2=P$ and $\im P=V^G$.
            \item Takeaway: We call each $V_i^{n_i}$ an \textbf{isotypical component}.
            \item What's going on in this example??
        \end{itemize}
        \item The permutational representation for $S_n$ decomposes into the sum of the trivial and standard reps; there is only one decomposition this way. If we look at $V_1\oplus V_\text{stand}^2$, then our decomposition will depend on a choice of a plane.
    \end{enumerate}
    \item Reminder.
    \begin{itemize}
        \item Last time, we chose an $f\in\C_\text{cl}[G]$, a representation $V$, and then took $\sum f(g)g:V\to V$ so that then $\sum f(g)g\in\Hom_G(V,V)$.
        \item Moreover, we proved that if $V$ is irreducible, then this endomorphism is equal to a scalar $\lambda$ times the identity matrix via Schur's lemma.
        \item Computing $\lambda$:
        \begin{equation*}
            \lambda = \frac{|G|}{d_V}\inp{f,\chi_V^*}
        \end{equation*}
        \begin{itemize}
            \item Hard to remember but easy to derive.
        \end{itemize}
        \item Define $V=V_1^{n_1}\oplus\cdots\oplus V_k^{n_k}$ and $P_i:V\to V_i^{n_i}$.
        \item In particular, look at
        \begin{equation*}
            P_i = \frac{d_V}{|G|}\sum_{g\in G}\chi_{V_i^*}(g)g
        \end{equation*}
        \begin{itemize}
            \item This averaging operator is consistent with what we had before.
        \end{itemize}
        \item $P_i$ acts on $V_i$ by
        \begin{equation*}
            \frac{d_{V_i}}{|G|}\frac{|G|}{d_{V_i}}\inp{\chi_{V_i^*},\chi_{V_i^*}} = 1
        \end{equation*}
        \item $P_i$ acts on $V_j$ by
        \begin{equation*}
            \frac{d_{V_i}}{|G|}\frac{|G|}{d_{V_i}}\inp{\chi_{V_i^*},\chi_{V_j^*}} = 0
        \end{equation*}
        \item Take $V=V_1^{n_1}\oplus\cdots\oplus V_k^{n_k}$ and apply $P_i$. It follows by the above that it is exactly the projection on $V_i^{n_i}$.
        \item Thus, $P_1+\cdots+P_k=1$. $P_i^2=P_i$. $P_iP_j=0$. This is called a/the (which one??) \textbf{idempotent decompostion}.
        \item Example: Let $v\in V$. Then $v=P_1v+\cdots+P_kv$.
        \item Additionally, we can take a function $f$ that is invariant under the group...??
    \end{itemize}
    \item We're done early.
    \item We will not start the Frobenius determinant today.
    \item We will start on next week's content then so we can begin thinking about it.
    \item \textbf{Associative algebra}: A vector space over a field $F$ that is also a (not necessarily commutative) ring, where we have a unit 1 in the ring, addition, and multiplication. Scalar multiplication: $\lambda a=(\lambda\cdot 1)\cdot a$. Associativity condition: $(\lambda a)b=\lambda(ab)$. \emph{Denoted by} $\bm{A}$.
    \begin{itemize}
        \item We'll only discuss finite-dimensional algebras in this course.
    \end{itemize}
    \item Examples:
    \begin{enumerate}
        \item $\R$, $\C$ (an algebra over $\R$).
        \item $\HH$, a 4d algebra over $\R$. The algebra of quaternions. $\HH=\R+\R i+\R j+\R k$.
        \begin{itemize}
            \item Hamilton's remarkable discovery: There is a 4D set of numbers that is not commutative but is still associative and helps describe rotation in 3D or 4D space.
            \item Multiplication rules:
            \begin{align*}
                i^2 &= j^2 = k^2 = -1&
                ijk &= -1
            \end{align*}
            \item We should spend part of our weekend reading a history of quaternions!
        \end{itemize}
        \item $M_{n\times n}(F)$, the \textbf{matrix algebra}.
        \item $A_1\oplus\cdots\oplus A_n$, the \textbf{direct sum} of algebras.
        \begin{itemize}
            \item Addition and multiplication are done pairwise.
        \end{itemize}
        \item $A_1\otimes A_2$.
        \begin{itemize}
            \item We will not talk about this today, though!
        \end{itemize}
    \end{enumerate}
    \item Let's go back; let $G$ be a finite group and consider $\C[G]$, the set of functions on $G$.
    \begin{itemize}
        \item This algebra has some basis $\bigoplus\C e_g$.
        \item To get the algebra structure, we just need a rule for multiplying basis elements. In this case, we use $e_{g_1}e_{g_2}=e_{g_1g_2}$.
        \item This is the \textbf{algebra over $\pmb{\C}$ of dimension $\bm{G}$}.
        \item Theorem: $\C[G]\cong M_{d_1\times d_1}(\C)\oplus\cdots\oplus M_{d_k\times d_k}(\C)$.
        \begin{itemize}
            \item We can prove this theorem from what we know: Schur's Lemma and complete reducability.
            \item We'll discuss it for several consecutive times.
            \item A similar result holds for \emph{many} algebras (e.g., semisimple algebra), not just \emph{group} algebras.
        \end{itemize}
    \end{itemize}
    \item HW1-2 will be graded later this week and handed back on Friday.
    \item In \textcite{bib:Etingof}, we can find a lot of history of some of this stuff. The comments are interesting and entertaining.
\end{itemize}



\section{Frobenius Determinant; Intro to Associative Algebras}
\begin{itemize}
    \item \marginnote{10/20:}Let $G=\{g_1,\dots,g_n\}$.
    \item \textbf{Frobenius determinant}: The polynomial defined as follows. \emph{Denoted by} $\bm{F(x_{g_1},\ldots,x_{g_n})}$. \emph{Given by}
    \begin{equation*}
        F(x_{g_1},\dots,x_{g_n}) = \det|x_{g_ig_j}|
    \end{equation*}
    \begin{itemize}
        \item The Frobenius determinant is a homogeneous polynomial with integer coefficients of degree $n$.
        \item $F(x_{g_1},\dots,x_{g_n})\in\Z[x_{g_1},\dots,x_{g_n}]$.
    \end{itemize}
    \item Theorem: There exist irreducible $P_1,\dots,P_m\in\Z[x_{g_1},\dots,x_{g_n}]$ such that
    \begin{equation*}
        F = P_1^{\deg P_1}\cdots P_k^{\deg P_k}
    \end{equation*}
    Moreover, $\chi_i(g)\approx\chi_g^{\deg P_i}$, where $\chi_g$ is the coefficient of $P_i$. (Is this last line correct??)
    \begin{proof}
        Let $\rho:G\to GL_n$ be the regular representation of $G$, and define $P_\rho=\sum\chi_{g_i}\rho(g_i)$. Then $P_\rho(x_{g_1},\dots,x_{g_n})=\pm I(x_{g_1},\dots,x_{g_n})$.

        We have that $P_\rho(e_{g_j})=\sum x_{g_i}g_ie_{g_j}=\sum x_{g_i}e_{g_ig_j}=\sum x_{g_ig_j^{-1}}e_{g_i}$, so the matrix of $P_\rho$ is $(x_{g_ig_j^{-1}})$ and thus has permuted columns and rows relative to the original matrix of which we took the Frobenius determinant.

        Recall that $\C[G]\cong V_1^{d_1}\oplus\cdots\oplus V_k^{d_k}$. Additionally, the matrix of each $V_i$ is $(\sum\chi_gg_i)$.

        Understanding this??
    \end{proof}
    \item \textbf{Group algebra}: The algebra $A$ over a field $F$ with one basis element $e_i$ for each $g_i\in G$ and the multiplication law
    \begin{equation*}
        e_i\cdot e_j = \sum_{i=1}^k\lambda_{ij}^ke_k
    \end{equation*}
    \begin{itemize}
        \item $A\cong F^n$.
    \end{itemize}
    \item \textbf{Division algebra}: An algebra $A$ such that for all nonzero $x\in A$, there exists a $y\in A$ such that $xy=1$.
    \item \textbf{Field}: A commutative division algebra.
    \item Examples.
    \begin{enumerate}
        \item $\C$ is a 2-dimensional algebra over $\R$.
        \item $\HH$ is a 4-dimensional algebra over $\R$.
        \begin{itemize}
            \item As discussed last time, the elements are of the form $q=a+bi+cj+dk$ where $i^2=j^2=k^2=-1=ijk$
            \item Note that it follows that
            \begin{equation*}
                \bar{q} = a-bi-cj-dk
            \end{equation*}
            \item Hence,
            \begin{equation*}
                q\bar{q} = a^2+b^2+c^2+d^2
            \end{equation*}
            \item Thus, we can define
            \begin{equation*}
                q^{-1} = \frac{q}{a^2+b^2+c^2+d^2}
            \end{equation*}
        \end{itemize}
    \end{enumerate}
    \item We now prove some results of division algebras.
    \begin{enumerate}
        \item If $F=\C$, then every finite-dimensional division algebra is $\C$.
        \begin{proof}
            Let $A$ be an arbitrary finite-dimensional division algebra over $\C$. Let $a\in A$, and let $L_a\in GL_n(A)$ send $a\mapsto[L_ax\mapsto ax]$.

            Then $\C\to M_{2\times 2}(\R)$ sends
            \begin{equation*}
                a+bi\mapsto L_{a+bi} =
                \begin{bmatrix}
                    \alpha & -\beta\\
                    \beta & \alpha\\
                \end{bmatrix}
            \end{equation*}
            Then $L_aL_b=L_{ab}$ and $L_a+L_b=L_{a^{-1}b}$, so $L_a$ has eigenvalue $\lambda$, so $L_ax=ax=\lambda x$, so $a=\lambda\cdot 1$.

            What is going on here and how does this work??
        \end{proof}
    \end{enumerate}
    \item Example of the above property.
    \begin{itemize}
        \item $\HH\to M_{4\times 4}(\R)$ sends $a+bi+cj+dk$ to ?? with determinant $(a^2+b^2+c^2+c^2)^2$.
        \item In general, the determinant of $A\to GL_n(A)$.
    \end{itemize}
    \item Theorem 1: Over $\R$, there are exactly three division algebras: $\R$, $\C$, and $\HH$.
    \item Theorem 2: Over $\F_q$ finite, every finite-dimensional division algebra is a field $\F_{q^n}$.
    \item \textbf{Representation} (of $A$): A module $V$ over $A$ equipped with a homomorphism of algebras $\rho:A\to M_{n\times n}(V)$.
    \item Observation:
    \begin{itemize}
        \item If $G$ is a group and $F$ is a field, then the group algebra is $F[G]=\bigoplus_{i=1}^nFe_{g_i}$. Herein, we define $e_{g_i}e_{g_j}=e_{g_ig_j}$.
        \item Modules over $F[G]$ are equivalent to $G$ reps.
        \item $F[G]\to M_{n\times n}(F)$ is equivalent to $G\to GL_n(F)$.
    \end{itemize}
    \item Schur's Lemma for associative algebras: Let $A$ be a finite-dimensional algebra, and let $M_1,M_2$ be simple $A$-modules.
    \begin{enumerate}
        \item Then if $f:M_1\to M_2$ is a nonzero morphism of $A$-modules, $f$ is isomorphic.
        \item If $M$ is simple, $\Hom_A(M,M)$ is a division algebra.
    \end{enumerate}
    \item Note that this version of Schur's Lemma implies that complete reducibility may fail for associative algebras. (why??)
    \item Theorem (Complete Reducibility): Let $A$ be a finite-dimensional algebra such that $M_1\subset M_2$. Then there exists $N$ such that $M_2=M_1\oplus N$. Moreover, it follows that
    \begin{equation*}
        A \cong M_{n_1}(D_1)\oplus\cdots\oplus M_{n_k}(D_k)
    \end{equation*}
\end{itemize}




\end{document}