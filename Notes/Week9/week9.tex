\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{8}

\begin{document}




\chapter{Symmetric Group Representation Characteristics}
\section{Frobenius Reciprocity; The Branching Theorem}
\begin{itemize}
    \item \marginnote{11/27:}Announcements.
    \begin{itemize}
        \item OH on Wednesday at 5:30 PM this week; not Tuesday.
        \item There will be extra OH next week pre-exam.
        \begin{itemize}
            \item Roughly like Monday/Wednesday next week.
        \end{itemize}
        \item Midterm will be returned on Wednesday; we can pick them up in-person in his office starting then.
        \item There are some grade boundaries: Pass/Fail we can do until Friday, withdrawal we can do until 5:00 PM today.
    \end{itemize}
    \item Let's finish the conversation about induction/restriction and prove the \textbf{branching theorem}.
    \item Reminder to start.
    \begin{itemize}
        \item We have two mathematical categories, $G$-reps and $H$-reps where $H\leq G$.
        \item These catagories are related by functors.
        \begin{itemize}
            \item $\res_H^G:G\text{-reps}\to H\text{-reps}$ and vice versa for $\ind_H^G$.
            \item See Figure \ref{fig:induceFunctor}.
        \end{itemize}
        \item Restrictions are stupidly simple.
        \item Inductions, most hands-on, we take copies of $W$ times cosets. Formulaically,
        \begin{equation*}
            \ind_H^GW = g_1W\oplus\cdots\oplus g_kW
        \end{equation*}
        where $k=(G:H)$ and $G=\bigsqcup_{i=1}^kg_iH$.
        \begin{itemize}
            \item In more detail, the action of $g$ on $g_iw$ is that of $g_{\sigma(i)}h_iw$.
            \item This is a genuinely hard construction.
            \item A matrix of this thing will be a block-permutation matrix like
            \def\hashfill{\tikz{
                \clip (0,0) rectangle (0.7,0.2);
                \foreach \x in {-0.1,0,...,0.7} {
                    \draw (\x,0) -- ++(0.3,0.3);
                }
            }}
            \begin{equation*}
                \begin{bNiceArray}[first-row,first-col,margin]{c|w{c}{8mm}|c}
                        & g_1W & & g_kW\\
                    g_1W & \hashfill & 0 & 0\\ \hline
                        & 0 & 0 & \hashfill\\ \hline
                    g_kW & 0 & \hashfill & 0\\
                \end{bNiceArray}
            \end{equation*}
        \end{itemize}
        \item As an alternate construction, we have that
        \begin{equation*}
            g_1W\oplus\cdots\oplus g_kW \cong \Hom_H(\C[G],W)
        \end{equation*}
        \begin{itemize}
            \item Recall that elements of the set on the right above are functions $f:G\to W$ such that $f(h(g))=hf(g)$.
            \item We map between the two via $f(g)\mapsto f(gx')$.
        \end{itemize}
        \item What is nice about induced representations is that $\dim[\ind_H^GW]=(\dim W)[G:H]$.
        \item There is a very easy statement of the character of an induced representation, the \textbf{Frobenius formula}.
        \begin{itemize}
            \item Recall that
            \begin{equation*}
                \tilde{\chi}_W(g) =
                \begin{cases}
                    0 & g\notin H\\
                    \chi_W(g) & g\in H
                \end{cases}
            \end{equation*}
            \item With this, we average:
            \begin{equation*}
                \chi_{\ind_H^GW}(g) = \frac{1}{|H|}\sum_{x\in G}\tilde{\chi}_W(xgx^{-1})
            \end{equation*}
            \item Essentially, we're taking a whole bunch of conjugates, summing them up, and dividing to get rid of overcounting.
        \end{itemize}
    \end{itemize}
    \item We now move onto \textbf{Frobenius reciprocity}, which is a relation between the functors/relations $\ind_H^G$ and $\res_H^G$.
    \begin{itemize}
        \item The first point where category theory gets interesting is the notion of \textbf{adjoint functors}, which we are about to touch on. It is a very subtle notion.
        \item Here's version 1 of the statement of Frobenius reciprocity.
        \begin{itemize}
            \item Recall that we have a scalar product on the space of class function, given by
            \begin{equation*}
                (\chi_1,\chi_2) = \frac{1}{|G|}\sum_{g\in G}\chi_1(g)\chi_2(g^{-1})
            \end{equation*}
            where $\chi_1,\chi_2$ are class functions on $G$.
            \item Recall that if $\chi_1=\chi_V$ and $\chi_2=\chi_W$, then
            \begin{equation*}
                (\chi_1,\chi_2) = \dim\Hom_G(V,W)
                = \dim\Hom_G\left( \bigoplus_{i=1}^kV_i^{n_i},\bigoplus_{i=1}^kV_i^{m_i} \right)
                = \sum_{i=1}^kn_im_i
            \end{equation*}
            \item Then the statement is as follows. If $V$ is a $G$-rep and $W$ is an $H$-rep, then
            \begin{equation*}
                (V,\ind_H^GW)_G = (\res_H^GV,W)_H
            \end{equation*}
            \begin{itemize}
                \item This notation denotes a scalar product in $G$ and scalar product in $H$ of the characters of each representation.
            \end{itemize}
            \item This is similar to the relation between adjoint maps $V\to W$ and $W^*\to V^*$.
        \end{itemize}
        \item Version 2.
        \begin{itemize}
            \item We have that
            \begin{equation*}
                \Hom_G(V,\ind_H^GW) \cong \Hom_H(\res_H^GV,W)
            \end{equation*}
            where the isomorphism is canonical.
            \item We will not check this last definition; we can tediously do it with definitions, and there's nothing complicated. Rudenko leaves this as an exercise to us.
            \item The canonical isomorphism sends a map $v\mapsto[g\mapsto\varphi(gv)]$ to the map $\phi:V\to W$.
        \end{itemize}
    \end{itemize}
    \item We now prove Version 1.
    \begin{proof}
        We have
        \begin{align*}
            (\chi_V,\chi_{\ind_H^GW})_G &= \frac{1}{|G|}\sum_{g_1\in G}\chi_V(g_1)\chi_{\ind_H^GW}(g_1^{-1})\\
            &= \frac{1}{|G|}\sum_{g_1\in G}\chi_V(g_1)\left( \frac{1}{|H|}\sum_{g_2\in G}\tilde{\chi}_W(g_2g_1^{-1}g_2^{-1}) \right)\\
            &= \frac{1}{|H|\cdot|G|}\sum_{g_1,g_2\in G}\chi_V(g_1)\tilde{\chi}_W(g_2g_1^{-1}g_2^{-1})\\
            &= \frac{1}{|H|\cdot|G|}\sum_{g_1,g_2\in G}\chi_V(\underbrace{g_2g_1g_2^{-1}}_h)\tilde{\chi}_W(\underbrace{g_2g_1^{-1}g_2^{-1}}_{h^{-1}})\\
            &= \frac{1}{|H|}\frac{1}{|G|}\sum_{h\in G}|G|\chi_V(h)\tilde{\chi}_W(h^{-1})\\
            &= (\chi_V|_H,\chi_W)_H\\
            &= (\res_H^GV,\chi_W)_H
        \end{align*}
        From line 4 to line 5: Fix $h$; then $g_2g_1g_2^{-1}=h$ iff $g_1=g_2^{-1}hg_2$, so we have overcounted by $|G|$ times. From line 5 to line 6: $\tilde{\chi}_W$ is zero whenever $h^{-1}\notin H$, so this ostensible sum over all $h\in G$ is \emph{de facto} only a sum over all $h\in H$; this is what allows us to consider $\chi_V$ as "restricted to $H$" in line 6.
    \end{proof}
    \item We now come to the branching theorem at long last.
    \item Example first.
    \begin{itemize}
        \item Consider $S_n>S_{n-1}$, where $S_{n-1}$ is the subgroup that fixes $n$. I.e., $S_3>S_2=\{e,(12)\}$, and we explicitly omit $(13),(23),(123),(132)$ because they all move 3.
        \item Let $\lambda\vdash n$.
        \item Let $\mu\leq\lambda$ be a Young diagram of a partition of $n-1$.
        \item Then
        \begin{enumerate}
            \item We have
            \begin{equation*}
                \res_{S_{n-1}}^{S_n}V_\lambda = \bigoplus_{\substack{\mu\leq\lambda\\|\mu|=n-1}}V_\mu
            \end{equation*}
            \begin{itemize}
                \item Example:
                \begin{equation*}
                    \res_{S_4}^{S_5}\tikz[baseline={(0,-0.1)}]{\node{\ydiagram{3,2}};}
                    = \tikz[baseline={(0,-0.1)}]{\node{\ydiagram{2,2}};}
                    \oplus\tikz[baseline={(0,-0.1)}]{\node{\ydiagram{3,1}};}
                \end{equation*}
            \end{itemize}
            \item We have
            \begin{equation*}
                \ind_{S_{n-1}}^{S_n}V_\mu = \bigoplus_{\substack{\mu\leq\lambda\\|\lambda|=n}}V_\lambda
            \end{equation*}
            \begin{itemize}
                \item Example:
                \begin{equation*}
                    \ind_{S_5}^{S_6}\tikz[baseline={(0,-0.1)}]{\node{\ydiagram{3,2}};}
                    = \tikz[baseline={(0,0.05)}]{\node{\ydiagram{3,2,1}};}
                    \oplus\tikz[baseline={(0,-0.1)}]{\node{\ydiagram{3,3}};}
                    \oplus\tikz[baseline={(0,-0.1)}]{\node{\ydiagram{4,2}};}
                \end{equation*}
            \end{itemize}
        \end{enumerate}
    \end{itemize}
    \item The reason that this theorem is called the branching theorem originates from the diagram in Figure \ref{fig:branchingTrm}, which (when continued) encapsulates the main idea of the theorem.
    \begin{figure}[h!]
        \centering
        \begin{tikzpicture}[grow=up,level 1/.style={sibling distance=2.25cm},level 3/.style={sibling distance=1.5cm}]
            \footnotesize
            \node (1) {\ydiagram{1}}
                child {node(2){\ydiagram{1,1}}
                    child {node(3){\ydiagram{1,1,1}}
                        child {node(4){\ydiagram{1,1,1,1}}}
                        child {node[white]{\ydiagram{2,1,1}}}
                    }
                    child {node[white]{\ydiagram{2,1}}}
                }
                child {node{\ydiagram{2}}
                    child {node{\ydiagram{2,1}}
                        child {node{\ydiagram{2,1,1}}}
                        child {node{\ydiagram{2,2}}}
                        child {node[white]{\ydiagram{3,1}}}
                    }
                    child {node{\ydiagram{3}}
                        child {node{\ydiagram{3,1}}}
                        child {node{\ydiagram{4}}}
                    }
                }
            ;
    
            \node at (1 -| 4,0) {$S_1$};
            \node at (2 -| 4,0) {$S_2$};
            \node at (3 -| 4,0) {$S_3$};
            \node at (4 -| 4,0) {$S_4$};
    
            \color{white}
            \node at (1 -| -4,0) {$S_1$};
            \node at (2 -| -4,0) {$S_2$};
            \node at (3 -| -4,0) {$S_3$};
            \node at (4 -| -4,0) {$S_4$};
        \end{tikzpicture}
        \caption{The branching theorem.}
        \label{fig:branchingTrm}
    \end{figure}
    \begin{itemize}
        \item This graph helps you understand induction and restriction.
        \item Dimensions are the number of paths from the bottom to a final Young diagram.
        \begin{itemize}
            \item For example, the dimension of $(3,1)$ is 3 because there are 3 paths to it, listed as follows.
            \begin{enumerate}
                \item $(1)\to(2)\to(3)\to(3,1)$.
                \item $(1)\to(2)\to(2,1)\to(3,1)$.
                \item $(1)\to(1,1)\to(2,1)\to(3,1)$.
            \end{enumerate}
        \end{itemize}
        \item Number of paths is equivalent to number of standard Young tableaux!
    \end{itemize}
    \item Theorem (Branching): The following two statements are true.
    \begin{align}
        \res_{S_{n-1}}^{S_n}V_\lambda &= \bigoplus_{\substack{\mu\leq\lambda\\|\mu|=n-1}}V_\mu\\
        \ind_{S_{n-1}}^{S_n}V_\mu &= \bigoplus_{\substack{\mu\leq\lambda\\|\lambda|=n}}V_\lambda
    \end{align}
    \begin{proof}
        We'll talk about the general idea of the proof now, and maybe do the details next time.\par
        \underline{$(9.1)\Longleftrightarrow(9.2)$}: Suppose first that the left statement above holds true. Then we have that
        \begin{equation*}
            (\res_{S_{n-1}}^{S_n}V_\lambda,V_\mu) =
            \begin{cases}
                0\\
                1 & \lambda\geq\mu
            \end{cases}
        \end{equation*}
        Thus, by Frobenius reciprocity,
        \begin{equation*}
            (V_\lambda,\ind_{S_{n-1}}^{S_n}V_\mu) = (\res_{S_{n-1}}^{S_n}V_\lambda,V_\mu) =
            \begin{cases}
                0\\
                1 & \lambda\geq\mu
            \end{cases}
        \end{equation*}
        Therefore, the second statement holds true. The proof is symmetric in the opposite direction.\par
        \underline{(9.1)}: Let's look at an example. Consider the Young diagram of $S_8$ shown in Figure \ref{fig:YDbranchingPf}.
        \begin{figure}[H]
            \centering
            \ydiagram{4,3,1}
            \caption{Proving the branching theorem.}
            \label{fig:YDbranchingPf}
        \end{figure}
        We want to restrict it down to $S_7$.
        Recall that $V_\lambda=\spn(S_8:\Delta(x_1,x_2,x_3)(x_4-x_5)(x_6-x_7))$.
        Now in $S_7$, we fix $x_8$. Consider subrepresentations of $V_\lambda$ filtered by degree as follows.
        \begin{equation*}
            \underbrace{
                \begin{bmatrix}
                    \\
                \end{bmatrix}
            }_{\deg_{x_3}\leq 0} \leq \underbrace{
                \begin{bmatrix}
                    \\
                    \\
                \end{bmatrix}
            }_{\deg_{x_5}\leq 1} \leq \underbrace{
                \begin{bmatrix}
                     & \\
                     & \\
                \end{bmatrix}
            }_{\deg_{x_8}\leq 2} \leq V_\lambda
        \end{equation*}
        The proof comes from the fact that if we now take quotients of these subrepresentations, e.g., via
        \begin{equation*}
            \deg=0, \deg\leq 1/\deg\leq 0, \deg\leq 2/\deg\leq 1, \dots
        \end{equation*}
        then since $x_8$ can only appear in three boxes, ...
    \end{proof}
    \item Practice with the above example and think it through.
\end{itemize}



\section{The Character of a Symmetric Group Representation}
\begin{itemize}
    \item \marginnote{11/29:}Announcements.
    \begin{itemize}
        \item OH today at 5:30.
        \item Our midterms are graded; we can look at them in his office whenever (I can do this during OH!).
    \end{itemize}
    \item Today, we'll formulate the main result he wants to prove next time.
    \item Goal is still to understand representations of $S_n$.
    \begin{itemize}
        \item We've constructed all of them using Specht modules, but what else do we want?
        \item We have dimension, we want characters, etc.
    \end{itemize}
    \item The main idea is to look at symmetric polynomials once again.
    \begin{itemize}
        \item Consider $\Q[x_1,\dots,x_n]^{S_n}$.
        \item We have proven the fundamental theorem that $\Q[x_1,\dots,x_n]^{S_n}=\Q[\sigma_1,\dots,\sigma_n]$ where $\sigma_k=\sum_{1\leq i_1\leq\cdots\leq i_k\leq n}x_{i_1}\cdots x_{i_k}$.
        \item We also proved in PSet 6, Q6 that these rings are equal to $\Q[p_1,\dots,p_k]$ and $\Q[h_1,\dots,h_k]$ where
        \begin{align*}
            p_k &= \sum_{i=1}^nx_i^k&
            h_k &= \sum_{1\leq i_1\leq\cdots\leq i_k\leq n}x_1\cdots x_k
        \end{align*}
        \begin{itemize}
            \item Example: If $n=3$ and $k=2$, then
            \begin{equation*}
                h_2 = x_1^2+x_2^2+x_3^2+x_1x_2+x_1x_3+x_2x_3
            \end{equation*}
        \end{itemize}
        \item Table of bases for $n$, $k$.
        \begin{table}[h!]
            \centering
            \small
            \renewcommand{\arraystretch}{1.2}
            \begin{tabular}{c|cccc}
                $k\,\backslash\,n$ & 1 & 2 & 3 & 4\\
                \hline
                0 & 1 & 1 & 1 & 1\\
                1 & $x_1$ & $x_1+x_2$ & $x_1+x_2+x_3$ & $\cdots$\\
                2 & $x_1^2$ & $x_1^2+x_2^2$, $x_1x_2$ & $\sigma_1^2$, $\sigma_2^2$ & $\sigma_1^2$, $\sigma_2^2$\\
                3 & $x_1^3$ & ...
            \end{tabular}
            \caption{Polynomial bases.}
            \label{tab:polynomialBases}
        \end{table}
        \item Now take
        \begin{equation*}
            \Lambda_k = [\C[x_1,\dots,x_k]]_{\deg=k-1}
            \cong [\C[x_1,\dots,x_{k+1}]]_{\deg=k-1}
            \cong \cdots
        \end{equation*}
        \begin{itemize}
            \item Alternatively, we can think of this thing as
            \begin{equation*}
                \Lambda_k = (\C[x_1,\dots])_k
            \end{equation*}
            with $\sigma_1^k,\sigma_2\sigma_1^{k-1},\dots$.
        \end{itemize}
        \item We call $\Lambda$ the ring of symmetric functions and define it to be equal to
        \begin{equation*}
            \Lambda = \Q[\sigma_1,\sigma_2,\sigma_3,\dots]
        \end{equation*}
        \item In every complete component, only finitely many of the $\sigma$ will participate, so we get finite things.
        \item This is a graded ring! We have
        \begin{equation*}
            \Lambda = \bigoplus_{k\geq 0}\Lambda_k
        \end{equation*}
        and $\Lambda_k\otimes\Lambda_\ell=\Lambda_{k+\ell}$
        \item This construction is called the \textbf{projective limit}, and we may have encountered it in commutative algebra under the definition
        \begin{equation*}
            \Lambda = \lim_{\rightarrow}\C[x_1,\dots,x_n]^{S_n}
        \end{equation*}
        \item We have identifies such as $p_2=\sigma_1^2-2\sigma_2$. This means that
        \begin{equation*}
            (x_1+\cdots+x_n)^2-2(x_1x_2+x_1x_3+\cdots) = x_1^2+x_2^2+\cdots
        \end{equation*}
        \item Observation: $\dim_\Q\Lambda_n$.
    \end{itemize}
    \item Now, we need to take a vector space on ring representations; we've done this already with the representation ring.
    \item Let $R_n$ be the $\Q$-vector space of functions $\chi:S_n\to\Q$ such that $\chi(x\sigma x^{-1})=\chi(\sigma)$. This is our favorite space of class functions.
    \item Theorem (Frobenius characteristic map): There is an isomorphism of vector spaces and of rings called the Frobenius characteristic: $\ch:\bigoplus_{n\geq 0}R_n\to\Lambda$.
    \begin{proof}
        Take $\chi_V\in R_k$, and $\chi_W\in R_\ell$. Let $V$ an $S_k$-rep, and $W$ an $S_\ell$-rep. We know that
        \begin{equation*}
            S_k\times S_\ell = S_{k+\ell}
        \end{equation*}
        So what we can do is induction $\ind_{S_k\times S_\ell}^{S_{k+\ell}}(V\otimes W)$. Call this operation $\chi_V\boxtimes\chi_W$.

        Now we write down the formula:
        \begin{equation*}
            \ch(\chi) = \frac{1}{n!}\sum_{\sigma\in S_n}\chi(\sigma)p_1^{\lambda_1(\sigma)}\cdots p_k^{\lambda_k(\sigma)}
        \end{equation*}
        where $\lambda_1(\sigma),\lambda_2(\sigma),\dots$ represent the cycle structure of $\sigma$; each $\lambda_i$ is a number of cycles of length $1,2,\dots$.
    \end{proof}
    \item Examples.
    \begin{enumerate}
        \item $S_1$.
        \begin{itemize}
            \item Sends the YD $(1)$ to $p_1=x_1+x_2+x_3+\cdots$.
        \end{itemize}
        \item $S_2$.
        \begin{itemize}
            \item Sends $(2)$ to $\frac{1}{2!}(p_1^2+p_2)=\frac{1}{2}((x_1+x_2)^2+x_1^2+x_2^2)=x_1^2+x_2^2+x_1x_2=h_2$.
            \item It also sends $(1,1)$ to $\frac{1}{2!}(p_1^2-p_2)=\frac{1}{2}((x_1+x_2)^2-x_1^2-x_2^2)=x_1x_2=\sigma_2$.
            \item Let's check our formula. What is $\ind_{S_1\times S_1}^{S_2}(1)\otimes(1)$? Since the induction of the trivial representation is the regular representation, which we can decompose, we know that this induction equals $(1,1)\oplus(2)$. It follows that $p_1^2=x_1^2+x_2^2+x_1x_2+x_1x_2=(x_1+x_2)^2$.
        \end{itemize}
        \item $S_3$.
        \begin{itemize}
            \item Sends $(3)$ to
            \begin{align*}
                \frac{1}{3!}(p_1^3+3p_1p_2+2p_3) &= \frac{1}{6}[(x_1+x_2+x_3)^3+3(x_1+x_2+x_3)(x_1^2+x_2^2+x_3^2)+2(x_1^3+x_2^3+x_3^3)]\\
                &= \frac{1}{6}[6(x_1^3+x_2^3+x_3^3)+6(x_1^2x_2+x_1x_2^2+x_1x_3^2+x_1^2x_3+\cdots)+6x_1x_2x_3]\\
                &= h_3
            \end{align*}
            \item Sends $(1,1,1)$ to
            \begin{align*}
                \frac{1}{3!}(p_1^3-3p_1p_2+2p_3) &= \frac{1}{6}[(x_1+x_2+x_3)^3-3(x_1+x_2+x_3)(x_1^2+x_2^2+x_3^2)+2(x_1^3+x_2^3+x_3^3)]\\
                &= x_1x_2x_3\\
                &= \sigma_3
            \end{align*}
            \item Sends $(2,1)$ to
            \begin{align*}
                \frac{1}{3!}(2p_1^3-p_3) &= \frac{1}{6}[2(x_1^3+x_2^3+x_3^3)+6(x_1^2x_2+\cdots)+12x_1x_2x_3]\\
                &= (x_1^2+\cdots)+2x_1x_2x_3
            \end{align*}
            \item Again, we can check that
            \begin{align*}
                \ind_{S_2\times S_1}^{S_3}[(1,1)\otimes(1)] &= \sigma_1\sigma_2
            \end{align*}
            \begin{itemize}
                \item We compute $\ind_{S_2}^{S_3}(1,1)=(1,1,1)\oplus(2,1)$ via the branching formula: There are only two ways to add a box!
                \item We have $\sigma_1\sigma_2-\sigma_3=(x_1+x_2+x_3)(x_1x_2+x_1x_3+x_2x_3)-x_1x_2x_3$.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
    \item Do we need to be fluent in the techniques by which you expanded all of the polynomials above??
    \item Thus, we have two conjectures:
    \begin{align*}
        \ch[(n)] &= h_n&
        \ch[\underbrace{(1,\dots,1)}_{n\text{ times}}] &= \sigma_n
    \end{align*}
    \item The theorem is cool because it sends all of representation theory to some symmetric polynomial game!
    \item How do we compute $\ch(V_\lambda)$?
    \begin{itemize}
        \item We say it equals $S_\lambda$, where $S_\lambda$ is a Schur polynomial.
        \item Take the YT of $\lambda$. Recall standard YTs.
        \item \textbf{Semistandard} (YT): Things strictly increase in columns, but only monotonically increase in rows. \emph{draw picture!}
        \item The six semistandard ones give us the Schur polynomial.
        \item Relation to RSK correspondence.
    \end{itemize}
    \item Proving why this stuff is true is not hard.
    \item To understand \emph{why} this is true, Google the \textbf{Schur-Weyl duality}.
\end{itemize}



\section{Office Hours}\label{sse:OH9}
\begin{itemize}
    \item I got a 68/100 on the midterm: 30, 24, 0, 14.
    \begin{itemize}
        \item I would have needed to show my work (or at least one example of a calculation) to get full credit for 2, even though it just said "find."
        \item Rudenko did not expect that finding conjugacy classes would be so difficult for us; he will adjust for this difficulty on the final.
    \end{itemize}
    \item Week 3, Lecture 2: You proved that $\inp{\chi_V,\chi_W}=\delta_{VW}$. To do so, you used a projection function $p=(1/|G|)\sum_{g\in G}gv$. You began your proof by proving that $p$ is a $G$-morphism and then never used this result again, as far as I can tell. Did you use it again? See pp. 45-47, 58 (it needs to be a morphism of $G$-representations to map between the representations $V,V^G$?).
    \item Week 3, Lecture 2: Same proof. To prove that $\im(P)=V^G$, do we need more than $p^2=p$? I think so, but you didn't do it explicitly. See pp. 46-47.
    \item Week 3, Lecture 2: Same proof. What's up with the trivial special case? See p. 48.
    \item *Week 3, Lecture 3: Cube thing (see picture from 10/13)?
    \begin{itemize}
        \item It's just a depiction of two different 3-coordinate bases of the same space. It was drawn to illustrate a possible relation between the orthonormal basis $\chi_1,\chi_2,\chi_3$ (cube) and the orthogonal basis $\chi_{C_1},\chi_{C_2},\chi_{C_3}$.
    \end{itemize}
    \item Week 3, Lecture 3: Why did we talk about the infinite-dimensional regular representation here? See p. 50.
    \item *Week 3, Lecture 3: What is the point of the misc. calculations involved in computing the $S_4$ character table? See p. 52.
    \begin{itemize}
        \item Just to check that we were on the right path and shown an example of using the orthogonality relations.
    \end{itemize}
    \item *Week 3, Lecture 3: Proof of the second orthogonality relation your way? It's in \textcite{bib:Serre}, but I don't think that's the way you proved it. See p. 52.
    \begin{itemize}
        \item To begin, note that it is a \emph{highly} nontrivial statement that if $A,B$ are matrices such that $AB=I$, then $BA=I$. It seems so simple to us, but think about it! For an arbitrary matrix $A,B$, $AB$ looks nothing like $BA$! We have two entirely different systems of equations.
        \item However, using this fact, basically it is possible to translate the orthogonality relation for the \emph{columns} into the orthogonality relation about the \emph{rows}.
    \end{itemize}
    \item *Week 3, Lecture 3: All the talk about the exceptional homomorphisms? See p. 52, 61 (the final representation has something to do with an \textbf{involution} of trace 2, and is a representation of a quotient group?).
    \begin{itemize}
        \item So the representation is $\rho:S_4\twoheadrightarrow S_3\xrightarrow{\tilde{\rho}}GL_n$, where $\tilde{\rho}:S_3\to GL_n$ is the representation of $\rho$ corresponding to the character $(2,0,1)$.
    \end{itemize}
    \item *Week 4, Lecture 1: Alternate construction of $R(G)$? See p. 63.
    \item *Week 4, Lecture 1: Extension of scalars with the representation ring? See p. 64.
    \begin{itemize}
        \item We don't need to know anything about this stuff.
        \item What it is though is basically analogous to extending the real numbers into a subset of the complex numbers by treating every $x\in\R$ as $x+0i\in\C$. Very trivial, silly concept.
        \item There is also such a thing as a \textbf{reduction of scalars}.
    \end{itemize}
    \item *Week 4, Lecture 1: Does multiplying a column vector in the basis $\{\delta_{C_i}\}$ by the character table put it in the basis $\{\chi_{V_i^*}\}$, or vice versa? See p. 66.
    \begin{itemize}
        \item Derive it for yourself.
        \item Example: Consider the character table for $S_3$ (Table \ref{tab:charTableS3}) represented as the following matrix.
        \begin{equation*}
            A =
            \begin{bmatrix}
                1 & 1 & 1\\
                1 & -1 & 1\\
                2 & 0 & 1\\
            \end{bmatrix}
        \end{equation*}
        \item Denote the conjugacy classes of $S_3$ by $e$, $(xx)$, $(xxx)$.
        \item Interpretation 1.
        \begin{itemize}
            \item We see that the standard representation is the class function sending
            \begin{align*}
                e &\mapsto 2&
                (xx) &\mapsto 0&
                (xxx) &\mapsto -1
            \end{align*}
            \item Additionally, we have
            \begin{align*}
                e &\mapsto 1& (xx) &\mapsto 0& (xxx) &\mapsto 0\tag{$\delta_e$}\\
                e &\mapsto 0& (xx) &\mapsto 1& (xxx) &\mapsto 0\tag{$\delta_{(xx)}$}\\
                e &\mapsto 0& (xx) &\mapsto 0& (xxx) &\mapsto 1\tag{$\delta_{(xxx)}$}
            \end{align*}
            \item Thus, we can express $\chi_{(2,1)}$ as a linear combination of the $\delta$'s via
            \begin{align*}
                \chi_{(2,1)} &= (2)\delta_e+(0)\delta_{(xx)}+(-1)\delta_{(xxx)}\\
                &= \chi_{(2,1)}(e)\delta_e+\chi_{(2,1)}(xx)\delta_{(xx)}+\chi_{(2,1)}(xxx)\delta_{(xxx)}\\
                &= \sum_{C_i}\chi_{(2,1)}(C_i)\delta_{C_i}
            \end{align*}
            \item It follows in particular that if we represent the $\delta_{C_i}$'s as the standard column vector basis of $\C^3$, then
            \begin{equation*}
                \chi_{(2,1)} = A^T\delta_{(xxx)}
            \end{equation*}
        \end{itemize}
        \item Interpretation 2.
        \begin{itemize}
            \item If we multiply $A$ by the column vector equal to each representation weighted by $|C_i|$, then we recover the $\delta$ basis:
            \begin{align*}
                \begin{bmatrix}
                    1\\
                    0\\
                    0\\
                \end{bmatrix}
                &= A
                \begin{bmatrix}
                    1\cdot 1\\
                    3\cdot 1\\
                    1\cdot 1\\
                \end{bmatrix}&
                \begin{bmatrix}
                    0\\
                    1\\
                    0\\
                \end{bmatrix}
                &= A
                \begin{bmatrix}
                    1\cdot 1\\
                    3\cdot -1\\
                    1\cdot 1\\
                \end{bmatrix}&
                \begin{bmatrix}
                    0\\
                    0\\
                    1\\
                \end{bmatrix}
                &= A
                \begin{bmatrix}
                    1\cdot 2\\
                    3\cdot 0\\
                    1\cdot -1\\
                \end{bmatrix}
            \end{align*}
            \item This interpretation is also what is expressed by the following formula from Lecture 4.2.
            \begin{equation*}
                \delta_{C_j}(g) = \sum_{V_i}\frac{|C_j|\bar{\chi}_{V_i}(g)}{|G|}\cdot\chi_{V_i}(g)
            \end{equation*}
        \end{itemize}
    \end{itemize}
    \item *Week 4, Lecture 2: Isotypical components example. See p. 68.
    \item **Week 4, Lecture 3: Proof the $\C$ is the only finite-dimensional division algebra? See p. 71.
    \begin{itemize}
        \item Let $A$ be an arbitrary finite-dimensional division algebra over $\C$.
        \item To prove that $A=\C$, we will use a bidirectional inclusion proof.
        \item Naturally, $\C\subset A$.
        \item To prove the reverse implication, start by letting $a\in A$ be arbitrary.
        \item Define the left-multiplication operator $L_a:A\to A$ by $x\mapsto ax$ for all $x\in A$.
        \item Recall that $A$ is a complex vector space in addition to being an algebra, the same way a ring is also a group. Thus, $L_a$ is a linear operator on a complex vector space.
        \item It follows by the theorem of linear algebra that $L_a$ has an eigenvalue $\lambda\in F=\C$ and corresponding eigenvector $b\in A$.
        \item Consequently, by the cancellation lemma,
        \begin{align*}
            L_ab &= \lambda b\\
            ab &= \lambda b\\
            a &= \lambda
        \end{align*}
        \item Therefore, $a\in A$ implies $a\in\C$, so $A=\C$.
    \end{itemize}
    \item *Week 6, Lecture 2: Proof that $\sqrt{2}/2$ is not an algebraic integer using Gauss's lemma? See p. 87.
    \begin{itemize}
        \item Let $\alpha:=\sqrt{2}/2$ for the sake of notation.
        \item Suppose for the sake of contradiction that $\alpha$ is an algebraic integer.
        \item Then there exists a monic polonomial $p(x)\in\Z[x]$ such that $p(\alpha)=0$.
        \item Observe that the minimal polynomial in $\Z[x]$ that annihilates $\alpha$ is $2x^2-1$.
        \item Thus, by polynomial division,
        \begin{equation*}
            p(x) = q(x)\cdot(2x^2-1)+r(x)
        \end{equation*}
        for some $q,r\in\Q[x]$ such that $\deg r\leq 2-1$.
        \item We have that
        \begin{equation*}
            r(\alpha) = p(\alpha)-q(\alpha)\cdot(2\alpha^2-1)
            = 0-q(\alpha)\cdot 0
            = 0
        \end{equation*}
        \item Additionally, since $r\in\Q[x]$ and $\deg r\leq 1$, we know that $r(x)=ux+v$ for some $u,v\in\Q$.
        \item We now prove that $u=v=0$.
        \begin{itemize}
            \item Suppose for the sake of contradiction that either $u$ or $v$ was not equal to zero.
            \item Combining the previous two claims reveals that
            \begin{align*}
                0 &= r(\alpha)\\
                &= u\alpha+v\\
                -\frac{v}{u} &= \alpha
            \end{align*}
            \item If $u=0$, then $\alpha$ is undefined and we have arrived at a contradiction. Thus, $u\neq 0$.
            \item Thus, $\alpha\in\Q$. But since $\alpha\notin\Q$ by definition, we have arrived at a contradiction.
            \item Therefore, $u=v=0$.
        \end{itemize}
        \item Having established that $r=0$, we know that $p=(2x^2-1)q$, i.e., $2x^2-1$ divides $p$.
        \item Now define $N$ to be the least common multiple of the denominators of the coefficients of $q$.
        \item Consider
        \begin{equation*}
            Np = (Nq)(2x^2-1)
        \end{equation*}
        \item It follows by Gauss's lemma that
        \begin{align*}
            c(Np) &= c[(Nq)(2x^2-1)]\\
            N &= c(Nq)\cdot c(2x^2-1)\\
            &= 1\cdot 1\\
            &= 1
        \end{align*}
        where $c$ denotes the \textbf{content}.
        \item But if $N=1$, then $q\in\Z[x]$, so leading term of $p$ --- equal to the product of $2x^2$ and the leading term of $q$ --- has a coefficient that is a multiple of 2, i.e., is \emph{not} equal to 1 as is required of a monic polynomial, a contradiction.
    \end{itemize}
    \item *Week 6, Lecture 3: Questions about Lemma 1 of the proof of Burnside's theorem. See p. 92.
    \begin{itemize}
        \item The roots $a_1,\dots,a_k$ of the minimal polynomial of the algebraic integer $a$ are known as \textbf{conjugate algebraic integers}.
        \item The conjugate algebraic integers of a root of unity are also roots of unity.
        \begin{itemize}
            \item Suppose $\varepsilon$ is a root of unity.
            \item Then the minimal polynomial of $\varepsilon$ is $x^n-1$ for some $n\in\N$.
            \item Naturally, the roots of this polynomial (the conjugate algebraic integers to $\varepsilon$) are all of the other roots of unity of order $n$.
        \end{itemize}
        \item The conjugate algebraic integers of a sum of roots of unity is a sum of roots of unity.
        \begin{itemize}
            \item It can be shown that the minimal polynomial for $\varepsilon_1+\varepsilon_2$ is
            \begin{equation*}
                p(x) = \prod_{i,j=1}^n(x-\varepsilon_1^i-\varepsilon_2^j)
            \end{equation*}
            \item Evidently, the above polynomial is symmetric under permutations of $\varepsilon_1^i,\varepsilon_2^j$, and we'd generate the same polynomial with any $\pm\varepsilon_1^i\pm\varepsilon_2^j$ as starting material.
            \item Explicit example.
            \begin{itemize}
                \item $\pm\sqrt{2}$ are conjugate algebraic integers, as solutions to $x^2-2$. Similarly, $\pm\sqrt{3}$ are conjugate algebraic integers as solutions to $x^2=3$.
                \item Thus, we expect the minimal polynomial for $\sqrt{2}+\sqrt{3}$ to be
                \begin{equation*}
                    p(x) = (x-\sqrt{2}-\sqrt{3})(x-\sqrt{2}+\sqrt{3})(x+\sqrt{2}-\sqrt{3})(x+\sqrt{2}+\sqrt{3})
                \end{equation*}
                \item Expanding, we obtain 
                \begin{align*}
                    p(x) &= (x^2-(\sqrt{2}+\sqrt{3})^2)(x^2-(\sqrt{2}-\sqrt{3})^2)\\
                    &= x^4-[(\sqrt{2}+\sqrt{3})^2+(\sqrt{2}-\sqrt{3})^2]x^2+(\sqrt{2}+\sqrt{3})^2(\sqrt{2}-\sqrt{3})^2\\
                    &= x^4-10x^2+1
                \end{align*}
                \item Indeed, the above polynomial is a monic polynomial 
                \item From the definition, this polynomial is evidently also the minimal polynomial for $\sqrt{2}-\sqrt{3}$, $-\sqrt{2}+\sqrt{3}$, and $-\sqrt{2}-\sqrt{3}$.
                \item Thus, the conjugate algebraic integers of $\sqrt{2}+\sqrt{3}$ are the four sums of all individual algebraic integers.
            \end{itemize}
        \end{itemize}
        \item How do we extend this argument to the case in the problem?? What about when $\varepsilon_1=-1$ and $\varepsilon_2=i$ so that simple powers don't access every combination as the $p(x)$ formula does?
        \item We know that $\prod_{i=1}^na_i\in\Z$ because of Vieta's formula.
        \begin{itemize}
            \item In particular, this tells us that $x_1\cdots x_n$ is equal to the last coefficient in the minimal polynomial which, by definition, is an integer.
        \end{itemize}
        \item Don't worry too much about all this, though: Burnside's theorem will no longer be on the final because Rudenko changed his mind.
    \end{itemize}
    \item *Week 7, Lecture 2: Symmetric polynomials and roots of symmetric polynomials. See p. 101.
    \item *Week 7, Lecture 2: Word in blackboard picture? See p. 102.
    \begin{itemize}
        \item "Remain" to show\dots
    \end{itemize}
    \item *Week 7, Lecture 2: What is $d$ in the proof of the alternating polynomials theorem? See p. 103.
    \begin{itemize}
        \item $d=n-1$.
    \end{itemize}
    \item *Week 8, Lecture 2: What is $d$ in the definition on p. 111.
    \begin{itemize}
        \item Consider the Specht polynomial corresponding to $(2,2,1)$.
        \begin{figure}[h!]
            \centering
            \ydiagram{2,2,1}
            \caption{Young diagram for $(2,2,1)$.}
            \label{fig:YD221}
        \end{figure}
        \item Since $(2,2,1)'=(3,2)$, the Specht polynomial is $(x_1-x_2)(x_1-x_3)(x_2-x_3)\cdot(x_4-x_5)$.
        \item $\Delta_{123}$ is of degree $3=\binom{3}{2}$ because looking at the first column of the YD, which corresponds to $\lambda_1'=3$, out of the 3 boxes, we must choose 2 for each of the three terms $(x_1-x_2),(x_1-x_3),(x_2-x_3)$. Then we just add this to the 2 choose 2 for the second column of the YD.
    \end{itemize}
    \item *Week 9, Lecture 2: Do we need to be fluent in the techniques you used to expand and reduce the various polynomial powers? How did you do that again?
\end{itemize}



\section{The Frobenius Characteristic Map}
\begin{itemize}
    \item \marginnote{12/1:}Proving the theorem.
    \item The statement is that there exists a function
    \begin{equation*}
        \ch:\underbrace{\bigoplus_{n\geq 0}\Q_\text{cl}(S_n)}_{\{f:S_n\to\Q:f(\sigma=\sigma^{-1})=f(i)\}}\to\bigoplus_{n\geq 0}\Lambda_n
    \end{equation*}
    where $\Lambda_n=\Q[\sigma_1,\sigma_2,\sigma_3,\dots]_{\deg=n}=[\Q[x_1,\dots,x_n]^{S_n}]_{\deg=n}$. Note that by convention, $\Lambda_0=\Q$. This function is given by
    \begin{equation*}
        \ch(\chi) = \frac{1}{n!}\sum_{\sigma\in S_n}\chi(\sigma)\cdot p_1^{\lambda_1(\sigma)}\cdot p_n^{\lambda_n(\sigma)}
    \end{equation*}
    where $p_k=x_1^k+x_2^k+\cdots$ and $\lambda_i(\sigma)$ is the number of cycles in $\sigma$ of length $i$. Moreover, $\ch$ is an isomorphism of $\Q$-algebras. To create a product of $V$ a $S_n$-rep and $W$ an $S_m$-rep, we map
    \begin{equation*}
        V\boxtimes W = \ind_{S_n\times S_m}^{S_{n+m}}(V\otimes W)
    \end{equation*}
    \begin{proof}
        To prove that $\ch$ is a ring isomorphism, we need\dots
        \begin{enumerate}
            \item $\boxtimes$ is associative;
            \item $\ch(\chi_1\boxtimes\chi_2)=\ch(\chi_1)\cdot\ch(\chi_2)$;
            \item $\ch((n))=h_n$.
        \end{enumerate}
        1,2,3 imply the theorem because 2,3 imply that $\ch$ is surjective, $\Lambda_n$ has a $\Q$-basis $h_{\lambda_1}\cdots h_{\lambda_n}$ for $\lambda_1\leq\cdots\leq\lambda_n$ and $\lambda_1+\lambda_2+\cdots+\lambda_n=n$. For example, for $\Lambda_5$, we have $h_1^5,h_1^3h_2,h_1,h_2^2,h_2h_3,h_1^2h_3,h_1h_4,h_5$. This surjectivity combined with the fact that $\dim\Q_\text{cl}[S_n]=\dim\Lambda_n$ implies that $\ch$ is an isomorphism of rings.

        Last thing: $\ch[(n)]=\frac{1}{n!}\sum p_1^{c_1(\sigma)}\cdots p_n^{c_n(\sigma)}$ where $c_i(\sigma)$ denotes the number of cycles of length $i$ and hence $\sum ic_i=n$. Denote $p_1^{c_1(\sigma)}\cdots p_n^{c_n(\sigma)}$ by $p^{c(\sigma)}$.
        \begin{proof}
            Let
            \begin{align*}
                \sum h_n t^n &= \sum(\sum_{i_1\leq\cdots\leq i_n}x_{i_1}\cdots x_{i_n}t^n)\\
                &= \frac{1}{1-x_1t}\cdot\frac{1}{1-x_2t}\cdots\frac{1}{1-x_nt}\\
                &= \exp(\log(\prod_{i=1}^n\frac{1}{1-x_it}))\\
                &= \exp(\sum_{i=1}^n-\log(1-x_it))\\
                &= \exp(x_1+\frac{x_1^2t^2}{2}+\frac{x_1^3t^3}{3}+\cdots+x_2+\frac{x_2^2t^2}{2}+\cdots)\\
                &= \exp(p_1+\frac{p_2t^2}{2}+\frac{p_3t^3}{3}+\cdots)\\
                &= \prod_{m\geq 1}\exp(\frac{p_mt^m}{m})\\
                &= *
            \end{align*}
            We get the second equality because each $1/(1-x_it)=1+x_i+x_i^2t^2+\cdots$.
            We need the power series $-\log(1-t)=t+t^2/2+\cdots$ and $\exp(t)=1+t+t^2/2!+\cdots$. Thus, $\exp(\log(1-t))=1-t$.
            Now note that
            \begin{align*}
                \sum_{n\geq 0}\ch[(n)]\cdot t^n &= \sum_{n\geq 0}\frac{t^n}{n!}\sum_{\sigma\in S_n}p_1^{c_1(\sigma)}\cdots p_n^{c_n(\sigma)}\\
                &= \sum_{n\geq 0}\frac{t^n}{n!}\left( \sum_{a_1+2a_2+\cdots+na_n=n}p_1^{a_1}\cdots p_n^{a_n} \right)\cdot\frac{n!}{1^{a_1}a_1!2^{a_2}a_2!\cdots n^{a_n}a_n!}\\
                &= \sum_{\substack{n\geq 0\\a_1,\dots,a_n:a_1+2a_2+\cdots+na_n=n}}\frac{1}{a_1!}\left( \frac{p_1}{1} \right)^{a_1}t^{a_1}\frac{1}{a_2!}\left( \frac{p_2}{2} \right)^{a_2}t^{a_2}\cdots\frac{1}{a_n!}\left( \frac{p_n}{n} \right)^{a_n}t^{a_n}\\
                &= \prod_{k=1}^n\left( \sum_{a_k=1}^\infty\frac{(p_k)^{a_k}t^{a_kk}}{a_k!ka_k} \right)\\
                &= \prod_{k\geq 1}\exp(\frac{p_kt^k}{k})\\
                &= *
            \end{align*}
            We're overcounting because we can cyclically permute cycles (i.e., $(12)=(21)$), hence the correction factor in the second line above.

            Note: This exponent/logarithm trick is a common computational trick in combinatorics, varieties, etc.
        \end{proof}
        Now we prove the part 3, i.e., that $\boxtimes$ is associative. We do this by direct computation.
        \begin{equation*}
            \underbrace{\ind_{S_{n+m}\times S_\ell}^{S_{n+m+\ell}}\left[ \ind_{S_n\times S_m}^{S_{n+m}}(\chi_1\otimes\chi_2) \right]\otimes\chi_3}_{(\chi_1\boxtimes\chi_2)\boxtimes\chi_3} = \ind_{s_n\times S_m\times S_\ell}^{S_{n+m+\ell}}(\chi_1\otimes\chi_2\otimes\chi_3)
        \end{equation*}
        ...
        
        Then proving 2 (homomorphism bit) is the hardest. We have
        \begin{align*}
            \ch(\ind_{S_n\times S_m}^{S_{n+m}}(\chi_1\otimes\chi_2)) &= \frac{1}{n!}\sum_{\sigma\in S_n}(\ind_{S_n\times S_m}^{S_{n+m}}\chi_1\otimes\chi_2)(\sigma)\underbrace{p_1^{c_1(\sigma)}\cdots p_{n+m}^{c_{n+m}(\sigma)}}_\psi\\
            &= \inp{\ind_{S_n\times S_m}^{S_{n+m}}(\chi_1\otimes\chi_2),\psi}_{S_{n+m}}\\
            &= \inp{\chi_1\otimes\chi_2,\res_{S_n\times S_m}^{S_{n+m}}\psi}\\
            &= \sum_{\substack{\sigma_1\in S_n\\\sigma_2\in S_m}}\chi_1(\sigma_1)\chi_2(\sigma_2)p_1^{c_1(\sigma_1)}\cdots p_n^{c_n(\sigma_1)}p_1^{c_1(\sigma_2)}\cdots p_m^{c_m(\sigma_2)}\\
            &= \ch(\chi_1)\ch(\chi_2)
        \end{align*}
        We use Frobenius reciprocity somewhere in here. We also have $\psi:S_n\to\Lambda_n$ and $\psi(\tau\sigma\tau^{-1})=\psi(\sigma)$.
    \end{proof}
    \item After another 10 years of trying to understand the representations of the symmetric group, we'll be here.
    \item At this point, we can study compact Lie groups.
\end{itemize}




\end{document}