\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{8}

\begin{document}




\chapter{Symmetric Group Representation Characteristics}
\section{Frobenius Reciprocity; The Branching Theorem}
\begin{itemize}
    \item \marginnote{11/27:}Announcements.
    \begin{itemize}
        \item OH on Wednesday at 5:30 PM this week; not Tuesday.
        \item There will be extra OH next week pre-exam.
        \begin{itemize}
            \item Roughly like Monday/Wednesday next week.
        \end{itemize}
        \item Midterm will be returned on Wednesday; we can pick them up in-person in his office starting then.
        \item There are some grade boundaries: Pass/Fail we can do until Friday, withdrawal we can do until 5:00 PM today.
    \end{itemize}
    \item Let's finish the conversation about induction/restriction and prove the \textbf{branching theorem}.
    \item Reminder to start.
    \begin{itemize}
        \item We have two mathematical categories, $G$-reps and $H$-reps where $H\leq G$.
        \item These catagories are related by functors.
        \begin{itemize}
            \item $\res_H^G:G\text{-reps}\to H\text{-reps}$ and vice versa for $\ind_H^G$.
            \item See Figure \ref{fig:induceFunctor}.
        \end{itemize}
        \item Restrictions are stupidly simple.
        \item Inductions, most hands-on, we take copies of $W$ times cosets. Formulaically,
        \begin{equation*}
            \ind_H^GW = g_1W\oplus\cdots\oplus g_kW
        \end{equation*}
        where $k=(G:H)$ and $G=\bigsqcup_{i=1}^kg_iH$.
        \begin{itemize}
            \item In more detail, the action of $g$ on $g_iw$ is that of $g_{\sigma(i)}h_iw$.
            \item This is a genuinely hard construction.
            \item A matrix of this thing will be a block-permutation matrix like
            \def\hashfill{\tikz{
                \clip (0,0) rectangle (0.7,0.2);
                \foreach \x in {-0.1,0,...,0.7} {
                    \draw (\x,0) -- ++(0.3,0.3);
                }
            }}
            \begin{equation*}
                \begin{bNiceArray}[first-row,first-col,margin]{c|w{c}{8mm}|c}
                        & g_1W & & g_kW\\
                    g_1W & \hashfill & 0 & 0\\ \hline
                        & 0 & 0 & \hashfill\\ \hline
                    g_kW & 0 & \hashfill & 0\\
                \end{bNiceArray}
            \end{equation*}
        \end{itemize}
        \item As an alternate construction, we have that
        \begin{equation*}
            g_1W\oplus\cdots\oplus g_kW \cong \Hom_H(\C[G],W)
        \end{equation*}
        \begin{itemize}
            \item Recall that elements of the set on the right above are functions $f:G\to W$ such that $f(h(g))=hf(g)$.
            \item We map between the two via $f(g)\mapsto f(gx')$.
        \end{itemize}
        \item What is nice about induced representations is that $\dim[\ind_H^GW]=(\dim W)[G:H]$.
        \item There is a very easy statement of the character of an induced representation, the \textbf{Frobenius formula}.
        \begin{itemize}
            \item Recall that
            \begin{equation*}
                \tilde{\chi}_W(g) =
                \begin{cases}
                    0 & g\notin H\\
                    \chi_W(g) & g\in H
                \end{cases}
            \end{equation*}
            \item With this, we average:
            \begin{equation*}
                \chi_{\ind_H^GW}(g) = \frac{1}{|H|}\sum_{x\in G}\tilde{\chi}_W(xgx^{-1})
            \end{equation*}
            \item Essentially, we're taking a whole bunch of conjugates, summing them up, and dividing to get rid of overcounting.
        \end{itemize}
    \end{itemize}
    \item We now move onto \textbf{Frobenius reciprocity}, which is a relation between the functors/relations $\ind_H^G$ and $\res_H^G$.
    \begin{itemize}
        \item The first point where category theory gets interesting is the notion of \textbf{adjoint functors}, which we are about to touch on. It is a very subtle notion.
        \item Here's version 1 of the statement of Frobenius reciprocity.
        \begin{itemize}
            \item Recall that we have a scalar product on the space of class function, given by
            \begin{equation*}
                (\chi_1,\chi_2) = \frac{1}{|G|}\sum_{g\in G}\chi_1(g)\chi_2(g^{-1})
            \end{equation*}
            where $\chi_1,\chi_2$ are class functions on $G$.
            \item Recall that if $\chi_1=\chi_V$ and $\chi_2=\chi_W$, then
            \begin{equation*}
                (\chi_1,\chi_2) = \dim\Hom_G(V,W)
                = \dim\Hom_G\left( \bigoplus_{i=1}^kV_i^{n_i},\bigoplus_{i=1}^kV_i^{m_i} \right)
                = \sum_{i=1}^kn_im_i
            \end{equation*}
            \item Then the statement is as follows. If $V$ is a $G$-rep and $W$ is an $H$-rep, then
            \begin{equation*}
                (V,\ind_H^GW)_G = (\res_H^GV,W)_H
            \end{equation*}
            \begin{itemize}
                \item This notation denotes a scalar product in $G$ and scalar product in $H$ of the characters of each representation.
            \end{itemize}
            \item This is similar to the relation between adjoint maps $V\to W$ and $W^*\to V^*$.
        \end{itemize}
        \item Version 2.
        \begin{itemize}
            \item We have that
            \begin{equation*}
                \Hom_G(V,\ind_H^GW) \cong \Hom_H(\res_H^GV,W)
            \end{equation*}
            where the isomorphism is canonical.
            \item We will not check this last definition; we can tediously do it with definitions, and there's nothing complicated. Rudenko leaves this as an exercise to us.
            \item The canonical isomorphism sends a map $v\mapsto[g\mapsto\varphi(gv)]$ to the map $\phi:V\to W$.
        \end{itemize}
    \end{itemize}
    \item We now prove Version 1.
    \begin{proof}
        We have
        \begin{align*}
            (\chi_V,\chi_{\ind_H^GW})_G &= \frac{1}{|G|}\sum_{g_1\in G}\chi_V(g_1)\chi_{\ind_H^GW}(g_1^{-1})\\
            &= \frac{1}{|G|}\sum_{g_1\in G}\chi_V(g_1)\left( \frac{1}{|H|}\sum_{g_2\in G}\tilde{\chi}_W(g_2g_1^{-1}g_2^{-1}) \right)\\
            &= \frac{1}{|H|\cdot|G|}\sum_{g_1,g_2\in G}\chi_V(g_1)\tilde{\chi}_W(g_2g_1^{-1}g_2^{-1})\\
            &= \frac{1}{|H|\cdot|G|}\sum_{g_1,g_2\in G}\chi_V(\underbrace{g_2g_1g_2^{-1}}_h)\tilde{\chi}_W(\underbrace{g_2g_1^{-1}g_2^{-1}}_{h^{-1}})\\
            &= \frac{1}{|H|}\frac{1}{|G|}\sum_{h\in G}|G|\chi_V(h)\tilde{\chi}_W(h^{-1})\\
            &= (\chi_V|_H,\chi_W)_H\\
            &= (\chi_{\res_H^GV},\chi_W)_H
        \end{align*}
        From line 4 to line 5: Fix $h$; then $g_2g_1g_2^{-1}=h$ iff $g_1=g_2^{-1}hg_2$, so we have overcounted by $|G|$ times. From line 5 to line 6: $\tilde{\chi}_W$ is zero whenever $h^{-1}\notin H$, so this ostensible sum over all $h\in G$ is \emph{de facto} only a sum over all $h\in H$; this is what allows us to consider $\chi_V$ as "restricted to $H$" in line 6.
    \end{proof}
    \item We now come to the branching theorem at long last.
    \item Example first.
    \begin{itemize}
        \item Consider $S_n>S_{n-1}$, where $S_{n-1}$ is the subgroup that fixes $n$. I.e., $S_3>S_2=\{e,(12)\}$, and we explicitly omit $(13),(23),(123),(132)$ because they all move 3.
        \item Let $\lambda\vdash n$.
        \item Let $\mu\leq\lambda$ be a Young diagram of a partition of $n-1$.
        \item Then
        \begin{enumerate}
            \item We have
            \begin{equation*}
                \res_{S_{n-1}}^{S_n}V_\lambda = \bigoplus_{\substack{\mu\leq\lambda\\|\mu|=n-1}}V_\mu
            \end{equation*}
            \begin{itemize}
                \item Example:
                \begin{equation*}
                    \res_{S_4}^{S_5}\tikz[baseline={(0,-0.1)}]{\node{\ydiagram{3,2}};}
                    = \tikz[baseline={(0,-0.1)}]{\node{\ydiagram{2,2}};}
                    \oplus\tikz[baseline={(0,-0.1)}]{\node{\ydiagram{3,1}};}
                \end{equation*}
            \end{itemize}
            \item We have
            \begin{equation*}
                \ind_{S_{n-1}}^{S_n}V_\mu = \bigoplus_{\substack{\mu\leq\lambda\\|\lambda|=n}}V_\lambda
            \end{equation*}
            \begin{itemize}
                \item Example:
                \begin{equation*}
                    \ind_{S_5}^{S_6}\tikz[baseline={(0,-0.1)}]{\node{\ydiagram{3,2}};}
                    = \tikz[baseline={(0,0.05)}]{\node{\ydiagram{3,2,1}};}
                    \oplus\tikz[baseline={(0,-0.1)}]{\node{\ydiagram{3,3}};}
                    \oplus\tikz[baseline={(0,-0.1)}]{\node{\ydiagram{4,2}};}
                \end{equation*}
            \end{itemize}
        \end{enumerate}
    \end{itemize}
    \item The reason that this theorem is called the branching theorem originates from the diagram in Figure \ref{fig:branchingTrm}, which (when continued) encapsulates the main idea of the theorem.
    \begin{figure}[h!]
        \centering
        \begin{tikzpicture}[grow=up,level 1/.style={sibling distance=2.25cm},level 3/.style={sibling distance=1.5cm}]
            \footnotesize
            \node (1) {\ydiagram{1}}
                child {node(2){\ydiagram{1,1}}
                    child {node(3){\ydiagram{1,1,1}}
                        child {node(4){\ydiagram{1,1,1,1}}}
                        child {node[white]{\ydiagram{2,1,1}}}
                    }
                    child {node[white]{\ydiagram{2,1}}}
                }
                child {node{\ydiagram{2}}
                    child {node{\ydiagram{2,1}}
                        child {node{\ydiagram{2,1,1}}}
                        child {node{\ydiagram{2,2}}}
                        child {node[white]{\ydiagram{3,1}}}
                    }
                    child {node{\ydiagram{3}}
                        child {node{\ydiagram{3,1}}}
                        child {node{\ydiagram{4}}}
                    }
                }
            ;
    
            \node at (1 -| 4,0) {$S_1$};
            \node at (2 -| 4,0) {$S_2$};
            \node at (3 -| 4,0) {$S_3$};
            \node at (4 -| 4,0) {$S_4$};
    
            \color{white}
            \node at (1 -| -4,0) {$S_1$};
            \node at (2 -| -4,0) {$S_2$};
            \node at (3 -| -4,0) {$S_3$};
            \node at (4 -| -4,0) {$S_4$};
        \end{tikzpicture}
        \caption{The branching theorem.}
        \label{fig:branchingTrm}
    \end{figure}
    \begin{itemize}
        \item This graph helps you understand induction and restriction.
        \item Dimensions are the number of paths from the bottom to a final Young diagram.
        \begin{itemize}
            \item For example, the dimension of $(3,1)$ is 3 because there are 3 paths to it, listed as follows.
            \begin{enumerate}
                \item $(1)\to(2)\to(3)\to(3,1)$.
                \item $(1)\to(2)\to(2,1)\to(3,1)$.
                \item $(1)\to(1,1)\to(2,1)\to(3,1)$.
            \end{enumerate}
        \end{itemize}
        \item Number of paths is equivalent to number of standard Young tableaux!
    \end{itemize}
    \item Theorem (Branching): The following two statements are true.
    \begin{align}
        \res_{S_{n-1}}^{S_n}V_\lambda &= \bigoplus_{\substack{\mu\leq\lambda\\|\mu|=n-1}}V_\mu\\
        \ind_{S_{n-1}}^{S_n}V_\mu &= \bigoplus_{\substack{\mu\leq\lambda\\|\lambda|=n}}V_\lambda
    \end{align}
    \begin{proof}
        We'll talk about the general idea of the proof now, and maybe do the details next time.\par
        \underline{$(9.1)\Longleftrightarrow(9.2)$}: Suppose first that the left statement above holds true. Then we have that
        \begin{equation*}
            (\res_{S_{n-1}}^{S_n}V_\lambda,V_\mu) =
            \begin{cases}
                0\\
                1 & \lambda\geq\mu
            \end{cases}
        \end{equation*}
        Thus, by Frobenius reciprocity,
        \begin{equation*}
            (V_\lambda,\ind_{S_{n-1}}^{S_n}V_\mu) = (\res_{S_{n-1}}^{S_n}V_\lambda,V_\mu) =
            \begin{cases}
                0\\
                1 & \lambda\geq\mu
            \end{cases}
        \end{equation*}
        Therefore, the second statement holds true. The proof is symmetric in the opposite direction.\par
        \underline{(9.1)}: Let's look at an example. Consider the Young diagram of $S_8$ shown in Figure \ref{fig:YDbranchingPf}.
        \begin{figure}[H]
            \centering
            \ydiagram{4,3,1}
            \caption{Proving the branching theorem.}
            \label{fig:YDbranchingPf}
        \end{figure}
        We want to restrict it down to $S_7$.
        Recall that $V_\lambda=\spn(S_8:\Delta(x_1,x_2,x_3)(x_4-x_5)(x_6-x_7))$.
        Now in $S_7$, we fix $x_8$. Consider subrepresentations of $V_\lambda$ filtered by degree as follows.
        \begin{equation*}
            \underbrace{
                \begin{bmatrix}
                    \\
                \end{bmatrix}
            }_{\deg_{x_3}\leq 0} \leq \underbrace{
                \begin{bmatrix}
                    \\
                    \\
                \end{bmatrix}
            }_{\deg_{x_5}\leq 1} \leq \underbrace{
                \begin{bmatrix}
                     & \\
                     & \\
                \end{bmatrix}
            }_{\deg_{x_8}\leq 2} \leq V_\lambda
        \end{equation*}
        The proof comes from the fact that if we now take quotients of these subrepresentations, e.g., via
        \begin{equation*}
            \deg=0, \deg\leq 1/\deg\leq 0, \deg\leq 2/\deg\leq 1, \dots
        \end{equation*}
        then since $x_8$ can only appear in three boxes, ...
    \end{proof}
    \item Practice with the above example and think it through.
\end{itemize}



\section{The Character of a Symmetric Group Representation}
\begin{itemize}
    \item \marginnote{11/29:}Announcements.
    \begin{itemize}
        \item OH today at 5:30.
        \item Our midterms are graded; we can look at them in his office whenever (I can do this during OH!).
    \end{itemize}
    \item Today, we'll formulate the main result he wants to prove next time.
    \item Goal is still to understand representations of $S_n$.
    \begin{itemize}
        \item We've constructed all of them using Specht modules, but what else do we want?
        \item We have dimension, we want characters, etc.
    \end{itemize}
    \item The main idea is to look at symmetric polynomials once again.
    \begin{itemize}
        \item Consider $\Q[x_1,\dots,x_n]^{S_n}$.
        \item We have proven the fundamental theorem that $\Q[x_1,\dots,x_n]^{S_n}=\Q[\sigma_1,\dots,\sigma_n]$ where $\sigma_k=\sum_{1\leq i_1\leq\cdots\leq i_k\leq n}x_{i_1}\cdots x_{i_k}$.
        \item We also proved in PSet 6, Q6 that these rings are equal to $\Q[p_1,\dots,p_k]$ and $\Q[h_1,\dots,h_k]$ where
        \begin{align*}
            p_k &= \sum_{i=1}^nx_i^k&
            h_k &= \sum_{1\leq i_1\leq\cdots\leq i_k\leq n}x_1\cdots x_k
        \end{align*}
        \begin{itemize}
            \item Example: If $n=3$ and $k=2$, then
            \begin{equation*}
                h_2 = x_1^2+x_2^2+x_3^2+x_1x_2+x_1x_3+x_2x_3
            \end{equation*}
        \end{itemize}
        \item Table of bases for $n$, $k$.
        \begin{table}[h!]
            \centering
            \small
            \renewcommand{\arraystretch}{1.2}
            \begin{tabular}{c|cccc}
                $k\,\backslash\,n$ & 1 & 2 & 3 & 4\\
                \hline
                0 & 1 & 1 & 1 & 1\\
                1 & $x_1$ & $x_1+x_2$ & $x_1+x_2+x_3$ & $\cdots$\\
                2 & $x_1^2$ & $x_1^2+x_2^2$, $x_1x_2$ & $\sigma_1^2$, $\sigma_2^2$ & $\sigma_1^2$, $\sigma_2^2$\\
                3 & $x_1^3$ & ...
            \end{tabular}
            \caption{Polynomial bases.}
            \label{tab:polynomialBases}
        \end{table}
        \item Now take
        \begin{equation*}
            \Lambda_k = [\C[x_1,\dots,x_k]]_{\deg=k-1}
            \cong [\C[x_1,\dots,x_{k+1}]]_{\deg=k-1}
            \cong \cdots
        \end{equation*}
        \begin{itemize}
            \item Alternatively, we can think of this thing as
            \begin{equation*}
                \Lambda_k = (\C[x_1,\dots])_k
            \end{equation*}
            with $\sigma_1^k,\sigma_2\sigma_1^{k-1},\dots$.
        \end{itemize}
        \item We call $\Lambda$ the ring of symmetric functions and define it to be equal to
        \begin{equation*}
            \Lambda = \Q[\sigma_1,\sigma_2,\sigma_3,\dots]
        \end{equation*}
        \item In every complete component, only finitely many of the $\sigma$ will participate, so we get finite things.
        \item This is a graded ring! We have
        \begin{equation*}
            \Lambda = \bigoplus_{k\geq 0}\Lambda_k
        \end{equation*}
        and $\Lambda_k\otimes\Lambda_\ell=\Lambda_{k+\ell}$
        \item This construction is called the \textbf{projective limit}, and we may have encountered it in commutative algebra under the definition
        \begin{equation*}
            \Lambda = \lim_{\rightarrow}\C[x_1,\dots,x_n]^{S_n}
        \end{equation*}
        \item We have identifies such as $p_2=\sigma_1^2-2\sigma_2$. This means that
        \begin{equation*}
            (x_1+\cdots+x_n)^2-2(x_1x_2+x_1x_3+\cdots) = x_1^2+x_2^2+\cdots
        \end{equation*}
        \item Observation: $\dim_\Q\Lambda_n$.
    \end{itemize}
    \item Now, we need to take a vector space on ring representations; we've done this already with the representation ring.
    \item Let $R_n$ be the $\Q$-vector space of functions $\chi:S_n\to\Q$ such that $\chi(x\sigma x^{-1})=\chi(\sigma)$. This is our favorite space of class functions.
    \item Theorem (Frobenius characteristic map): There is an isomorphism of vector spaces and of rings called the Frobenius characteristic: $\ch:\bigoplus_{n\geq 0}R_n\to\Lambda$.
    \begin{proof}
        Take $\chi_V\in R_k$, and $\chi_W\in R_\ell$. Let $V$ an $S_k$-rep, and $W$ an $S_\ell$-rep. We know that
        \begin{equation*}
            S_k\times S_\ell = S_{k+\ell}
        \end{equation*}
        So what we can do is induction $\ind_{S_k\times S_\ell}^{S_{k+\ell}}(V\otimes W)$. Call this operation $\chi_V\boxtimes\chi_W$.

        Now we write down the formula:
        \begin{equation*}
            \ch(\chi) = \frac{1}{n!}\sum_{\sigma\in S_n}\chi(\sigma)p_1^{\lambda_1(\sigma)}\cdots p_k^{\lambda_k(\sigma)}
        \end{equation*}
        where $\lambda_1(\sigma),\lambda_2(\sigma),\dots$ represent the cycle structure of $\sigma$; each $\lambda_i$ is a number of cycles of length $1,2,\dots$.
    \end{proof}
    \item Examples.
    \begin{enumerate}
        \item $S_1$.
        \begin{itemize}
            \item Sends the YD $(1)$ to $p_1=x_1+x_2+x_3+\cdots$.
        \end{itemize}
        \item $S_2$.
        \begin{itemize}
            \item Sends $(2)$ to $\frac{1}{2!}(p_1^2+p_2)=\frac{1}{2}((x_1+x_2)^2+x_1^2+x_2^2)=x_1^2+x_2^2+x_1x_2=h_2$.
            \item It also sends $(1,1)$ to $\frac{1}{2!}(p_1^2-p_2)=\frac{1}{2}((x_1+x_2)^2-x_1^2-x_2^2)=x_1x_2=\sigma_2$.
            \item Let's check our formula. What is $\ind_{S_1\times S_1}^{S_2}(1)\otimes(1)$? Since the induction of the trivial representation is the regular representation, which we can decompose, we know that this induction equals $(1,1)\oplus(2)$. It follows that $p_1^2=x_1^2+x_2^2+x_1x_2+x_1x_2=(x_1+x_2)^2$.
        \end{itemize}
        \item $S_3$.
        \begin{itemize}
            \item Sends $(3)$ to
            \begin{align*}
                \frac{1}{3!}(p_1^3+3p_1p_2+2p_3) &= \frac{1}{6}[(x_1+x_2+x_3)^3+3(x_1+x_2+x_3)(x_1^2+x_2^2+x_3^2)+2(x_1^3+x_2^3+x_3^3)]\\
                &= \frac{1}{6}[6(x_1^3+x_2^3+x_3^3)+6(x_1^2x_2+x_1x_2^2+x_1x_3^2+x_1^2x_3+\cdots)+6x_1x_2x_3]\\
                &= h_3
            \end{align*}
            \item Sends $(1,1,1)$ to
            \begin{align*}
                \frac{1}{3!}(p_1^3-3p_1p_2+2p_3) &= \frac{1}{6}[(x_1+x_2+x_3)^3-3(x_1+x_2+x_3)(x_1^2+x_2^2+x_3^2)+2(x_1^3+x_2^3+x_3^3)]\\
                &= x_1x_2x_3\\
                &= \sigma_3
            \end{align*}
            \item Sends $(2,1)$ to
            \begin{align*}
                \frac{1}{3!}(2p_1^3-p_3) &= \frac{1}{6}[2(x_1^3+x_2^3+x_3^3)+6(x_1^2x_2+\cdots)+12x_1x_2x_3]\\
                &= (x_1^2+\cdots)+2x_1x_2x_3
            \end{align*}
            \item Again, we can check that
            \begin{align*}
                \ind_{S_2\times S_1}^{S_3}[(1,1)\otimes(1)] &= \sigma_1\sigma_2
            \end{align*}
            \begin{itemize}
                \item We compute $\ind_{S_2}^{S_3}(1,1)=(1,1,1)\oplus(2,1)$ via the branching formula: There are only two ways to add a box!
                \item We have $\sigma_1\sigma_2-\sigma_3=(x_1+x_2+x_3)(x_1x_2+x_1x_3+x_2x_3)-x_1x_2x_3$.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
    \item Do we need to be fluent in the techniques by which you expanded all of the polynomials above??
    \item Thus, we have two conjectures:
    \begin{align*}
        \ch[(n)] &= h_n&
        \ch[\underbrace{(1,\dots,1)}_{n\text{ times}}] &= \sigma_n
    \end{align*}
    \item The theorem is cool because it sends all of representation theory to some symmetric polynomial game!
    \item How do we compute $\ch(V_\lambda)$?
    \begin{itemize}
        \item We say it equals $S_\lambda$, where $S_\lambda$ is a Schur polynomial.
        \item Take the YT of $\lambda$. Recall standard YTs.
        \item \textbf{Semistandard} (YT): Things strictly increase in columns, but only monotonically increase in rows. \emph{draw picture!}
        \item The six semistandard ones give us the Schur polynomial.
        \item Relation to RSK correspondence.
    \end{itemize}
    \item Proving why this stuff is true is not hard.
    \item To understand \emph{why} this is true, Google the \textbf{Schur-Weyl duality}.
\end{itemize}



\section{Office Hours}\label{sse:OH9}
\begin{itemize}
    \item I got a 68/100 on the midterm: 30, 24, 0, 14.
    \begin{itemize}
        \item I would have needed to show my work (or at least one example of a calculation) to get full credit for 2, even though it just said "find."
        \item Rudenko did not expect that finding conjugacy classes would be so difficult for us; he will adjust for this difficulty on the final.
    \end{itemize}
    \item Week 3, Lecture 2: You proved that $\inp{\chi_V,\chi_W}=\delta_{VW}$. To do so, you used a projection function $p=(1/|G|)\sum_{g\in G}gv$. You began your proof by proving that $p$ is a $G$-morphism and then never used this result again, as far as I can tell. Did you use it again? See pp. 45-47, 58 (it needs to be a morphism of $G$-representations to map between the representations $V,V^G$?).
    \item Week 3, Lecture 2: Same proof. To prove that $\im(P)=V^G$, do we need more than $p^2=p$? I think so, but you didn't do it explicitly. See pp. 46-47.
    \item Week 3, Lecture 2: Same proof. What's up with the trivial special case? See p. 48.
    \item *Week 3, Lecture 3: Cube thing (see picture from 10/13)?
    \begin{itemize}
        \item It's just a depiction of two different 3-coordinate bases of the same space. It was drawn to illustrate a possible relation between the orthonormal basis $\chi_1,\chi_2,\chi_3$ (cube) and the orthogonal basis $\chi_{C_1},\chi_{C_2},\chi_{C_3}$.
    \end{itemize}
    \item Week 3, Lecture 3: Why did we talk about the infinite-dimensional regular representation here? See p. 50.
    \item *Week 3, Lecture 3: What is the point of the misc. calculations involved in computing the $S_4$ character table? See p. 52.
    \begin{itemize}
        \item Just to check that we were on the right path and shown an example of using the orthogonality relations.
    \end{itemize}
    \item *Week 3, Lecture 3: Proof of the second orthogonality relation your way? It's in \textcite{bib:Serre}, but I don't think that's the way you proved it. See p. 52.
    \begin{itemize}
        \item To begin, note that it is a \emph{highly} nontrivial statement that if $A,B$ are matrices such that $AB=I$, then $BA=I$. It seems so simple to us, but think about it! For an arbitrary matrix $A,B$, $AB$ looks nothing like $BA$! We have two entirely different systems of equations.
        \item However, using this fact, basically it is possible to translate the orthogonality relation for the \emph{columns} into the orthogonality relation about the \emph{rows}.
    \end{itemize}
    \item *Week 3, Lecture 3: All the talk about the exceptional homomorphisms? See p. 52, 61 (the final representation has something to do with an \textbf{involution} of trace 2, and is a representation of a quotient group?).
    \begin{itemize}
        \item So the representation is $\rho:S_4\twoheadrightarrow S_3\xrightarrow{\tilde{\rho}}GL_n$, where $\tilde{\rho}:S_3\to GL_n$ is the representation of $\rho$ corresponding to the character $(2,0,1)$.
    \end{itemize}
    \item *Week 4, Lecture 1: Alternate construction of $R(G)$? See p. 63.
    \item *Week 4, Lecture 1: Extension of scalars with the representation ring? See p. 64.
    \begin{itemize}
        \item We don't need to know anything about this stuff.
        \item What it is though is basically analogous to extending the real numbers into a subset of the complex numbers by treating every $x\in\R$ as $x+0i\in\C$. Very trivial, silly concept.
        \item There is also such a thing as a \textbf{reduction of scalars}.
    \end{itemize}
    \item *Week 4, Lecture 1: Does multiplying a column vector in the basis $\{\delta_{C_i}\}$ by the character table put it in the basis $\{\chi_{V_i^*}\}$, or vice versa? See p. 66.
    \begin{itemize}
        \item Derive it for yourself.
        \item Example: Consider the character table for $S_3$ (Table \ref{tab:charTableS3}) represented as the following matrix.
        \begin{equation*}
            A =
            \begin{bmatrix}
                1 & 1 & 1\\
                1 & -1 & 1\\
                2 & 0 & 1\\
            \end{bmatrix}
        \end{equation*}
        \item Denote the conjugacy classes of $S_3$ by $e$, $(xx)$, $(xxx)$.
        \item Interpretation 1.
        \begin{itemize}
            \item We see that the standard representation is the class function sending
            \begin{align*}
                e &\mapsto 2&
                (xx) &\mapsto 0&
                (xxx) &\mapsto -1
            \end{align*}
            \item Additionally, we have
            \begin{align*}
                e &\mapsto 1& (xx) &\mapsto 0& (xxx) &\mapsto 0\tag{$\delta_e$}\\
                e &\mapsto 0& (xx) &\mapsto 1& (xxx) &\mapsto 0\tag{$\delta_{(xx)}$}\\
                e &\mapsto 0& (xx) &\mapsto 0& (xxx) &\mapsto 1\tag{$\delta_{(xxx)}$}
            \end{align*}
            \item Thus, we can express $\chi_{(2,1)}$ as a linear combination of the $\delta$'s via
            \begin{align*}
                \chi_{(2,1)} &= (2)\delta_e+(0)\delta_{(xx)}+(-1)\delta_{(xxx)}\\
                &= \chi_{(2,1)}(e)\delta_e+\chi_{(2,1)}(xx)\delta_{(xx)}+\chi_{(2,1)}(xxx)\delta_{(xxx)}\\
                &= \sum_{C_i}\chi_{(2,1)}(C_i)\delta_{C_i}
            \end{align*}
            \item It follows in particular that if we represent the $\delta_{C_i}$'s as the standard column vector basis of $\C^3$, then
            \begin{equation*}
                \chi_{(2,1)} = A^T\delta_{(xxx)}
            \end{equation*}
        \end{itemize}
        \item Interpretation 2.
        \begin{itemize}
            \item If we multiply $A$ by the column vector equal to each representation weighted by $|C_i|$, then we recover the $\delta$ basis:
            \begin{align*}
                \begin{bmatrix}
                    1\\
                    0\\
                    0\\
                \end{bmatrix}
                &= A
                \begin{bmatrix}
                    1\cdot 1\\
                    3\cdot 1\\
                    1\cdot 1\\
                \end{bmatrix}&
                \begin{bmatrix}
                    0\\
                    1\\
                    0\\
                \end{bmatrix}
                &= A
                \begin{bmatrix}
                    1\cdot 1\\
                    3\cdot -1\\
                    1\cdot 1\\
                \end{bmatrix}&
                \begin{bmatrix}
                    0\\
                    0\\
                    1\\
                \end{bmatrix}
                &= A
                \begin{bmatrix}
                    1\cdot 2\\
                    3\cdot 0\\
                    1\cdot -1\\
                \end{bmatrix}
            \end{align*}
            \item This interpretation is also what is expressed by the following formula from Lecture 4.2.
            \begin{equation*}
                \delta_{C_j}(g) = \sum_{V_i}\frac{|C_j|\bar{\chi}_{V_i}(g)}{|G|}\cdot\chi_{V_i}(g)
            \end{equation*}
        \end{itemize}
    \end{itemize}
    \item *Week 4, Lecture 2: Isotypical components example. See p. 68.
    \item **Week 4, Lecture 3: Proof the $\C$ is the only finite-dimensional division algebra? See p. 71.
    \begin{itemize}
        \item Let $A$ be an arbitrary finite-dimensional division algebra over $\C$.
        \item To prove that $A=\C$, we will use a bidirectional inclusion proof.
        \item Naturally, $\C\subset A$.
        \item To prove the reverse implication, start by letting $a\in A$ be arbitrary.
        \item Define the left-multiplication operator $L_a:A\to A$ by $x\mapsto ax$ for all $x\in A$.
        \item Recall that $A$ is a complex vector space in addition to being an algebra, the same way a ring is also a group. Thus, $L_a$ is a linear operator on a complex vector space.
        \item It follows by the theorem of linear algebra that $L_a$ has an eigenvalue $\lambda\in F=\C$ and corresponding eigenvector $b\in A$.
        \item Consequently, by the cancellation lemma,
        \begin{align*}
            L_ab &= \lambda b\\
            ab &= \lambda b\\
            a &= \lambda
        \end{align*}
        \item Therefore, $a\in A$ implies $a\in\C$, so $A=\C$.
    \end{itemize}
    \item *Week 6, Lecture 2: Proof that $\sqrt{2}/2$ is not an algebraic integer using Gauss's lemma? See p. 87.
    \begin{itemize}
        \item Let $\alpha:=\sqrt{2}/2$ for the sake of notation.
        \item Suppose for the sake of contradiction that $\alpha$ is an algebraic integer.
        \item Then there exists a monic polonomial $p(x)\in\Z[x]$ such that $p(\alpha)=0$.
        \item Observe that the minimal polynomial in $\Z[x]$ that annihilates $\alpha$ is $2x^2-1$.
        \item Thus, by polynomial division,
        \begin{equation*}
            p(x) = q(x)\cdot(2x^2-1)+r(x)
        \end{equation*}
        for some $q,r\in\Q[x]$ such that $\deg r\leq 2-1$.
        \item We have that
        \begin{equation*}
            r(\alpha) = p(\alpha)-q(\alpha)\cdot(2\alpha^2-1)
            = 0-q(\alpha)\cdot 0
            = 0
        \end{equation*}
        \item Additionally, since $r\in\Q[x]$ and $\deg r\leq 1$, we know that $r(x)=ux+v$ for some $u,v\in\Q$.
        \item We now prove that $u=v=0$.
        \begin{itemize}
            \item Suppose for the sake of contradiction that either $u$ or $v$ was not equal to zero.
            \item Combining the previous two claims reveals that
            \begin{align*}
                0 &= r(\alpha)\\
                &= u\alpha+v\\
                -\frac{v}{u} &= \alpha
            \end{align*}
            \item If $u=0$, then $\alpha$ is undefined and we have arrived at a contradiction. Thus, $u\neq 0$.
            \item Thus, $\alpha\in\Q$. But since $\alpha\notin\Q$ by definition, we have arrived at a contradiction.
            \item Therefore, $u=v=0$.
        \end{itemize}
        \item Having established that $r=0$, we know that $p=(2x^2-1)q$, i.e., $2x^2-1$ divides $p$.
        \item Now define $N$ to be the least common multiple of the denominators of the coefficients of $q$.
        \item Consider
        \begin{equation*}
            Np = (Nq)(2x^2-1)
        \end{equation*}
        \item It follows by Gauss's lemma that
        \begin{align*}
            c(Np) &= c[(Nq)(2x^2-1)]\\
            N &= c(Nq)\cdot c(2x^2-1)\\
            &= 1\cdot 1\\
            &= 1
        \end{align*}
        where $c$ denotes the \textbf{content}.
        \item But if $N=1$, then $q\in\Z[x]$, so leading term of $p$ --- equal to the product of $2x^2$ and the leading term of $q$ --- has a coefficient that is a multiple of 2, i.e., is \emph{not} equal to 1 as is required of a monic polynomial, a contradiction.
    \end{itemize}
    \item *Week 6, Lecture 3: Questions about Lemma 1 of the proof of Burnside's theorem. See p. 92.
    \begin{itemize}
        \item The roots $a_1,\dots,a_k$ of the minimal polynomial of the algebraic integer $a$ are known as \textbf{conjugate algebraic integers}.
        \item The conjugate algebraic integers of a root of unity are also roots of unity.
        \begin{itemize}
            \item Suppose $\varepsilon$ is a root of unity.
            \item Then the minimal polynomial of $\varepsilon$ is $x^n-1$ for some $n\in\N$.
            \item Naturally, the roots of this polynomial (the conjugate algebraic integers to $\varepsilon$) are all of the other roots of unity of order $n$.
        \end{itemize}
        \item The conjugate algebraic integers of a sum of roots of unity is a sum of roots of unity.
        \begin{itemize}
            \item It can be shown that the minimal polynomial for $\varepsilon_1+\varepsilon_2$ is
            \begin{equation*}
                p(x) = \prod_{i,j=1}^n(x-\varepsilon_1^i-\varepsilon_2^j)
            \end{equation*}
            \item Evidently, the above polynomial is symmetric under permutations of $\varepsilon_1^i,\varepsilon_2^j$, and we'd generate the same polynomial with any $\pm\varepsilon_1^i\pm\varepsilon_2^j$ as starting material.
            \item Explicit example.
            \begin{itemize}
                \item $\pm\sqrt{2}$ are conjugate algebraic integers, as solutions to $x^2-2$. Similarly, $\pm\sqrt{3}$ are conjugate algebraic integers as solutions to $x^2=3$.
                \item Thus, we expect the minimal polynomial for $\sqrt{2}+\sqrt{3}$ to be
                \begin{equation*}
                    p(x) = (x-\sqrt{2}-\sqrt{3})(x-\sqrt{2}+\sqrt{3})(x+\sqrt{2}-\sqrt{3})(x+\sqrt{2}+\sqrt{3})
                \end{equation*}
                \item Expanding, we obtain 
                \begin{align*}
                    p(x) &= (x^2-(\sqrt{2}+\sqrt{3})^2)(x^2-(\sqrt{2}-\sqrt{3})^2)\\
                    &= x^4-[(\sqrt{2}+\sqrt{3})^2+(\sqrt{2}-\sqrt{3})^2]x^2+(\sqrt{2}+\sqrt{3})^2(\sqrt{2}-\sqrt{3})^2\\
                    &= x^4-10x^2+1
                \end{align*}
                \item Indeed, the above polynomial is a monic polynomial 
                \item From the definition, this polynomial is evidently also the minimal polynomial for $\sqrt{2}-\sqrt{3}$, $-\sqrt{2}+\sqrt{3}$, and $-\sqrt{2}-\sqrt{3}$.
                \item Thus, the conjugate algebraic integers of $\sqrt{2}+\sqrt{3}$ are the four sums of all individual algebraic integers.
            \end{itemize}
        \end{itemize}
        \item How do we extend this argument to the case in the problem?? What about when $\varepsilon_1=-1$ and $\varepsilon_2=i$ so that simple powers don't access every combination as the $p(x)$ formula does?
        \item We know that $\prod_{i=1}^na_i\in\Z$ because of Vieta's formula.
        \begin{itemize}
            \item In particular, this tells us that $x_1\cdots x_n$ is equal to the last coefficient in the minimal polynomial which, by definition, is an integer.
        \end{itemize}
        \item Don't worry too much about all this, though: Burnside's theorem will no longer be on the final because Rudenko changed his mind.
    \end{itemize}
    \item *Week 7, Lecture 2: Symmetric polynomials and roots of symmetric polynomials. See p. 101.
    \item *Week 7, Lecture 2: Word in blackboard picture? See p. 102.
    \begin{itemize}
        \item "Remain" to show\dots
    \end{itemize}
    \item *Week 7, Lecture 2: What is $d$ in the proof of the alternating polynomials theorem? See p. 103.
    \begin{itemize}
        \item $d=n-1$.
    \end{itemize}
    \item *Week 8, Lecture 2: What is $d$ in the definition on p. 111.
    \begin{itemize}
        \item Consider the Specht polynomial corresponding to $(2,2,1)$.
        \begin{figure}[h!]
            \centering
            \ydiagram{2,2,1}
            \caption{Young diagram for $(2,2,1)$.}
            \label{fig:YD221}
        \end{figure}
        \item Since $(2,2,1)'=(3,2)$, the Specht polynomial is $(x_1-x_2)(x_1-x_3)(x_2-x_3)\cdot(x_4-x_5)$.
        \item $\Delta_{123}$ is of degree $3=\binom{3}{2}$ because looking at the first column of the YD, which corresponds to $\lambda_1'=3$, out of the 3 boxes, we must choose 2 for each of the three terms $(x_1-x_2),(x_1-x_3),(x_2-x_3)$. Then we just add this to the 2 choose 2 for the second column of the YD.
    \end{itemize}
    \item *Week 9, Lecture 2: Do we need to be fluent in the techniques you used to expand and reduce the various polynomial powers? How did you do that again?
\end{itemize}



\section{The Frobenius Characteristic Map}
\begin{itemize}
    \item \marginnote{12/1:}Proving the theorem.
    \item The statement is that there exists a function
    \begin{equation*}
        \ch:\underbrace{\bigoplus_{n\geq 0}\Q_\text{cl}(S_n)}_{\{f:S_n\to\Q:f(\sigma=\sigma^{-1})=f(i)\}}\to\bigoplus_{n\geq 0}\Lambda_n
    \end{equation*}
    where $\Lambda_n=\Q[\sigma_1,\sigma_2,\sigma_3,\dots]_{\deg=n}=[\Q[x_1,\dots,x_n]^{S_n}]_{\deg=n}$. Note that by convention, $\Lambda_0=\Q$. This function is given by
    \begin{equation*}
        \ch(\chi) = \frac{1}{n!}\sum_{\sigma\in S_n}\chi(\sigma)\cdot p_1^{\lambda_1(\sigma)}\cdot p_n^{\lambda_n(\sigma)}
    \end{equation*}
    where $p_k=x_1^k+x_2^k+\cdots$ and $\lambda_i(\sigma)$ is the number of cycles in $\sigma$ of length $i$. Moreover, $\ch$ is an isomorphism of $\Q$-algebras. To create a product of $V$ a $S_n$-rep and $W$ an $S_m$-rep, we map
    \begin{equation*}
        V\boxtimes W = \ind_{S_n\times S_m}^{S_{n+m}}(V\otimes W)
    \end{equation*}
    \begin{proof}
        To prove that $\ch$ is a ring isomorphism, we need\dots
        \begin{enumerate}
            \item $\boxtimes$ is associative;
            \item $\ch(\chi_1\boxtimes\chi_2)=\ch(\chi_1)\cdot\ch(\chi_2)$;
            \item $\ch((n))=h_n$.
        \end{enumerate}
        1,2,3 imply the theorem because 2,3 imply that $\ch$ is surjective, $\Lambda_n$ has a $\Q$-basis $h_{\lambda_1}\cdots h_{\lambda_n}$ for $\lambda_1\leq\cdots\leq\lambda_n$ and $\lambda_1+\lambda_2+\cdots+\lambda_n=n$. For example, for $\Lambda_5$, we have $h_1^5,h_1^3h_2,h_1,h_2^2,h_2h_3,h_1^2h_3,h_1h_4,h_5$. This surjectivity combined with the fact that $\dim\Q_\text{cl}[S_n]=\dim\Lambda_n$ implies that $\ch$ is an isomorphism of rings.

        Last thing: $\ch[(n)]=\frac{1}{n!}\sum p_1^{c_1(\sigma)}\cdots p_n^{c_n(\sigma)}$ where $c_i(\sigma)$ denotes the number of cycles of length $i$ and hence $\sum ic_i=n$. Denote $p_1^{c_1(\sigma)}\cdots p_n^{c_n(\sigma)}$ by $p^{c(\sigma)}$.
        \begin{proof}
            Let
            \begin{align*}
                \sum h_n t^n &= \sum(\sum_{i_1\leq\cdots\leq i_n}x_{i_1}\cdots x_{i_n}t^n)\\
                &= \frac{1}{1-x_1t}\cdot\frac{1}{1-x_2t}\cdots\frac{1}{1-x_nt}\\
                &= \exp(\log(\prod_{i=1}^n\frac{1}{1-x_it}))\\
                &= \exp(\sum_{i=1}^n-\log(1-x_it))\\
                &= \exp(x_1+\frac{x_1^2t^2}{2}+\frac{x_1^3t^3}{3}+\cdots+x_2+\frac{x_2^2t^2}{2}+\cdots)\\
                &= \exp(p_1+\frac{p_2t^2}{2}+\frac{p_3t^3}{3}+\cdots)\\
                &= \prod_{m\geq 1}\exp(\frac{p_mt^m}{m})\\
                &= *
            \end{align*}
            We get the second equality because each $1/(1-x_it)=1+x_i+x_i^2t^2+\cdots$.
            We need the power series $-\log(1-t)=t+t^2/2+\cdots$ and $\exp(t)=1+t+t^2/2!+\cdots$. Thus, $\exp(\log(1-t))=1-t$.
            Now note that
            \begin{align*}
                \sum_{n\geq 0}\ch[(n)]\cdot t^n &= \sum_{n\geq 0}\frac{t^n}{n!}\sum_{\sigma\in S_n}p_1^{c_1(\sigma)}\cdots p_n^{c_n(\sigma)}\\
                &= \sum_{n\geq 0}\frac{t^n}{n!}\left( \sum_{a_1+2a_2+\cdots+na_n=n}p_1^{a_1}\cdots p_n^{a_n} \right)\cdot\frac{n!}{1^{a_1}a_1!2^{a_2}a_2!\cdots n^{a_n}a_n!}\\
                &= \sum_{\substack{n\geq 0\\a_1,\dots,a_n:a_1+2a_2+\cdots+na_n=n}}\frac{1}{a_1!}\left( \frac{p_1}{1} \right)^{a_1}t^{a_1}\frac{1}{a_2!}\left( \frac{p_2}{2} \right)^{a_2}t^{a_2}\cdots\frac{1}{a_n!}\left( \frac{p_n}{n} \right)^{a_n}t^{a_n}\\
                &= \prod_{k=1}^n\left( \sum_{a_k=1}^\infty\frac{(p_k)^{a_k}t^{a_kk}}{a_k!ka_k} \right)\\
                &= \prod_{k\geq 1}\exp(\frac{p_kt^k}{k})\\
                &= *
            \end{align*}
            We're overcounting because we can cyclically permute cycles (i.e., $(12)=(21)$), hence the correction factor in the second line above.

            Note: This exponent/logarithm trick is a common computational trick in combinatorics, varieties, etc.
        \end{proof}
        Now we prove the part 3, i.e., that $\boxtimes$ is associative. We do this by direct computation.
        \begin{equation*}
            \underbrace{\ind_{S_{n+m}\times S_\ell}^{S_{n+m+\ell}}\left[ \ind_{S_n\times S_m}^{S_{n+m}}(\chi_1\otimes\chi_2) \right]\otimes\chi_3}_{(\chi_1\boxtimes\chi_2)\boxtimes\chi_3} = \ind_{s_n\times S_m\times S_\ell}^{S_{n+m+\ell}}(\chi_1\otimes\chi_2\otimes\chi_3)
        \end{equation*}
        ...
        
        Then proving 2 (homomorphism bit) is the hardest. We have
        \begin{align*}
            \ch(\ind_{S_n\times S_m}^{S_{n+m}}(\chi_1\otimes\chi_2)) &= \frac{1}{n!}\sum_{\sigma\in S_n}(\ind_{S_n\times S_m}^{S_{n+m}}\chi_1\otimes\chi_2)(\sigma)\underbrace{p_1^{c_1(\sigma)}\cdots p_{n+m}^{c_{n+m}(\sigma)}}_\psi\\
            &= \inp{\ind_{S_n\times S_m}^{S_{n+m}}(\chi_1\otimes\chi_2),\psi}_{S_{n+m}}\\
            &= \inp{\chi_1\otimes\chi_2,\res_{S_n\times S_m}^{S_{n+m}}\psi}\\
            &= \sum_{\substack{\sigma_1\in S_n\\\sigma_2\in S_m}}\chi_1(\sigma_1)\chi_2(\sigma_2)p_1^{c_1(\sigma_1)}\cdots p_n^{c_n(\sigma_1)}p_1^{c_1(\sigma_2)}\cdots p_m^{c_m(\sigma_2)}\\
            &= \ch(\chi_1)\ch(\chi_2)
        \end{align*}
        We use Frobenius reciprocity somewhere in here. We also have $\psi:S_n\to\Lambda_n$ and $\psi(\tau\sigma\tau^{-1})=\psi(\sigma)$.
    \end{proof}
    \item After another 10 years of trying to understand the representations of the symmetric group, we'll be here.
    \item At this point, we can study compact Lie groups.
\end{itemize}



\section{Final Review Sheet}
\begin{itemize}
    \item \marginnote{1/18:}Problem-by-problem breakdown.
    \item (30 points) Question 1: One of the following three prompts. I can memorize answers to these. This is for speed and for credit.
    \item Define algebraic integers. Prove that they form a ring. Prove that the dimension of an irreducible representation divides the order of the group.
    \begin{proof}[Answer]
        ${\color{white}hi}$\\
        \textbf{Algebraic integers}: The set of all $x\in\C$ such that there exist some $a_0,\dots,a_{n-1}\in\Z$ such that
        \begin{equation*}
            x^n+a_{n-1}x^{n-1}+\cdots+a_1x+a_0 = 0
        \end{equation*}
        To prove that the algebraic integers form a ring, it will suffice to show that if $x,y\in\bar{\Z}$, then $x+y,xy\in\bar{\Z}$. Let $x,y\in\bar{\Z}$. Then there exist monic polynomials with integer coefficients such that
        \begin{align*}
            x^n+a_{n-1}x^{n-1}+\cdots+a_0 &= 0&
            y^m+b_{m-1}y^{m-1}+\cdots+b_0 &= 0
        \end{align*}
        To each of these polynomials corresponds a matrix with integer coefficients for which $x,y$, respectively, is an eigenvalue. Explicitly, these matrices are the Frobenius matrices
        \begin{align*}
            A &=
            \begin{bmatrix}
                0 & 1 &  & \\
                 & \ddots & \ddots & \\
                 &  & 0 & 1\\
                -a_0 & -a_1 & \cdots & -a_{n-1}\\
            \end{bmatrix}&
            B &=
            \begin{bmatrix}
                0 & 1 &  & \\
                 & \ddots & \ddots & \\
                 &  & 0 & 1\\
                -b_0 & -b_1 & \cdots & -b_{m-1}\\
            \end{bmatrix}
        \end{align*}
        which have characteristic polynomials equal to the above polynomials and thus have $x,y$ as eigenvalues. To these eigenvalues must correspond some eigenvectors $v,w$, which we will not explicitly solve for. The important thing is that we now know that
        \begin{align*}
            Av &= xv&
            Bw &= yw
        \end{align*}
        Now, using the Kronecker product, we have that
        \begin{align*}
            (A\otimes B)(v\otimes w) &= Av\otimes Bw
                = xv\otimes yw
                = xy(v\otimes w)\\
            (A\otimes I_m+I_n\otimes B)(v\otimes w) &= Av\otimes I_mw+I_nv\otimes Bw
                = xv\otimes 1w+1v\otimes yw
                % = x(v\otimes w)+y(v\otimes w)
                = (x+y)(v\otimes w)
        \end{align*}
        Thus, $x+y,xy$ are eigenvalues of matrices with integer coefficients. Thus, they are zeroes of the characteristic polynomials, which will be monic polynomials with coefficients that are the sums and products of integers, and hence are integers, too. Therefore, $x+y,xy\in\bar{\Z}$.\par\medskip
        We now prove that the dimension $d_V$ of an irrep divides the order $|G|$ of a group.\par
        We begin with three definitions: Let $C:=\{g_1,\dots,g_s\}\subset G$ be a conjugacy class of the finite group $G$ where naturally $s=|C|$, let $e_C:=g_1+\cdots+g_s\in\Z[G]\subset\C[G]$, and let $\rho:\C[G]\to GL(V)$ be the group homomorphism corresponding to the irreducible representation $V$.\par
        Recall that the $e_C$'s for all conjugacy classes form a basis of the center of the group algebra $\C[G]$. Thus, $e_C\in Z(\C[G])$. It follows easily that $\rho(e_C)$ satisfies Schur's lemma for associative algebras, and from there that there exists $\lambda\in\C$ such that $\rho(e_C)=\lambda I_{d_V}$. Moreover,
        \begin{align*}
            \rho(g_1+\cdots+g_s) &= \lambda I_{d_V}\\
            \tr(\rho(g_1+\cdots+g_s)) &= \tr(\lambda I_{d_V})\\
            \tr(\rho(g_1))+\cdots+\tr(\rho(g_s)) &= \lambda\tr(I_{d_V})\\
            |C|\chi(C) &= \lambda d_V\\
            \lambda &= \frac{|C|\chi(C)}{d_V}
        \end{align*}
        Additionally, we can prove that $\lambda\in\bar{\Z}$. Recall that there exist $a_0,\dots,a_{n-1}\in\Z$ such that
        \begin{equation*}
            e_C^n+a_{n-1}e_C^{n-1}+\cdots+a_0 = 0
        \end{equation*}
        Recall that the essential reason that this is true is that the left multiplication by $e_C$ map from $\Z[G]\to\Z[G]$ will have eigenvalue $e_C$ (by definition) and a matrix of integers (because of its domain and range), so we use the characteristic polynomial as before. Anyway, back to the point: Let $v\in V$ be nonzero. Then
        \begin{align*}
            0 &= \rho(0)v\\
            &= \rho(e_C^n+a_{n-1}e_C^{n-1}+\cdots+a_0)v\\
            &= \left[ \rho(e_C)^n+a_{n-1}\rho(e_C)^{n-1}+\cdots+a_0 \right]v\\
            &= \left[ \lambda^n+a_{n-1}\lambda^{n-1}+\cdots+a_0 \right]I_{d_V}v\\
            &= \left[ \lambda^n+a_{n-1}\lambda^{n-1}+\cdots+a_0 \right]v
        \end{align*}
        It follows by the zero product property that the coefficient in brackets must be zero, giving us the monic, integer coefficient polynomial we need to prove that $\lambda\in\bar{\Z}$.\par
        Additionally, recall that $\chi(C)\in\bar{\Z}$. This is because $g_i^n=e$ for some $n$, so $\rho(g)$ may be diagonal with roots of unity along the diagonal (i.e., \emph{unitary}), so the character is the sum of $n^\text{th}$ roots of unity. All of these roots of unity are algebraic integers, meaning that their sum is, too, since we've already proven that the algebraic integers form a ring. Moreover, it follows because complex roots of a polynomial imply that their conjugate is also a root that $\overline{\chi(C)}\in\bar{\Z}$, too. Thus, by the first orthogonality relation,
        \begin{align*}
            |G| &= \sum_C|C|\chi(C)\overline{\chi(C)}\\
            \frac{|G|}{d_V} &= \sum_C\frac{|C|\chi(C)}{d_V}\cdot\overline{\chi(C)}
        \end{align*}
        Thus, since $\bar{\Z}$ is a ring, $|G|/d_V\in\bar{\Z}$. Naturally, it is also in $\Q$. It follows that it is in $\Q\cap\bar{\Z}=\Z$. Note that this last equality holds via a somewhat complicated polynomial argument with prime divisors and such. Regardless, since $|G|/d_V$ is an integer, this means that $d_V\mid|G|$, as desired.
    \end{proof}
    \item Explain the construction of irreducible representations of $S_n$ via Specht polynomials. Prove irreducibility of these representations.
    \begin{proof}[Answer]
        ${\color{white}hi}$\\
        Roughly speaking, we establish a bijective correspondence between the partitions $\lambda\vdash n$ and the irreps of $S_n$ as follows. Take a partition of $n$. For the sake of clarity, we will look at a representative example. Consider $S_7$ and let $\lambda=(4,2,1)$. Then letting $\lambda'=(3,2,1,1)$, we define the Specht polynomial of $\lambda$ to be
        \begin{equation*}
            \Sp_\lambda(x_1,\dots,x_7) := \Delta(x_1,x_2,x_3)\Delta(x_4,x_5)\Delta(x_6)\Delta(x_7)
        \end{equation*}
        based off of the inverse partition. Each $\Delta$ term denotes a Vandermonde determinant, which we can evaluate to get the final, expanded Specht polynomial as follows:
        \begin{equation*}
            \Sp_\lambda = (x_1-x_2)(x_1-x_3)(x_2-x_3)(x_4-x_5)
        \end{equation*}
        This is how to construct a Specht polynomial.\par
        Now let's look at a different, simpler example for the full construction of irreducible representations. In particular, let's look at $\lambda=(2,1)$ for $S_3$. Here,
        \begin{equation*}
            \Sp_\lambda = x_1-x_2
        \end{equation*}
        Now we apply all of the elements of $S_3$ to it to generate a basis of the vector space that will be our irrep. Applying $e,(123),(132),(12),(13),(23)$ in order via the permutation representation, we get
        \begin{equation*}
            \inp{x_1-x_2,x_2-x_3,x_3-x_1,x_2-x_1,x_3-x_2,x_1-x_3}
        \end{equation*}
        Since $x_1-x_2$ and $x_2-x_3$ don't even contain the same variables, they're clearly linearly independent in the polynomial space. Let's call them $a,b$ respectively. However, every other polynomial present is linearly dependent on $a,b$: For example, $x_3-x_1=-(a+b)$ and $x_2-x_1=-a$. Thus, the true basis of the Specht module, which we will denote $V_\lambda$ and is our final irrep, is
        \begin{equation*}
            \inp{x_1-x_2,x_2-x_3}
        \end{equation*}
        Alternatively, we can construct a basis directly using standard Young tableaux. The two possible SYTs for $(2,1)$ put $x_1,x_2$ vertically stacked and $x_1,x_3$ vertically stacked. This implies a basis $\inp{\Delta(x_1,x_2),\Delta(x_1,x_3)}=\inp{x_1-x_2,x_1-x_3}$, which works just as well as the one above.\par\medskip
        We now prove that Specht modules constitute irreducible representations. Let $\lambda\vdash n$, let $V_\lambda$ be the corresponding Specht module, let $d:=d(\lambda)$ be the degree of $\Sp_\lambda$, and let $R_d$ be the subset of $\C[x_1,\dots,x_n]$ consisting of polynomials of degree $d$. By complete reducibility, let $V_\lambda=\bigoplus W_i^{n_i}$ and $R_d=\bigoplus W_i^{m_i}$, where the $W_i$ are the irreps of $S_n$. Recall that by Schur's lemma,
        \begin{equation*}
            \dim\Hom_{S_n}(V_\lambda,R_d) = \sum_in_im_i
        \end{equation*}
        Moreover, if we actually inspect a homomorphism in this set, we will find that it is a linear map sending polynomials to polynomials. In particular, since the argument is antisymmetric in $x_1,\dots,x_{\lambda_1'}$, so is its image. This pattern continues for all Vandermonde determinants. In fact, what it ends up implying --- since any antisymmetric polynomial is equal to the product of a symmetric polynomial and a Vandermonde determinant --- is that the image is divisible by all of the Vandermonde determinants composing the argument. Indeed, the image is divisible by the argument. But since both image and argument have the same degree, that means that they are only related by a scalar factor, so the homomorphism must be a homothety. For our purposes, the takeaway is that
        \begin{equation*}
            \dim\Hom_{S_n}(V_\lambda,R_d) = 1
        \end{equation*}
        It follows by transitivity that there must exist some $i$ for which $n_i=m_i=1$, and we must have $n_j=m_j=0$ for all $j\neq i$. But this means that $V_\lambda=W_i$ is irreducible, as desired.
    \end{proof}
    \item Define induction and restriction, state the formula for the character of an induced representation. State and prove Frobenius reciprocity.
    \begin{proof}[Answer]
        \textbf{Restriction} takes a representation $\rho:G\to GL(V)$ and a subgroup $H\leq G$, and returns the restricted representation $\tilde{\rho}:H\to GL(V)$ defined by $\tilde{\rho}(g):=\rho(g)$.\par
        \textbf{Induction} takes a subgroup $H\leq G$ and a representation $\rho:H\to GL(W)$, and returns an induced representation $\tilde{\rho}:G\to GL(V)$. Defining this one is a bit trickier. In a nutshell, let $g_1H,\dots,g_sH$ be all cosets of $H$ in $G$, where $s=(G:H)$ and we pick $g_1\in H$. Define vector spaces $g_1W,\dots,g_sW$ identical to $W$ except for the name, and let $V:=g_1W\oplus\cdots\oplus g_sW$. Then if $g\in G$ is arbitrary, we say $\tilde{\rho}(g)$ acts on the basis of $V$ as follows: Let $g_jw$ be a basis vector. $gg_j=g_kh$ for some $g_k$ in the system of representatives and $h\in H$. Thus,
        \begin{equation*}
            [\tilde{\rho}(g)](g_jw) = g_k\rho(h)w
        \end{equation*}
        The formula for the character of an induced representation is as follows.
        \begin{equation*}
            \chi_{\ind_H^GW}(g) = \frac{1}{|H|}\sum_{\substack{x\in G\\x^{-1}gx\in H}}\chi_W(x^{-1}gx)
        \end{equation*}
        Frobenius reciprocity relates the inner product of the characters of two $G$-reps to the inner product of two related $H$-reps: In simplified notation, we have
        \begin{equation*}
            (V,\ind_H^GW)_G = (\res_H^GV,W)
        \end{equation*}
        We can prove this equality through a long transitivity chain as follows.
        \begin{align*}
            (\chi_V,\chi_{\ind_H^GW})_G &= \frac{1}{|G|}\sum_{g_1\in G}\chi_V(g_1)\overline{\chi_{\ind_H^GW}(g_1)}\\
            &= \frac{1}{|G|}\sum_{g_1\in G}\chi_V(g_1)\chi_{\ind_H^GW}(g_1^{-1})\\
            &= \frac{1}{|G|}\sum_{g_1\in G}\chi_V(g_1)\left( \frac{1}{|H|}\sum_{\substack{g_2\in G\\g_2^{-1}g_1^{-1}g_2\in H}}\chi_W(g_2^{-1}g_1^{-1}g_2) \right)\\
            &= \frac{1}{|H|\cdot|G|}\sum_{g_1\in G}\sum_{g_2\in G}\chi_V(g_1)\chi_W(g_2^{-1}g_1^{-1}g_2)\\
            &= \frac{1}{|H|\cdot|G|}\sum_{g_2\in G}\sum_{g_1\in G}\chi_V(g_2^{-1}g_1g_2)\chi_W(g_2^{-1}g_1^{-1}g_2)\\
            &= \frac{1}{|H|\cdot|G|}\cdot|G|\sum_{g_1\in G}\chi_V(g_2^{-1}g_1g_2)\chi_W(g_2^{-1}g_1^{-1}g_2)\\
            &= \frac{1}{|H|}\sum_{h\in G}\chi_V(h)\chi_W(h^{-1})\\
            &= \frac{1}{|H|}\sum_{h\in H}\chi_V(h)\chi_W(h^{-1})\\
            &= \frac{1}{|H|}\sum_{h\in H}\chi_V(h)\overline{\chi_W(h)}\\
            &= (\chi_V|_H,\chi_W)_H\\
            &= (\chi_{\res_H^GV},\chi_W)_H
        \end{align*}
        Note that we can expand the second summation to be over all $g_2\in G$ by defining $\chi_W$ to be zero whenever its argument $g_2^{-1}g_1^{-1}g_2\notin H$.
    \end{proof}
    \item Tips for remembering the answers to each questions/quick generalized outlines.
    \begin{enumerate}
        \item Algebraic integers.
        \begin{itemize}
            \item Define: Easy.
            \item Steps to prove they form a ring.
            \begin{itemize}
                \item $x,y\in\bar{\Z}$, then $x+y,xy\in\bar{\Z}$.
                \item Apply definition of algebraic integers to $x,y$.
                \item Pull back into Frobenius matrices.
                \item Use $A\otimes B$ and $A\otimes I_m+I_n\otimes B$.
                \item Pull back to characteristic polynomials.
            \end{itemize}
            \item Steps to prove that $d_V\mid|G|$.
            \begin{itemize}
                \item Three definitions: $C:=\{g_1,\dots,g_s\}$, $e_C=g_1+\cdots+g_s$, $\rho:\C[G]\to GL(V)$.
                \item $e_C\in Z(\C[G])$ implies $\rho(e_C)=\lambda I_{d_V}$.
                \item Expand above result into $\lambda=|C|\chi(C)/d_V$.
                \item Left multiplication map on $\Z[G]$ implies $e_C^n+a_{n-1}e_C^{n-1}+\cdots+a_0=0$.
                \item Use above to prove $\lambda$ is an algebraic integer.
                \item Use roots of unity and complex roots of a polynomial to prove $\overline{\chi(C)}\in\bar{\Z}$.
                \item Use first orthogonality relation and $\Q\cap\bar{\Z}=\Z$ to finish the proof.
            \end{itemize}
        \end{itemize}
        \item Specht modules.
        \begin{itemize}
            \item Logical way.
            \begin{itemize}
                \item Take $\lambda$.
                \item Make a Specht polynomial.
                \item Apply everything to it.
                \item Choose some elements as a basis.
            \end{itemize}
            \item Quick way.
            \begin{itemize}
                \item Take $\lambda$.
                \item Apply the hook length formula.
                \item Make SYTs.
                \item Write the basis in Specht polynomials.
            \end{itemize}
            \item Prove irreducibility.
            \begin{itemize}
                \item Definitions: $V_\lambda$, $d=d_\lambda$, $R_d$ (polynomials of degree $d$).
                \item Apply complete reducibility.
                \item Look at $\dim\Hom_{S_n}(V_\lambda,R_d)$.
                \item Get sum.
                \item Get 1. ($P=f(\Delta)$.)
            \end{itemize}
        \end{itemize}
        \item Induction/restriction.
        \begin{itemize}
            \item Induction: $gg_j=g_kh$.
            \item Frobenius recipricity is very much like saying restriction is the \emph{adjoint} (functor) of induction. In fact, that's exactly what the statement says.
        \end{itemize}
    \end{enumerate}
    \item (25 points) Question 2: Given a representation of $S_n$, write down its basis using standard tableaux, prove that the corresponding polynomials indeed form a basis, and compute the character.
    \item Let's work some examples.
    \item Example: $(2,2)$.
    \begin{proof}[Answer]
        The hook length formula tells us that we can write
        \begin{equation*}
            \frac{4!}{3\cdot 2\cdot 2\cdot 1} = 2
        \end{equation*}
        SYTs for $\lambda=(2,2)$. By inspection, these are
        \begin{align*}
            \ytableaushort{13,24}&&
            \ytableaushort{12,34}
        \end{align*}
        It follows by taking Vandermonde determinants of the columns that our basis is\footnote{This does, indeed, match with our example from the 11/13 class. See also Figure \ref{fig:SYT4}.}
        \begin{equation*}
            V_{(2,2)} = \langle\underbrace{(x_1-x_2)(x_3-x_4)}_a,\underbrace{(x_1-x_3)(x_2-x_4)}_b\rangle
        \end{equation*}
        We will see as we work toward computing the character that $a,b$ suffice as basis vectors.\par
        We will compute the character directly, that is by finding a matrix for a representative member of each conjugacy class of $S_4$ and taking the trace. Take as representative members of the five conjugacy classes the elements
        \begin{equation*}
            \{e,(12),(123),(1234),(12)(34)\}
        \end{equation*}
        To simplify computations, it will be useful to introduce the vector
        \begin{equation*}
            b-a = (x_1-x_4)(x_2-x_3)
        \end{equation*}
        $e$ will send $a\mapsto a$ and $b\mapsto b$. $(12)$ will send $a\mapsto -a$ and $b\mapsto b-a$. $(123)$ will send $a\mapsto b-a$ and $b\mapsto -a$. $(1234)$ will send $a\mapsto a-b$ and $b\mapsto -b$. $(12)(34)$ will send $a\mapsto a$ and $b\mapsto b$. Thus, the matrices of these five elements are
        \begin{align*}
            \underbrace{
                \begin{bmatrix}
                    1 & 0\\
                    0 & 1\\
                \end{bmatrix}
            }_e&&
            \underbrace{
                \begin{bmatrix}
                    -1 & -1\\
                    0 & 1\\
                \end{bmatrix}
            }_{(12)}&&
            \underbrace{
                \begin{bmatrix}
                    -1 & -1\\
                    1 & 0\\
                \end{bmatrix}
            }_{(123)}&&
            \underbrace{
                \begin{bmatrix}
                    1 & 0\\
                    -1 & -1\\
                \end{bmatrix}
            }_{(1234)}&&
            \underbrace{
                \begin{bmatrix}
                    1 & 0\\
                    0 & 1\\
                \end{bmatrix}
            }_{(12)(34)}
        \end{align*}
        Thus, the character is
        \begin{equation*}
            (2,0,-1,0,2)
        \end{equation*}
        Notice how all of the representative elements send $a,b$ to linear combinations of themselves verifying $\inp{a,b}$ as the basis of $V_{(2,2)}$.
    \end{proof}
    \item Steps.
    \begin{enumerate}
        \item Hook length formula.
        \item Standard Young tableaux.
        \item Specht module basis.
        \item Explicit character computation.
        \begin{itemize}
            \item Look for linear combinations that show up often and write them down!
            \item Use the procedural method if necessary.
        \end{itemize}
        \item Proof of basis validity falls out.
    \end{enumerate}
    \item (25 points) A problem about characters similar to problems 1 and 2 from the midterm.
    \item Problem 1 from the midterm tested the following skills.
    \begin{itemize}
        \item You are given the character table for a group.
        \item Fill in holes using the orthogonality relations.
        \item Compute tensor products, wedge/symmetric squares, and/or the permutational representation.
        \item Decompose reducible representations into irreducibles.
    \end{itemize}
    \item Problem 2 from the midterm tested the following skills.
    \begin{itemize}
        \item You are given a small group (like the quaternion group, the symmetries of a square, $A_4$, etc.).
        \item Compute conjugacy classes.
        \item Compute the character table.
        \item Decompose reducible representations into isotypical components.
        \item Diagonalize an endomorphism.
    \end{itemize}
    \item Example: Character table for the quaternion group.
    \begin{proof}[Answer]
        We are given
        \begin{equation*}
            Q_8 = \{1,-1,i,-i,j,-j,k,-k\}
        \end{equation*}
        with relations
        \begin{align*}
            (-1)^2 &= 1&
            i^2 &= j^2 = k^2 = ijk = -1
        \end{align*}
        To compute the conjugacy classes, start by putting the identity (1) by itself, as per usual. Similarly, $-1$ will commute out of any $g(-1)g^{-1}$, so it's in a class by itself. Then we may observe that $(j)(i)(-j)=-i$, so $i,-i$ are in the same conjugacy class. Similar relations hold for $j,-j$ and $k,-k$. Moreover, there is no way to conjugate $i$ over to $\pm j,\pm k$, so these three classes are disjoint. Thus, the five conjugacy classes are
        \begin{equation*}
            \{1\},\{-1\},\{i,-i\},\{j,-j\},\{k,-k\}
        \end{equation*}
        We know from the sum of the squares of the dimensionalities that we're gonna have one 2D representation and four 1D representations. We can immediately construct a trivial representation $(1,1,1,1,1)$. For the first character to always be 1 and orthogonality to be satisfied, our remaining options are $(1,1,-1,-1,1)$, $(1,1,-1,1,-1)$, and $(1,1,1,-1,-1)$. These correspond to various ways we can assign $-1$ to $i,j$ for instance and have it multiply with the rest per the rules. We can then recover the 2D one via orthogonality relations. In particular, the second orthogonality relation tells us that it is of the form $(\pm 2,\pm 2,0,0,0)$. Our knowledge that characters of the identity are always positive gets us to $(2,\pm 2,0,0,0)$. And then the first orthogonality relation with the first row gets us to $(2,-2,0,0,0)$. Thus, altogether, the character table is
        \begin{table}[h!]
            \centering
            \small
            \renewcommand{\arraystretch}{1.4}
            \begin{tabular}{c|c|c|c|c|c|}
                      & $\{1\}$ & $\{-1\}$ & $\{i,-i\}$ & $\{j,-j\}$ & $\{k,-k\}$\\
                \hline
                $V_1$ & 1 & 1 & 1 & 1 & 1\\ \hline
                $V_2$ & 1 & 1 & $-1$ & $-1$ & 1\\ \hline
                $V_3$ & 1 & 1 & $-1$ & 1 & $-1$\\ \hline
                $V_4$ & 1 & 1 & 1 & $-1$ & $-1$\\ \hline
                $V_5$ & 2 & $-2$ & 0 & 0 & 0\\
            \end{tabular}
            \caption{Character table for $Q_8$.}
            \label{tab:charTableQ8}
        \end{table}
    \end{proof}
    \item Review the Midterm 1 review sheet!
    \item Memorize rotation matrix form:
    \begin{equation*}
        \begin{bmatrix}
            \cos\theta & -\sin\theta\\
            \sin\theta & \cos\theta\\
        \end{bmatrix}
    \end{equation*}
    \item Pull together content from the last few weeks.
    \item \textbf{Partition} (of $n\in\N$): An ordered tuple satisfying the following constraints. \emph{Denoted by} $\bm{\lambda}$, $\bm{(\lambda_1,\ldots,\lambda_k)}$. \emph{Constraints}
    \begin{enumerate}
        \item $\lambda_i\in\N$ for $i=1,\dots,k$;
        \item $\lambda_1\geq\cdots\geq\lambda_k$;
        \item $\lambda_1+\cdots+\lambda_k=n$.
    \end{enumerate}
    \item \textbf{Inverse} (of $\lambda$): The partition $(\lambda_1',\dots,\lambda_k')$ defined as follows. \emph{Denoted by} $\bm{\lambda'}$. \emph{Given by}
    \begin{equation*}
        \lambda_i' = |\{\lambda_j\mid\lambda_j>i\}|
    \end{equation*}
    for all $i=1,\dots,k$.
    \item \textbf{Young diagram} (of $\lambda$): A left-aligned set of vertically stacked rows of boxes, with $\lambda_i$ boxes in the $i^\text{th}$ row and $\lambda_1'$ rows total.
    \item Recall \textbf{graded algebras}, though I don't think they're too important.
    \item \textbf{Elementary symmetric polynomial} (of degree $k$ in $n$): The symmetric polynomial defined as follows. \emph{Denoted by} $\bm{\sigma_k}$. \emph{Given by}
    \begin{equation*}
        \sigma_k = \sum_{1\leq i_1<\cdots<i_k\leq n}x_{i_1}\cdots x_{i_k}
    \end{equation*}
    \item Examples:
    \begin{itemize}
        \item $\sigma_1=x_1+\cdots+x_n=\sum_{1\leq i\leq n}x_i$.
        \item $\sigma_2=x_1x_2+\cdots+x_1x_n+x_2x_3+\cdots=\sum_{1\leq i<j\leq n}x_ix_j$.
        \item $\sigma_n=x_1\cdots x_n$.
    \end{itemize}
    \item \textbf{Lexicographic order} (on monomials): An ordering of monomials based on the following rule. \emph{Denoted by} $\bm{\succ}$. \emph{Given by}\par
    $x_1^{a_1}\cdots x_n^{a_n}\succ x_1^{b_1}\cdots x_n^{b_n}$\dots
    \begin{enumerate}
        \item If $a_1>b_1$ OR\dots
        \item If $a_1=b_1$ and $a_2>b_2$ OR\dots
        \item $a_1=b_1$ and $a_2=b_2$ and $a_3>b_3$ OR\dots
        \item So on and so forth.
    \end{enumerate}
    \item \textbf{Largest monomial} (of $P\neq 0$); The monomial in $P(x_1,\dots,x_n)\neq 0$ that is the largest lexicographically. \emph{Denoted by} $\bm{LM(P)}$.
    \item $\bm{C_{LM}(P)}$: The coefficient of $LM(P)$.
    \item $LM(PQ)=LM(P)LM(Q)$, if $P,Q\neq 0$.
    \item Lemma: If $P\in\Q[x_1,\dots,x_n]^{S_n}$ and $LM(P)=x_1^{a_1}\cdots x_n^{a_n}$, then $a_1\geq\cdots\geq a_n$.
    \item Fundamental theorem of symmetric polynomials:
    \begin{equation*}
        \Q[x_1,\dots,x_n]^{S_n} = \Q[\sigma_1,\dots,\sigma_n]
    \end{equation*}
    \item \textbf{Antisymmetric} (polynomial): A polynomial $P(x_1,\dots,x_n)$ such that
    \begin{equation*}
        \sigma P=(-1)^\sigma P
    \end{equation*}
    \item \textbf{Vandermonde determinant}:
    \begin{equation*}
        \Delta(x_1,\dots,x_n) = \prod_{1\leq i<j\leq n}(x_j-x_i) =
        \begin{bmatrix}
            1 & 1 & \cdots & 1\\
            x_1 & x_2 & \cdots & x_n\\
            \vdots & \vdots & \vdots & \vdots\\
            x_1^{n-1} & x_2^{n-1} & \cdots & x_n^{n-1}\\
        \end{bmatrix}
    \end{equation*}
    \item Theorem: If $P\in\Q[x_1,\dots,x_n]^\text{alt}$ (i.e., $P\in\Q[x_1,\dots,x_n]$ and $P$ is antisymmetric), then $P=P'\Delta(x_1,\dots,x_n)$ where $P'$ is symmetric (i.e., $P'\in\Q[x_1,\dots,x_n]^{S_n}$).
    \item Corollary: If $P$ is antisymmetric and $\deg(P)<n(n-1)/2$, then $P=0$.
    \item \textbf{Specht polynomial} (of $\lambda$): The polynomial derived by taking the Young diagram of $\lambda$, writing numbers down the columns from the left all the way until $n$, and taking Vandermonde determinants of the columns. \emph{Denoted by} $\bm{\Sp_\lambda}$. \emph{Given by}
    \begin{equation*}
        \Sp_\lambda = \Delta(x_1,\dots,x_{\lambda_1'})\Delta(x_{\lambda_1'+1},\dots,x_{\lambda_2'})\cdots\Delta(x_{\lambda_1'+\cdots+\lambda_{k-1}'+1},\dots,x_n)
    \end{equation*}
    \begin{itemize}
        \item For example, $\Sp_{(4,2,1)}$ is written from
        \begin{equation*}
            \ytableaushort{1467,25,3}
        \end{equation*}
        and equals
        \begin{equation*}
            \Sp_{(4,2,1)} = \Delta(x_1,x_2,x_3)\Delta(x_4,x_5)\Delta(x_6)\Delta(x_7)
            = (x_1-x_2)(x_1-x_3)(x_2-x_3)(x_4-x_5)
        \end{equation*}
    \end{itemize}
    \item \textbf{Degree} (of a Specht polynomial): The degree of the given polynomial as defined in Lecture 7.1. \emph{Denoted by} $\bm{d(\lambda)}$. \emph{Given by}
    \begin{equation*}
        d(\lambda) = \sum_{i=1}^{k'}\frac{\lambda_i'(\lambda_i'-1)}{2}
    \end{equation*}
    \item Corollary: If $d'<d$, then $\Hom(V_\lambda,R_{d'})=0$.
    \item Theorem 2: Let $\lambda_1,\lambda_2$ be partitions of $n$. Then $V_{\lambda_1}\cong V_{\lambda_2}$ iff $\lambda_1=\lambda_2$.
    \item Corollary: $V_\lambda$'s are all irreps of $S_n$.
    \item We have that
    \begin{equation*}
        d = \binom{\lambda_1}{2}+\binom{\lambda_2}{2}+\cdots
    \end{equation*}
    \item \textbf{Young tableau}: A Young diagram filled with integers. \emph{Also known as} \textbf{YT}.
    \item \textbf{Standard} (Young tableau): A YT filled in with numbers $1,\dots,n$, wherein each appears exactly once and the numbers increase in rows and in columns. \emph{Also known as} \textbf{SYT}.
    \item \textbf{Specht module} (of $\lambda$): The vector space where each basis element is a polynomial written from a standard Young tableau of $\lambda$ in the same manner that $\Sp_\lambda$ is written from its filled-in Young diagram. \emph{Denoted by} $\bm{V_\lambda}$.
    \begin{itemize}
        \item For example, the Specht module of $(2,2)$ is written from
        \begin{align*}
            \ytableaushort{13,24}&&
            \ytableaushort{12,34}
        \end{align*}
        and equals
        \begin{equation*}
            V_{(2,2)} = \langle(x_1-x_2)(x_3-x_4),(x_1-x_3)(x_2-x_4)\rangle
        \end{equation*}
    \end{itemize}
    \item Theorem: $\dim V_\lambda$ is the number of SYTs of shape $\lambda$.
    \item Corollary: $\dim V_\lambda=\dim V_{\lambda'}$.
    \item Fact: We have the following identity.
    \begin{equation*}
        V_{\lambda'} = V_\lambda\otimes(\text{sign})
    \end{equation*}
    \item Theorem (RSK): There exists a bijection between permutations in $S_n$ and pairs of SYTs of the same shape (i.e., of \textbf{area} $n$).
    \begin{itemize}
        \item See Figure \ref{fig:SchenstedAlgorithm} and the associated discussion.
    \end{itemize}
    \item \textbf{Hook} (of a cell): The set of all cells in a Young diagram directly to the right of or directly beneath the cell in question, including the cell in question.
    \item \textbf{Length} (of a hook): The cardinality of the hook in question.
    \item \textbf{Hook length formula}: The formula given as follows, where $n$ is the number being partitioned. \emph{Given by}
    \begin{equation*}
        \#\SYT_\lambda = \frac{n!}{\prod\text{length of all hooks}}
    \end{equation*}
    \item \textbf{Restriction} (of $V$ to $H\leq G$): The vector space $V$ viewed as an $H$-representation, i.e., a straight-up functional restriction of $\rho_V$. \emph{Denoted by} $\bm{\res_H^G(V)}$.
    \begin{itemize}
        \item Sometimes, we see how the space breaks into subrepresentations under the subgroup.
    \end{itemize}
    \item \textbf{Induction}: \emph{See above}.
    \item The dimension of an induced representation can be calculated via
    \begin{equation*}
        \dim\ind_H^GW = (\dim W)(G:H)
    \end{equation*}
    \item Theorem (Frobenius): Let $H<G$, and let $W$ be an $H$-rep. Then
    \begin{equation*}
        \chi_{\ind_H^GW}(g) = \frac{1}{|H|}\sum_{\substack{x\in G\\x^{-1}gx\in H}}\chi_W(x^{-1}gx)
    \end{equation*}
    \item \textbf{Frobenius reciprocity}: The following equality. \emph{Given by}
    \begin{equation*}
        (\chi_V^{},\chi_{\ind_H^GW})_G^{} = (\chi_{\res_H^GV},\chi_W^{})_H^{}
    \end{equation*}
    \item Recall the Branching Theorem.
    \item Recall the Frobenius characteristic map.
    \item Search for any place I wrote something down about the final!
    \begin{itemize}
        \item Learn representations of $S_4$ by heart. See Table \ref{tab:charTableS4}.
        \item Final will include some stuff from the last HW, but it will be easier.
        \item Rudenko did not expect that finding conjugacy classes would be so difficult for us; he will adjust for this difficulty on the final.
        \item No Burnside's theorem on the final.
    \end{itemize}
    \item Review PSets!
\end{itemize}




\end{document}