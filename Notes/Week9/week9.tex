\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\setcounter{chapter}{8}

\begin{document}




\chapter{???}
\section{???}
\begin{itemize}
    \item \marginnote{11/27:}Announcements.
    \begin{itemize}
        \item OH on Wednesday at 5:30 PM this week; not Tuesday.
        \item There will be extra OH next week pre-exam.
        \begin{itemize}
            \item Roughly like Monday/Wednesday next week.
        \end{itemize}
        \item Midterm will be returned on Wednesday; we can pick them up in-person in his office starting then.
        \item There are some grade boundaries: Pass/Fail we can do til Friday, withdrawal we can do til 5:00 PM today.
    \end{itemize}
    \item Let's finish the conversation about induction/restriction and prove the \textbf{branching theorem}.
    \item Reminder to start.
    \begin{itemize}
        \item We have two mathematical categories, $G$-reps and $H$-reps where $H\leq G$.
        \item These catagories are related by functors.
        \item $\res_H^G:G\text{-reps}\to H\text{-reps}$ and vice versa for $\ind_H^G$.
        \item Restrictions are stupidly simple.
        \item Inductions, most hands-on, we take copies of $W$ times cosets. Formulaically,
        \begin{equation*}
            \ind_H^GW = g_1W\oplus\cdots\oplus g_kW
        \end{equation*}
        where $k=(G:H)$ and $G=\bigsqcup_{i=1}^kg_iH$.
        \begin{itemize}
            \item In more detail, the action of $g$ on $g_iw$ is that of $g_{\sigma(i)}h_iw$.
            \item This is a genuinely hard construction.
            \item A matrix of this thing will be a permutation matrix via
            \begin{equation*}
                copy from board
            \end{equation*}
            \item Note that
            \begin{equation*}
                g_1W\oplus\cdots\oplus g_kW \cong \Hom_H(\C[G],W)
            \end{equation*}
            \begin{itemize}
                \item Recall that elements of the set on the right above are functions $f:G\to W$ such that $f(h(g))=hf(g)$.
                \item We map between the two via $f(g)\mapsto f(gx')$.
            \end{itemize}
            \item What is nice about induced representations is that $\dim[\ind_H^GW]=(\dim W)[G:H]$.
        \end{itemize}
        \item Moreover, there is a very easy statement, the \textbf{Frobenius formula}.
        \begin{itemize}
            \item Recall that
            \begin{equation*}
                \tilde{\chi}_W(g) =
                \begin{cases}
                    0 & g\notin H\\
                    \chi_W(g) & g\in H
                \end{cases}
            \end{equation*}
            \item With this, we average.
            \begin{equation*}
                \chi_{\ind_H^GW}(g) = \sum_{x\in G}\tilde{\chi}_W(xgx^{-1})
            \end{equation*}
            \item Essentially, we're taking a whole bunch of conjugates, summing them up, and dividing to get rid of overcounting.
        \end{itemize}
    \end{itemize}
    \item We now move onto \textbf{Frobenius reciprocity}, which is a relation between the functors/relations $\ind_H^G$ and $\res_H^G$.
    \begin{itemize}
        \item The first point where category theory gets interesting is the notion of \textbf{adjoint functors}, which we are about to touch on. It is a very subtle notion.
        \item Here's version 1 of the statement of Frobenius reciprocity.
        \begin{itemize}
            \item Recall that we have a scalar product on the space of class function, given by
            \begin{equation*}
                (\chi_1,\chi_2) = \frac{1}{|G|}\sum_{g\in G}\chi_1(g)\chi_2(g^{-1})
            \end{equation*}
            where $\chi_1,\chi_2$ are class functions on $G$.
            \item Recall that if $\chi_1=\chi_V$ and $\chi_2=\chi_W$, then
            \begin{equation*}
                (\chi_1,\chi_2) = \dim\Hom_G(V,W)
                = \dim\Hom_G\left( \bigoplus_{i=1}^kV_i^{n_i},\bigoplus_{i=1}^kV_i^{m_i} \right)
                = \sum_{i=1}^kn_im_i
            \end{equation*}
            \item Then the statement is as follows. If $V$ is a $G$-rep and $W$ is an $H$-rep, then
            \begin{equation*}
                (V,\ind_H^GW)_G = (\res_H^GV,W)_H
            \end{equation*}
            \begin{itemize}
                \item Denoting scalar product in $G$ and scalar product in $W$ of the characters of each representation.
            \end{itemize}
            \item This is similar to the relation between adjoint maps $V\to W$ and $W^*\to V^*$.
        \end{itemize}
        \item Version 2.
        \begin{itemize}
            \item We have that
            \begin{equation*}
                \Hom_G(V,\ind_H^GW) \cong \Hom_H(\res_H^GV,W)
            \end{equation*}
            where the isomorphism is canonical.
            \item We will not check this last definition; we can tediously do it with definitions, and there's nothing complicated. Rudenko leaves this as an exercise to us.
        \end{itemize}
        \item Constructing...something: Take $v\in V$, $g\in G$, $\varphi:V\to W$. We send $g\mapsto\varphi(gv)$.
    \end{itemize}
    \item We now prove Version 1.
    \begin{proof}
        We have
        \begin{align*}
            (\chi_V,\chi_{\ind_H^GW})_G &= \frac{1}{|G|}\sum_{g_1\in G}\chi_V(g_1)\left( \frac{1}{|H|}\sum_{g_2\in G}\tilde{\chi}_W(g_2g_1^{-1}g_2^{-1}) \right)\\
            &= \frac{1}{|H|\cdot|G|}\sum_{g_1,g_2\in G}\chi_V(g)\tilde{\chi}_W(g_2g_1^{-1}g_2^{-1})\\
            &= \frac{1}{|H|\cdot|G|}\sum_{g_1,g_2\in G}\chi_V(\underbrace{g_2g_1g_2^{-1}}_h)\tilde{\chi}_W(\underbrace{g_2g_1^{-1}g_2^{-1}}_{h^{-1}})\\
            &= \frac{1}{|H|}\frac{1}{|G|}\sum_{h\in G}|G|\chi_V(h)\tilde{\chi}_W(h^{-1})\\
            &= (\chi_V|_H,\chi_W)_H\\
            &= (\res_H^GV,\chi_W)_H
        \end{align*}
        From line 3 to line 4: Fix $h$; then $g_2g_1g_2^{-1}=h$ iff $g_1=g_2^{-1}hg_2$, so we have overcounted by $|G|$ times.
    \end{proof}
    \item We now come to the branching theorem at long last.
    \item Example first.
    \begin{itemize}
        \item Consider $S_n>S_{n-1}$, where $S_{n-1}$ is the subgroup of permutations fixing $n$. I.e., $S_3>S_2=\{e,(12)\}$.
        \item Let $\lambda$ be a partition of $n$; \emph{there's notation for this}!
        \item Let $\mu\leq\lambda$ be a Young diagram of a partition of $n-1$.
        \item Then
        \begin{enumerate}
            \item We have
            \begin{equation*}
                \res_{S_{n-1}}^{S_n}V_\lambda = \bigoplus_{\mu\leq\lambda}V_\mu
            \end{equation*}
            \begin{itemize}
                \item Example: \emph{Draw out pictures}
            \end{itemize}
            \item We have
            \begin{equation*}
                \ind_{S_{n-1}}^{S_n}V_\mu = \bigoplus_{\mu\leq\lambda}V_\lambda
            \end{equation*}
            \begin{itemize}
                \item Example: \emph{Draw out pictures}
            \end{itemize}
        \end{enumerate}
    \end{itemize}
    \item The reason that this theorem is called the branching theorem originates from the following diagram, which (when continued) encapsulates the main idea of the theorem.
    \emph{picture}
    \begin{itemize}
        \item This graph helps you understand induction and restriction.
        \item Dimensions are the number of paths from the left to a a final Young diagram.
        \begin{itemize}
            \item For example, the dimension of $(3,1)$ is 3 because there are 3 paths to it (\emph{list them}).
        \end{itemize}
        \item Number of paths formula is equivalent to standard Young tableaux!
    \end{itemize}
    \item Theorem (Branching): The following two statements are true.
    \begin{align}
        \res_{S_{n-1}}^{S_n}V_\lambda &= \bigoplus_{\mu\leq\lambda}V_\mu\\
        \ind_{S_{n-1}}^{S_n}V_\mu &= \bigoplus_{\mu\leq\lambda}V_\lambda
    \end{align}
    \begin{proof}
        We'll talk about the general idea of the proof now, and maybe do the details next time.

        \underline{$(1)\Longleftrightarrow(2)$}: We have that \emph{stuff at bottom of board}

        \underline{(1)}: Let's look at an example. Here's a YD of $S_8$. We want to restrict it down to $S_7$.
        Recall that $V_\lambda=\spn(S_8:\Delta(x_1,x_2,x_3)(x_4-x_5)(x_6-x_7))$.
        Now in $S_7$, we fix $x_8$. Consider subrepresentations of $V_\lambda$ filtered by degree as follows.
        \begin{equation*}
            copy from board
        \end{equation*}
        The proof comes from the fact that if we now take quotients of these subrepresentations, then since $x_8$ can only appear in three boxes, ...
    \end{proof}
    \item Practice with the above example and think it through.
\end{itemize}



\section{???}
\begin{itemize}
    \item \marginnote{11/29:}Announcements.
    \begin{itemize}
        \item OH today at 5:30.
        \item Our midterms are graded; we can look at them in his office whenever (I can do this during OH!).
    \end{itemize}
    \item Today, we'll formulate the main result he wants to prove next time.
    \item Goal is still to understand representations of $S_n$.
    \begin{itemize}
        \item We've constructed all of them using Specht modules, but what else do we want?
        \item We have dimension, we want characters, etc.
    \end{itemize}
    \item The main idea is to look at symmetric polynomials once again.
    \begin{itemize}
        \item Consider $\Q[x_1,\dots,x_n]^{S_n}$.
        \item We have proven the fundamental theorem that $\Q[x_1,\dots,x_n]^{S_n}=\Q[\sigma_1,\dots,\sigma_n]$ where $\sigma_k=\sum_{1\leq i_1\leq\cdots\leq i_k\leq n}x_{i_1}\cdots x_{i_k}$.
        \item We also proved in the homework that these rings are equal to $\Q[p_1,\dots,p_k]$ and $\Q[h_1,\dots,h_k]$ where
        \begin{align*}
            p_k &= \sum x_i^k&
            h_k &= \sum_{i_1\leq\cdots\leq i_k}x_1\cdots x_k
        \end{align*}
        \item Example: If $n=3$, then
        \begin{equation*}
            h_2 = x_1^2+x_2^2+x_3^2+x_1x_2+x_1x_3+x_2x_3
        \end{equation*}
        \item Table of bases for $n$, $k$.
        \begin{table}[h!]
            \centering
            \begin{tabular}{c|c|c|c|c}
                kn & 1 & 2 & 3 & 4\\
                0 & 1 & 1 & 1 & 1\\
                1 & $x_1$ & $x_1+x_2$ & $x_1+x_2+x_3$ & $\cdots$\\
                2 & $x_1^2$ & $x_1^2+x_2^2$, $x_1x_2$ & $\sigma_1^2$, $\sigma_2^2$ & $\sigma_1^2$, $\sigma_2^2$\\
                3 & $x_1^3$ & ...
            \end{tabular}
            \caption{...}
            \label{...}
        \end{table}
        \item Now take
        \begin{equation*}
            \Lambda_k = [\C[x_1,\dots,x_k]]_{\deg=k-1}
            \cong [\C[x_1,\dots,x_{k+1}]]_{\deg=k-1}
            \cong \cdots
        \end{equation*}
        \begin{itemize}
            \item Alternatively, we can think of this thing as
            \begin{equation*}
                \Lambda_k = (\C[x_1,\dots])_k
            \end{equation*}
            with $\sigma_1^k,\sigma_2\sigma_1^{k-1},\dots$.
        \end{itemize}
        \item We call $\Lambda$ the ring of symmetric functions and define it to be equal to
        \begin{equation*}
            \Lambda = \Q[\sigma_1,\sigma_2,\sigma_3,\dots]
        \end{equation*}
        \item In every complete component, only finitely many of the $\sigma$ will participate, so we get finite things.
        \item This is a graded ring! We have
        \begin{equation*}
            \Lambda = \bigoplus_{k\geq 0}\Lambda_k
        \end{equation*}
        and $\Lambda_k\otimes\Lambda_\ell=\Lambda_{k+\ell}$
        \item This construction is called the \textbf{projective limit}, and we may have encountered it in commutative algebra under the definition
        \begin{equation*}
            \Lambda = \lim_{\rightarrow}\C[x_1,\dots,x_n]^{S_n}
        \end{equation*}
        \item We have identifies such as $p_2=\sigma_1^2-2\sigma_2$. This means that
        \begin{equation*}
            (x_1+\cdots+x_n)^2-2(x_1x_2+x_1x_3+\cdots) = x_1^2+x_2^2+\cdots
        \end{equation*}
        \item Observation: $\dim_\Q\Lambda_n$.
    \end{itemize}
    \item Now, we need to take a vector space on ring representations; we've done this already with the representation ring.
    \item Let $R_n$ be the $\Q$-vector space of functions $\chi:S_n\to\Q$ such that $\chi(x\sigma x^{-1})=\chi(\sigma)$. This is our favorite space of class functions.
    \item Theorem (Frobenius characteristic map): There is an isomorphism of vector spaces and of rings called the Frobenius characteristic: $\ch:\bigoplus_{n\geq 0}R_n\to\Lambda$.
    \begin{proof}
        Take $\chi_V\in R_k$, and $\chi_W\in R_\ell$. Let $V$ an $S_k$-rep, and $W$ an $S_\ell$-rep. We know that
        \begin{equation*}
            S_k\times S_\ell = S_{k+\ell}
        \end{equation*}
        So what we can do is induction $\ind_{S_k\times S_\ell}^{S_{k+\ell}}(V\otimes W)$. Call this operation $\chi_V\boxtimes\chi_W$.

        Now we write down the formula:
        \begin{equation*}
            \ch(\chi) = \frac{1}{n!}\sum_{\sigma\in S_n}\chi(\sigma)p_1^{\lambda_1(\sigma)}\cdots p_k^{\lambda_k(\sigma)}
        \end{equation*}
        where $\lambda_1(\sigma),\lambda_2(\sigma),\dots$ represent the cycle structure of $\sigma$; each $\lambda_i$ is a number of cycles of length $1,2,\dots$.
    \end{proof}
    \item Examples.
    \begin{enumerate}
        \item $S_1$.
        \begin{itemize}
            \item Sends the YD $(1)$ to $p_1=x_1+x_2+x_3+\cdots$.
        \end{itemize}
        \item $S_2$.
        \begin{itemize}
            \item Sends $(2)$ to $\frac{1}{2!}(p_1^2+p_2)=\frac{1}{2}((x_1+x_2)^2+x_1^2+x_2^2)=x_1^2+x_2^2+x_1x_2=h_2$.
            \item It also sends $(1,1)$ to $\frac{1}{2!}(p_1^2-p_2)=\frac{1}{2}((x_1+x_2)^2-x_1^2-x_2^2)=x_1x_2=\sigma_2$.
            \item Let's check our formula. What is $\ind_{S_1\times S_1}^{S_2}(1)\otimes(1)$? Since the induction of the trivial representation is the regular representation, which we can decompose, we know that this induction equals $(1,1)\oplus(2)$. It follows that $p_1^2=x_1^2+x_2^2+x_1x_2+x_1x_2=(x_1+x_2)^2$.
        \end{itemize}
        \item $S_3$.
        \begin{itemize}
            \item Sends $(3)$ to
            \begin{align*}
                \frac{1}{3!}(p_1^3+3p_1p_2+2p_3) &= \frac{1}{6}[(x_1+x_2+x_3)^3+3(x_1+x_2+x_3)(x_1^2+x_2^2+x_3^2)+2(x_1^3+x_2^3+x_3^3)]\\
                &= \frac{1}{6}[6(x_1^3+x_2^3+x_3^3)+6(x_1^2x_2+x_1x_2^2+x_1x_3^2+x_1^2x_3+\cdots)+6x_1x_2x_3]\\
                &= h_3
            \end{align*}
            \item Sends $(1,1,1)$ to
            \begin{align*}
                \frac{1}{3!}(p_1^3-3p_1p_2+2p_3) &= \frac{1}{6}[(x_1+x_2+x_3)^3-3(x_1+x_2+x_3)(x_1^2+x_2^2+x_3^2)+2(x_1^3+x_2^3+x_3^3)]\\
                &= x_1x_2x_3\\
                &= \sigma_3
            \end{align*}
            \item Sends $(2,1)$ to
            \begin{align*}
                \frac{1}{3!}(2p_1^3-p_3) &= \frac{1}{6}[2(x_1^3+x_2^3+x_3^3)+6(x_1^2x_2+\cdots)+12x_1x_2x_3]\\
                &= (x_1^2+\cdots)+2x_1x_2x_3
            \end{align*}
            \item Again, we can check that
            \begin{align*}
                \ind_{S_2\times S_1}^{S_3}[(1,1)\otimes(1)] &= \sigma_1\sigma_2
            \end{align*}
            \begin{itemize}
                \item We compute $\ind_{S_2}^{S_3}(1,1)=(1,1,1)\oplus(2,1)$ via the branching formula: There are only two ways to add a box!
                \item We have $\sigma_1\sigma_2-\sigma_3=(x_1+x_2+x_3)(x_1x_2+x_1x_3+x_2x_3)-x_1x_2x_3$.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
    \item Do we need to be fluent in the techniques by which you expanded all of the polynomials above??
    \item Thus, we have two conjectures:
    \begin{align*}
        \ch[(n)] &= h_n&
        \ch[\underbrace{(1,\dots,1)}_{n\text{ times}}] &= \sigma_n
    \end{align*}
    \item The theorem is cool because it sends all of representation theory to some symmetric polynomial game!
    \item How do we compute $\ch(V_\lambda)$?
    \begin{itemize}
        \item We say it equals $S_\lambda$, where $S_\lambda$ is a Schur polynomial.
        \item Take the YT of $\lambda$. Recall standard YTs.
        \item \textbf{Semistandard} (YT): Things strictly increase in columns, but only monotonically increase in rows. \emph{draw picture!}
        \item The six semistandard ones give us the Schur polynomial.
        \item Relation to RSK correspondence.
    \end{itemize}
    \item Proving why this stuff is true is not hard.
    \item To understand \emph{why} this is true, Google the \textbf{Schur-Weyl duality}.
\end{itemize}



\section{Office Hours}
\begin{itemize}
    \item I got a 68/100 on the midterm: 30, 24, 0, 14.
    \begin{itemize}
        \item I would have needed to show my work (or at least one example of a calculation) to get full credit for 2, even though it just said "find."
        \item Rudenko did not expect that finding conjugacy classes would be so difficult for us; he will adjust for this difficulty on the final.
    \end{itemize}
    \item Week 3, Lecture 2: You proved that $\inp{\chi_V,\chi_W}=\delta_{VW}$. To do so, you used a projection function $p=(1/|G|)\sum_{g\in G}gv$. You began your proof by proving that $p$ is a $G$-morphism and then never used this result again, as far as I can tell. Did you use it again? See pp. 45-47, 58 (it needs to be a morphism of $G$-representations to map between the representations $V,V^G$?).
    \item Week 3, Lecture 2: Same proof. To prove that $\im(P)=V^G$, do we need more than $p^2=p$? I think so, but you didn't do it explicitly. See pp. 46-47.
    \item Week 3, Lecture 2: Same proof. What's up with the trivial special case? See p. 48.
    \item *Week 3, Lecture 3: Cube thing (see picture from 10/13)?
    \begin{itemize}
        \item It's just a depiction of two different 3-coordinate bases of the same space. It was drawn to illustrate a possible relation between the orthonormal basis $\chi_1,\chi_2,\chi_3$ (cube) and the orthogonal basis $\chi_{C_1},\chi_{C_2},\chi_{C_3}$.
    \end{itemize}
    \item Week 3, Lecture 3: Why did we talk about the infinite-dimensional regular representation here? See p. 50.
    \item *Week 3, Lecture 3: What is the point of the misc. calculations involved in computing the $S_4$ character table? See p. 52.
    \begin{itemize}
        \item Just to check that we were on the right path and shown an example of using the orthogonality relations.
    \end{itemize}
    \item *Week 3, Lecture 3: Proof of the second orthogonality relation your way? It's in \textcite{bib:Serre}, but I don't think that's the way you proved it. See p. 52.
    \begin{itemize}
        \item To begin, note that it is a \emph{highly} nontrivial statement that if $A,B$ are matrices such that $AB=I$, then $BA=I$. It seems so simple to us, but think about it! For an arbitrary matrix $A,B$, $AB$ looks nothing like $BA$! We have two entirely different systems of equations.
        \item However, using this fact, basically it is possible to translate the orthogonality relation for the \emph{columns} into the orthogonality relation about the \emph{rows}.
    \end{itemize}
    \item *Week 3, Lecture 3: All the talk about the exceptional homomorphisms? See p. 52, 61 (the final representation has something to do with an \textbf{involution} of trace 2, and is a representation of a quotient group?).
    \begin{itemize}
        \item So the representation is $\rho:S_4\twoheadrightarrow S_3\xrightarrow{\tilde{\rho}}GL_n$, where $\tilde{\rho}:S_3\to GL_n$ is the representation of $\rho$ corresponding to the character $(2,0,1)$.
    \end{itemize}
    \item *Week 4, Lecture 1: Alternate construction of $R(G)$? See p. 63.
    \item *Week 4, Lecture 1: Extension of scalars with the representation ring? See p. 64.
    \begin{itemize}
        \item We don't need to know anything about this stuff.
        \item What it is though is basically analogous to extending the real numbers into a subset of the complex numbers by treating every $x\in\R$ as $x+0i\in\C$. Very trivial, silly concept.
        \item There is also such a thing as a \textbf{reduction of scalars}.
    \end{itemize}
    \item *Week 4, Lecture 1: Does multiplying a column vector in the basis $\{\delta_{C_i}\}$ by the character table put it in the basis $\{\chi_{V_i^*}\}$, or vice versa? See p. 66.
    \begin{itemize}
        \item Derive it for yourself.
    \end{itemize}
    \item *Week 4, Lecture 2: Isotypical components example. See p. 68.
    \item **Week 4, Lecture 3: Proof the $\C$ is the only finite-dimensional division algebra? See p. 71.
    \begin{itemize}
        \item Let $A$ be an arbitrary finite-dimensional division algebra over $\C$.
        \item To prove that $A=\C$, we will use a bidirectional inclusion proof.
        \item Naturally, $\C\subset A$.
        \item To prove the reverse implication, start by letting $a\in A$ be arbitrary.
        \item Define the left-multiplication operator $L_a:A\to A$ by $x\mapsto ax$ for all $x\in A$.
        \item Recall that $A$ is a complex vector space in addition to being an algebra, the same way a ring is also a group. Thus, $L_a$ is a linear operator on a complex vector space.
        \item It follows by the theorem of linear algebra that $L_a$ has an eigenvalue $\lambda\in F=\C$ and corresponding eigenvector $b\in A$.
        \item Consequently, by the cancellation lemma,
        \begin{align*}
            L_ab &= \lambda b\\
            ab &= \lambda b\\
            a &= \lambda
        \end{align*}
        \item Therefore, $a\in A$ implies $a\in\C$, so $A=\C$.
    \end{itemize}
    \item *Week 6, Lecture 2: Proof that $\sqrt{2}/2$ is not an algebraic integer using Gauss's lemma? See p. 87.
    \begin{itemize}
        \item Let $\alpha:=\sqrt{2}/2$ for the sake of notation.
        \item Suppose for the sake of contradiction that $\alpha$ is an algebraic integer.
        \item Then there exists a monic polonomial $p(x)\in\Z[x]$ such that $p(\alpha)=0$.
        \item Observe that the minimal polynomial in $\Z[x]$ that annihilates $\alpha$ is $2x^2-1$.
        \item Thus, by polynomial division,
        \begin{equation*}
            p(x) = q(x)\cdot(2x^2-1)+r(x)
        \end{equation*}
        for some $q,r\in\Q[x]$ such that $\deg r\leq 2-1$.
        \item We have that
        \begin{equation*}
            r(\alpha) = p(\alpha)-q(\alpha)\cdot(2\alpha^2-1)
            = 0-q(\alpha)\cdot 0
            = 0
        \end{equation*}
        \item Additionally, since $r\in\Q[x]$ and $\deg r\leq 1$, we know that $r(x)=ux+v$ for some $u,v\in\Q$.
        \item We now prove that $u=v=0$.
        \begin{itemize}
            \item Suppose for the sake of contradiction that either $u$ or $v$ was not equal to zero.
            \item Combining the previous two claims reveals that
            \begin{align*}
                0 &= r(\alpha)\\
                &= u\alpha+v\\
                -\frac{v}{u} &= \alpha
            \end{align*}
            \item If $u=0$, then $\alpha$ is undefined and we have arrived at a contradiction. Thus, $u\neq 0$.
            \item Thus, $\alpha\in\Q$. But since $\alpha\notin\Q$ by definition, we have arrived at a contradiction.
            \item Therefore, $u=v=0$.
        \end{itemize}
        \item Having established that $r=0$, we know that $p=(2x^2-1)q$, i.e., $2x^2-1$ divides $p$.
        \item Now define $N$ to be the least common multiple of the denominators of the coefficients of $q$.
        \item Consider
        \begin{equation*}
            Np = (Nq)(2x^2-1)
        \end{equation*}
        \item It follows by Gauss's lemma that
        \begin{align*}
            c(Np) &= c[(Nq)(2x^2-1)]\\
            N &= c(Nq)\cdot c(2x^2-1)\\
            &= 1\cdot 1\\
            &= 1
        \end{align*}
        where $c$ denotes the \textbf{content}.
        \item But if $N=1$, then $q\in\Z[x]$, so leading term of $p$ --- equal to the product of $2x^2$ and the leading term of $q$ --- has a coefficient that is a multiple of 2, i.e., is \emph{not} equal to 1 as is required of a monic polynomial, a contradiction.
    \end{itemize}
    \item *Week 6, Lecture 3: Questions about Lemma 1 of the proof of Burnside's theorem. See p. 92.
    \begin{itemize}
        \item The roots $a_1,\dots,a_k$ of the minimal polynomial of the algebraic integer $a$ are known as \textbf{conjugate algebraic integers}.
        \item The conjugate algebraic integers of a root of unity are also roots of unity.
        \begin{itemize}
            \item Suppose $\varepsilon$ is a root of unity.
            \item Then the minimal polynomial of $\varepsilon$ is $x^n-1$ for some $n\in\N$.
            \item Naturally, the roots of this polynomial (the conjugate algebraic integers to $\varepsilon$) are all of the other roots of unity of order $n$.
        \end{itemize}
        \item The conjugate algebraic integers of a sum of roots of unity is a sum of roots of unity.
        \begin{itemize}
            \item It can be shown that the minimal polynomial for $\varepsilon_1+\varepsilon_2$ is
            \begin{equation*}
                p(x) = \prod_{i,j=1}^n(x-\varepsilon_1^i-\varepsilon_2^j)
            \end{equation*}
            \item Evidently, the above polynomial is symmetric under permutations of $\varepsilon_1^i,\varepsilon_2^j$, and we'd generate the same polynomial with any $\pm\varepsilon_1^i\pm\varepsilon_2^j$ as starting material.
            \item Explicit example.
            \begin{itemize}
                \item $\pm\sqrt{2}$ are conjugate algebraic integers, as solutions to $x^2-2$. Similarly, $\pm\sqrt{3}$ are conjugate algebraic integers as solutions to $x^2=3$.
                \item Thus, we expect the minimal polynomial for $\sqrt{2}+\sqrt{3}$ to be
                \begin{equation*}
                    p(x) = (x-\sqrt{2}-\sqrt{3})(x-\sqrt{2}+\sqrt{3})(x+\sqrt{2}-\sqrt{3})(x+\sqrt{2}+\sqrt{3})
                \end{equation*}
                \item Expanding, we obtain 
                \begin{align*}
                    p(x) &= (x^2-(\sqrt{2}+\sqrt{3})^2)(x^2-(\sqrt{2}-\sqrt{3})^2)\\
                    &= x^4-[(\sqrt{2}+\sqrt{3})^2+(\sqrt{2}-\sqrt{3})^2]x^2+(\sqrt{2}+\sqrt{3})^2(\sqrt{2}-\sqrt{3})^2\\
                    &= x^4-10x^2+1
                \end{align*}
                \item Indeed, the above polynomial is a monic polynomial 
                \item From the definition, this polynomial is evidently also the minimal polynomial for $\sqrt{2}-\sqrt{3}$, $-\sqrt{2}+\sqrt{3}$, and $-\sqrt{2}-\sqrt{3}$.
                \item Thus, the conjugate algebraic integers of $\sqrt{2}+\sqrt{3}$ are the four sums of all individual algebraic integers.
            \end{itemize}
        \end{itemize}
        \item How do we extend this argument to the case in the problem?? What about when $\varepsilon_1=-1$ and $\varepsilon_2=i$ so that simple powers don't access every combination as the $p(x)$ formula does?
        \item We know that $\prod_{i=1}^na_i\in\Z$ because of Vieta's formula.
        \begin{itemize}
            \item In particular, this tells us that $x_1\cdots x_n$ is equal to the last coefficient in the minimal polynomial which, by definition, is an integer.
        \end{itemize}
    \end{itemize}
    \item *Week 7, Lecture 2: Symmetric polynomials and roots of symmetric polynomials. See p. 101.
    \item *Week 7, Lecture 2: Word in blackboard picture? See p. 102.
    \begin{itemize}
        \item "Remain" to show\dots
    \end{itemize}
    \item *Week 7, Lecture 2: What is $d$ in the proof of the alternating polynomials theorem? See p. 103.
    \begin{itemize}
        \item $d=n-1$.
    \end{itemize}
    \item *Week 8, Lecture 2: What is $d$ in the definition on p. 111.
    \begin{itemize}
        \item Consider the Specht polynomial corresponding to $(2,2,1)$.
        \begin{figure}[h!]
            \centering
            \ydiagram{2,2,1}
            \caption{Young diagram for $(2,2,1)$.}
            \label{fig:YD221}
        \end{figure}
        \item Since $(2,2,1)'=(3,2)$, the Specht polynomial is $(x_1-x_2)(x_1-x_3)(x_2-x_3)\cdot(x_4-x_5)$.
        \item $\Delta_{123}$ is of degree $3=\binom{3}{2}$ because looking at the first column of the YD, which corresponds to $\lambda_1'=3$, out of the 3 boxes, we must choose 2 for each of the three terms $(x_1-x_2),(x_1-x_3),(x_2-x_3)$. Then we just add this to the 2 choose 2 for the second column of the YD.
    \end{itemize}
    \item *Week 9, Lecture 2: Do we need to be fluent in the techniques you used to expand and reduce the various polynomial powers? How did you do that again?
\end{itemize}




\end{document}