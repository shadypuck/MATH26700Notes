\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}

\begin{document}




\chapter{Introduction to Representation Theory}
\section{Motivating and Defining Representations}
\begin{itemize}
    \item \marginnote{9/27:}Rudenko would happily approve my final substitution, but it's not his call; it's Boller's.
    \item HW will be due every week on Wednesday or thereabouts.
    \begin{itemize}
        \item Submit in paper in a mailbox, location TBA.
        \item First HW due next Wednesday.
    \end{itemize}
    \item Midterm eventually and an in-class final.
    \item Grading scheme in the syllabus.
    \item OH not available MW after class (Rudenko has to run to something else), but F after class, we can ask him anything.
    \begin{itemize}
        \item Regular OH MTh, time TBA.
    \end{itemize}
    \item There is no specific book for the course.
    \begin{itemize}
        \item First 8 lectures come from \textcite{bib:Serre}; amazing book but very concise; gets confusing later on. Most lectures are made up by Rudenko.
    \end{itemize}
    \item Course outline.
    \begin{enumerate}
        \item Character theory: Beautiful, not too hard.
        \item Non-commutative algebra: More abstract/general approach to the same thing.
        \item Advanced topics, $S_n$.
    \end{enumerate}
    \item This course's focus: Representations of finite groups in finite dimensions over $\C$.
    \item This course is for math-inclined people (not quite physics) and lays the foundation for all other Rep Theory.
    \begin{itemize}
        \item The ideas would be presented in a very different way in Physics Rep Theory.
    \end{itemize}
    \item We can always ask questions and stop him to correct mistakes during class.
    \item Why we care about representations.
    \begin{itemize}
        \item Start with a group $G$, finite. For example, let $G\equiv S_1$.
        \item People started to play with $S_4$ (permutations of roots of a polynomial of degree 4) in Galois theory.
        \begin{itemize}
            \item Galois theory primer: Consider a polynomial like $x^4+3x+1=0$; the roots $\alpha_1,\alpha_2,\alpha_3,\alpha_4$ satisfy tons of equations, e.g., $\alpha_1\alpha_2\alpha_3\alpha_4=1$ since 1 is the $x^0$ term.
        \end{itemize}
        \item But groups also occur in much more natural places, e.g., isometries of $\R^3$ that preserve a tetrahedron.
        \item $S_4$ is also orientation-preserving isometries of $\R^3$ that preserve a cube.
        \item Many things lead to the same group!
        \item Theory of abstract groups developed far later than any of these perspectives; was developed to unify them.
    \end{itemize}
    \item Recall group actions: Take $G,X=\{x_1,\dots,x_n\}$ both finite. We want $G\acts X$, which is a homomorphism $A:G\to S_n$.
    \item What can we do now?
    \begin{itemize}
        \item We can look at orbits, which are smaller pieces.
        \item We can look at the stabilizer.
        \item We can identify orbits with cosets.
        \item If we understand all possible subgroups, we understand all possible actions.
    \end{itemize}
    \item This story is not boring, but it's simplistic.
    \item Rudenko doesn't assume we remember everything (phew!).
    \item Main definition (general to start, then we simplify).
    \item \textbf{Group representation} (of $G$ on $V$): A group homomorphism $G\to GL(V)$, for $G$ a group, $V$ a finite-dimensional vector space over some field $\F$ with basis $\{e_1,\dots,e_n\}$, and $GL(V)$ the set of isomorphic linear maps $L:V\to V$. \emph{Denoted by} $\bm{\rho}$.
    \begin{itemize}
        \item Recall that $GL(V)=GL_n(\F)$ is the set of all $n\times n$ invertible matrices.
    \end{itemize}
    \item For every element $g\in G$, $g\mapsto\rho(g)=A_g$. Essentially, you're mapping to elements that satisfy certain equations.
    \begin{itemize}
        \item For example, $A_e=E_n$, $A_{g_1g_2}=A_{g_1}A_{g_2}$, and $A_{g^{-1}}={A_g}^{-1}$.
        \item Thus, representations are a "concrete way to think about groups."
        \item If you don't understand abstract group $G$, let us compare it to a group that we do understand! Like a group can \emph{act} on $S_n$, we can \emph{represent} a group in a vector space.
    \end{itemize}
    \item In this course, $G$ is finite, $\F=\C$, and $V$ is finite dimensional.
    \begin{itemize}
        \item This is the most simple case, but also a very interesting one. The theory is much, much easier, so we can get much more complicated, but this is a good place to start.
        \item We could make $G$ compact, but we're not gonna go that far.
    \end{itemize}
    \item Examples to get an idea of what's going on.
    \begin{enumerate}
        \item $\dim\rho=1$ (means $\dim V=1$). Then $\rho:G\to GL_1(V)=\C^\times$. The codomain is referred to as the \textbf{character} of the group.
        \begin{itemize}
            \item An example group homomorphism $S_n\to\C^\times$ is the sign function $\sigma\to\sign(\sigma)=\{\pm 1\}$.
            \item Another example is the \textbf{trivial representation}, $G\to\C^\times$ and $g\mapsto 1$.
        \end{itemize}
        \item Smallest one: Let $G=S_3$. The structure is already pretty rich, and this will be part of the homework.
        \begin{itemize}
            \item \textbf{Trivial representation} again.
            \item \textbf{Alternating representation}.
            \item \textbf{Standard representation}.
            \item \textbf{Regular representation}.
        \end{itemize}
    \end{enumerate}
    \item \textbf{Trivial representation}: The representation $\rho:G\to GL(V)$ sending $g\mapsto 1$ for all $g\in G$. \emph{Denoted by} $\pmb{\ydiagram{3}}$, $\bm{(3)}$.
    \begin{itemize}
        \item The boxes notation is too much of a detour to explain now.
        \item Note that $1\in GL(V)$ is the identity map on $V$!
    \end{itemize}
    \item \textbf{Alternating representation}: The representation $\rho:G\to GL(V)$ sending $g\mapsto\sign(g)$ for all $g\in G$. \emph{Denoted by} $\pmb{\ydiagram{1,1,1}}$, $\bm{(1,1,1)}$.
    \item \textbf{Standard representation}: The representation $\rho:S_n\to GL(V)$ sending $\sigma\mapsto(x_{\sigma(1)},\dots,x_{\sigma(n)})$, where $V=\{(x_1,\dots,x_n)\in\C^n\mid x_1+\cdots+x_n=0\}$ is a $(n-1)$-dimensional vector space. \emph{Denoted by} $\pmb{\ydiagram{2,1}}$, $\bm{(2,1)}$.
    \begin{itemize}
        \item A 2D representation like rotating a triangle.
        \item This gives something with real numbers.
        \item Example: $S_3\acts V$ by $\sigma((x_1,x_2,x_3))=(x_{\sigma(1)},x_{\sigma(2)},x_{\sigma(3)})$.
    \end{itemize}
    \item \textbf{Regular representation}: The representation $\rho:G\to\Hom(\C^n)$ defined by $g\mapsto\sigma_g$, where $G=\{g_1,\dots,g_n\}$, $\{e_{g_1},\dots,e_{g_n}\}$ is a basis of $\C^n$, $\cdot$ is the group action of $\rho(G)\acts\C^n$ by $\rho(g)\cdot e_g=e_{gg_i}$, and $\sigma_g(e_{g_i})=\rho(g)\cdot e_g=e_{gg_i}$.
    \begin{itemize}
        \item This is a permutation of vectors.
        \item Thus, for $S_3$, it will already be 6-dimensional (it's very high dimensional).
    \end{itemize}
    \item How do we know that representation theory is tractable? Sure, we can define all these things, but how do we know that it will lead anywhere? Here's an example.
    \begin{itemize}
        \item Let $G=\Z/2\Z=\{e,g\}$, $V=\C^n$, $A$ an $n\times n$ matrix over $\C$, $\rho:G\to GL_n(\C)$, and $A:=\rho(g)$. Since $g^2=e$, we know for example that $A^2=E_n$.
        \item But how do we find the matrices $A$?
        If we look at eigenvalues of $A$, there are only two possibilities: $\pm 1$. The structure of $A$ can be very complicated with Jordan normal form and all that, but in fact, these are the \textbf{semisimple matrices}, so it's not that bad.
        \item Since $A^2=E$, we know that $(A-E)(A+E)=0$. Consider $(A-E):V\to V$. Naturally, it has $\ker(A-E)$ and $\im(A-E)$. In this particular case, Rudenko claims that $\ker(A-E)\cap\im(A-E)=\{0\}$.
        \begin{proof}
            Let $v\in\ker(A-E)\cap\im(A-E)$ be arbitrary. Since $v\in\im(A-E)$, there exists $w\in V$ such that $v=(A-E)w=Aw-w$. Since $v\in\ker(A-E)$, we have $(A-E)v=0$, so $Av=v$. It follows that $A(Aw-w)=Aw-w$ but also $A(Aw-w)=Ew-Aw=w-Aw$. Thus,
            \begin{align*}
                Aw-w &= w-Aw\\
                2Aw &= 2w\\
                Aw &= w
            \end{align*}
            But then $w\in\ker(A-E)$, so $v=(A-E)w=0$.
        \end{proof}
        \item This combined with the fact that every vector in a vector space is in either the image or the kernel of a linear map\footnote{See Theorem 3.6 of \textcite{bib:Axler}.} implies that $V=\ker(A-E)\oplus\im(A-E)$.
        \item Let the kernel have basis $e_1,\dots,e_k$ and the image have basis $e_{k+1},\dots,n$; then all $A$ are of the following form.
        \begin{equation*}
            \begin{bNiceArray}{ccc|ccc}[margin,first-row,first-col]
                    & 1 &        & k & k+1 &        &  n\\
                1   & 1 &        &   &     &        &   \\
                    &   & \ddots &   &     &        &   \\
                k   &   &        & 1 &     &        &   \\
                \hline
                k+1 &   &        &   & -1  &        &   \\
                    &   &        &   &     & \ddots &   \\
                n   &   &        &   &     &        & -1\\
            \end{bNiceArray}
        \end{equation*}
        \item Next time, we will discuss sums of representations, of which this is an example of the theory.
    \end{itemize}
    \item The same kind of thing, \textbf{simple representations}, happens with all finite groups?? This is where we're going. It's not rocket science; in fact, we'll see it next week.
    \item Last thing for today: A remarkable story.
    \begin{itemize}
        \item The story of representation theory started quite different.
        \item A beautiful theorem that we can prove now!
        \item Frobenius determinant.
        \item Think of $G=\{g_1,\dots,g_n\}$. Picture its multiplication table.
        \item In every row and column, you see each element once.
        \item Let's associate to the multiplication table an actual determinant in the linear algebra sense. Consider elements $x_{g_1},\dots,x_{g_n}$. Define the $n\times n$ matrix $(x_{g_ig_j})$. Take its determinant. It will be a polynomial in $n$ variables, i.e., an element of the ring $\Z[x_{g_1},\dots,x_{g_n}]$.
        \item Example: Consider
        \begin{equation*}
            \begin{vmatrix}
                e & g\\
                g & e\\
            \end{vmatrix}
        \end{equation*}
        \begin{itemize}
            \item The determinant is $x_e^2-x_g^2=(x_e-x_g)(x_e+x_g)$.
        \end{itemize}
        \item Example: $G=\Z/3\Z$.
        \begin{itemize}
            \item If the elements are $e,g,g^2$ and we map these, respectively, to variables $a,b,c$, we get the matrix
            \begin{equation*}
                \begin{bNiceMatrix}
                    e & g & g^2\\
                    g & g^2 & e\\
                    g^2 & e & g\\
                \end{bNiceMatrix}
                \mapsto
                \begin{vNiceMatrix}
                    a & b & c\\
                    b & c & a\\
                    c & a & b\\
                \end{vNiceMatrix}
            \end{equation*}
            \item The determinant is $3abc-a^3-b^3-c^3=(a+b+c)(a^2+b^2+c^2-ab-bc-ac)=(a+b+c)(a+\zeta b+\zeta^2c)(a+\zeta^2b+\zeta c)$ where $\zeta^3=1$ is a root of unity.
        \end{itemize}
        \item Frobenius's theorem: If $G$ is a finite group and we take this Frobenius determinant, then this determinant is equal to ${P_1}^{d_1}\cdots{P_k}^{d_k}$ where $P_1,\dots,P_k$ are irreducible polynomials in $x_g,\dots,x_{g_j}$, then $\deg P_i=d_i$ and $k$ is the number of conjugacy classes.
        \item Example: Take $S_3$; we'll get a polynomial of degree $|S_3|=6$ but the Frobenius determinant $FD=(x_{g_1}+\cdots+x_{g_k})(x_{g_1}\pm\cdots)(\text{some pol. of deg 2})^2$
        \item The proof is remarkable and deep and uses what would become character theory. These polynomials are related to representations and the number of simplest irreducible representations. The theory that came out came as a way to understand this miracle. We'll forget FD's for now, but then come back and prove it later.
    \end{itemize}
\end{itemize}



\section{Key Definitions and Category Theory Primer}
\begin{itemize}
    \item \marginnote{9/29:}OH: TW 4:30 or 5:00 most likely; he will confirm later.
    \item Today: Definitions in greater generality.
    \item As before, let $G$ be a finite group and $V$ be a finite-dimensional vector space.
    \item Goal of this course: Understand representations of $G$, that is\dots
    \begin{itemize}
        \item Homomorphisms $\rho:G\to GL(V)=GL_n(\C)$;
        \item That send $g\mapsto A_g\in GL_n(\C)$;
        \item And satisfy $A_e=E$, $A_{g_1}A_{g_2}=A_{g_1g_2}$, and $A_{g^{-1}}={A_g}^{-1}$.
    \end{itemize}
    \item What are some things we might want to do?
    \begin{itemize}
        \item Build new representations from old? Investigate and/or classify irreducible representations?
        \item Before we can see if any of this works or not, we need a ton of definitions: Sum, equality, etc.
    \end{itemize}
    \item Rudenko will start today's lecture with some general thoughts on the \textbf{category} of representations.
    \item Categories are things that now permeates math.
    \item \textbf{Category}: A \emph{class} (not a set) of \emph{objects} (some things; you don't know anything about them), and then a bunch of properties.
    \begin{figure}[h!]
        \centering
        \begin{tikzpicture}
            \coordinate (0) at ($(0,0)  +rand*(0.2,0)+rand*(0,0.2)$);
            \coordinate (1) at ($(0:1)  +rand*(0.2,0)+rand*(0,0.2)$);
            \coordinate (2) at ($(60:1) +rand*(0.2,0)+rand*(0,0.2)$);
            \coordinate (3) at ($(120:1)+rand*(0.2,0)+rand*(0,0.2)$);
            \coordinate (4) at ($(180:1)+rand*(0.2,0)+rand*(0,0.2)$);
            \coordinate (5) at ($(240:1)+rand*(0.2,0)+rand*(0,0.2)$);
            \coordinate (6) at ($(300:1)+rand*(0.2,0)+rand*(0,0.2)$);
    
            \foreach \c in {0,...,6} {
                \fill [blx] (\c) circle (2pt);
            }
    
            \draw [bly,thick,->,shorten >=4pt,shorten <=4pt] (0) to[bend left=20] (4);
            \draw [bly,thick,->,shorten >=4pt,shorten <=4pt] (4) to[bend left=20] (0);
            \draw [bly,thick,->,shorten >=4pt,shorten <=4pt] (0) to[bend left=20] (3);
            \draw [bly,thick,->,shorten >=4pt,shorten <=4pt] (3) to[bend left=20] (2);
            \draw [bly,thick,->,shorten >=4pt,shorten <=4pt] (2) to[bend left=20] (1);
            \draw [bly,thick,->,shorten >=4pt,shorten <=4pt] (2) to[bend left=20] (0);
            \draw [bly,thick,->,shorten >=4pt,shorten <=4pt] (5) to[bend left=20] (6);
            \draw [bly,thick,->,shorten >=4pt,shorten <=4pt] (6) to[bend left=20] (5);
        \end{tikzpicture}
        \caption{The general structure of a category.}
        \label{fig:category}
    \end{figure}
    \begin{itemize}
        \item Objects $a,b$ in category $C$ are denoted by $a,b\in\bm{\Ob(C)}$.
        \item There are also \textbf{morphisms} between the objects. These are drawn as arrows and lie in $\Hom(a,b)$.
        \item There is also composition: $\Hom(a,b)\times\Hom(b,c)\to\Hom(a,c)$.
        \begin{itemize}
            \item What does this notation mean??
        \end{itemize}
        \item Properties.
        \begin{enumerate}
            \item Associativity.
            \item Existence of a unit: For any object $a$, there exists $\id_a\in\Hom(a,a)$ such that any morphism pre- or post-composed to this identity yields the same morphism.
            \begin{itemize}
                \item Example: If $f\in\Hom(a,b)$, then $\id_b\circ f=f=f\circ\id_a$.
            \end{itemize}
        \end{enumerate}
        \item Rudenko: So a category is basically two pieces of data and a bunch of properties.
    \end{itemize}
    \item Examples of categories:
    \begin{itemize}
        \item Category of sets and maps between them.
        \item Category of vector spaces over $\F$ where $\Ob(C)$ is the vector spaces and $\Hom(V,W)$ is filled with \emph{linear} maps because you don't just want maps --- you want maps that respect the structure.
        \item Category of groups where $\Hom(G_1,G_2)$ is the set of group homomorphisms.
        \item Category of topological spaces and continuous maps.
        \item Category of abelian groups.
        \item Trivial category and the identity map; thus, categories need not be chonky.
    \end{itemize}
    \pagebreak
    \item Comments on category theory.
    \begin{itemize}
        \item We'll see some pretty significant category theory at the end of the course.
        \item We'll see categories in every course we take; some people try to avoid them. Rudenko doesn't want to go into the material in depth, but he wants to use language from it.
        \item Surprisingly, even under the stripped-down of axioms of category theory, you can say quite a lot.
        \item Why any of this discussion of category theory matters: If you know the basics of category theory, you can guess the definitions of direct sum, equality, etc. for representations.
    \end{itemize}
    \item \textbf{Category of representations}. \emph{Denoted by} $\bm{\Rep_G}$.
    \item Take two $G$-representations $V,W$; how do we define a map between them?
    \begin{itemize}
        \item Recall that $V,W$ are vector spaces.
    \end{itemize}
    \item \textbf{Morphism} (of $G$-representations): A map $f:V\to W$ such that\dots
    \begin{enumerate}
        \item $f$ is linear;
        \item $f$ respects the structure of the representations; explicitly, for every $g\in G$, $\rho_V(g)\circ f=f\circ\rho_W(g)$\footnote{Recall that the object, $\rho_V(g)$ is a linear map! Thus, it can be composed with other linear maps like $f$.}.
    \end{enumerate}
    \item On constraint 2, above: This condition is summarized via a \textbf{commutative diagram}.
    \begin{figure}[h!]
        \centering
        \DisableQuotes
        \begin{tikzcd}
            V \arrow[d,"\rho_V^{}(g)"'] \arrow[r,"f"] & W \arrow[d,"\rho_W^{}(g)"]\\
            V \arrow[r,"f"'] & W\\
        \end{tikzcd}
        \begin{tikzpicture}[remember picture,overlay]
            \draw [-{[bend]>}] (-2.07,0.3) arc[start angle=180,end angle=-150,radius=2mm];
        \end{tikzpicture}
        \EnableQuotes
        \vspace{-1.5em}
        \caption{Commutative diagram, morphisms.}
        \label{fig:CDmorphisms}
    \end{figure}
    \begin{itemize}
        \item Commutative diagrams are very category-theory-esque things.
    \end{itemize}
    \item That was a very abstract definition; let's make it concrete.
    \begin{itemize}
        \item Suppose you have a pair of representations $V=\C^n,W=\C^m$, and we have our map $f$ between them given by an $m\times n$ matrix.
        \item Let $\rho_V(g)=A_g$ be an $n\times n$ matrix, and let $\rho_W(g)=B_g$ be an $m\times m$ matrix.
        \item Then $FA_g=B_gF$.
    \end{itemize}
    \item Examples.
    \begin{enumerate}
        \item An interesting example: Let's look at $S_3\acts V_\text{perm}=\C^3$, a \textbf{permutation representation}.
        \begin{itemize}
            \item For all $\sigma\in S_3$, $\rho(\sigma):(x_1,x_2,x_3)\mapsto(x_{\sigma(1)},x_{\sigma(2)},x_{\sigma(3)})$.
        \end{itemize}
        \item There's also the trivial representation $S_3\acts V_{(3)}=\C$ defined by $\rho(\sigma):x\mapsto x$.
    \end{enumerate}
    \item Are the above 2 representations related?
    \begin{itemize}
        \item Yes! We can, in fact, find a \emph{morphism} between them.
        \item In particular, define $f:V_{(3)}\to V_\text{perm}$ by $f(x)=(x,x,x)$.
        \begin{itemize}
            \item Since permuting 3 of the same thing does nothing, the commutativity of Figure \ref{fig:CDmorphisms} holds. Therefore, $f$ is a morphism of $G$-representations as defined above.
            \item More explicitly,
            \begin{equation*}
                f[\rho_{(3)}(\sigma)(x)] = f(x)
                = (x,x,x)
                = \rho_\text{perm}(\sigma)((x,x,x))
                = \rho_\text{perm}(\sigma)[f(x)]
            \end{equation*}
        \end{itemize}
        \item Is $f$ \textbf{reversible}?
        \begin{itemize}
            \item Is "reversible" the right word??
        \end{itemize}
        \item Define $\tilde{f}:V_\text{perm}\to V_{(3)}$ by $\tilde{f}:(x_1,x_2,x_3)\mapsto x_1+x_2+x_3$.
        \begin{itemize}
            \item Since addition is commutative, the commutativity of Figure \ref{fig:CDmorphisms} holds.
            \item More explicitly,
            \begin{align*}
                f[\rho_\text{perm}(\sigma)((x_1,x_2,x_3))] &= f((x_{\sigma(1)},x_{\sigma(2)},x_{\sigma(3)}))\\
                &= x_{\sigma(1)}+x_{\sigma(2)}+x_{\sigma(3)}\\
                &= x_1+x_2+x_3\tag*{Commutativity of addition}\\
                &= f((x_1,x_2,x_3))\\
                &= \rho_{(3)}(\sigma)[f((x_1,x_2,x_3))]\\
            \end{align*}
        \end{itemize}
        \item Takeaway: The existence of maps between representations is interesting.
    \end{itemize}
    \item Next question: How do we define an \textbf{isomorphism} of two representations?
    \item \textbf{Isomorphism} (of $G$-representations): A morphism of G-reps that is an isomorphism of vector spaces.
    \item Category theory helps us again here because it generalizes the concepts of an isomorphism!
    \begin{itemize}
        \item If $f:V\to W$ and $g:W\to V$ are category-theoretic morphisms, then the constraints $f\circ g=\id_W$ and $g\circ f=\id_V$ make $f$ and $g$ into category-theoretic \emph{iso}morphisms, regardless of what $V$ and $W$ might be.
        \item Back in the context of representations, let $f:V\to V$ be an isomorphism of vector spaces. Then we do indeed have $\rho_V(g)\circ f=f\circ\rho_V(g)$, as we would hope from category theory!
    \end{itemize}
    \item Recall the condition $FA_g=B_gF$. Supposing $F$ is an isomorphism (and thus has an inverse), we get $FA_gF^{-1}=B_g$ as our new condition.
    \begin{itemize}
        \item Essentially, we can do \emph{simultaneous conjugation} of all matrices.
        \item As per usual with isomorphisms, we get to \emph{change bases}.
        \item Essentially, we can represent the nice permutation representation in a very nasty basis but still have it be valid.
    \end{itemize}
    \item Many other notions (e.g., direct sum) will not be explained by Rudenko, but we can read about them!
    \item However, we'll do a few more.
    \item A representation sitting inside another: a \textbf{subrepresentation}.
    \item \textbf{Subrepresentation} (of $V$): A subspace $W\subset V$ such that for all $w\in W$ and $g\in G$, we have that $\rho_V(g)W\subset W$, where $V$ is a $G$-representation with $\rho_V:G\to GL(V)$.
    \begin{itemize}
        \item Many people will just write the critical condition as $gW\subset W$.
    \end{itemize}
    \item Subrepresentations in category theory: We have another commutative diagram.
    \begin{figure}[h!]
        \centering
        \DisableQuotes
        \begin{tikzcd}
            W \arrow[d,"\rho_V^{}(g)"'] \arrow[r,hook] & V \arrow[d,"\rho_V^{}(g)"]\\
            W                           \arrow[r,hook] & V                         \\
        \end{tikzcd}
        \EnableQuotes
        \vspace{-1.5em}
        \caption{Commutative diagram, subrepresentations.}
        \label{fig:CDsubreps}
    \end{figure}
    \item Example: The trivial representation, the standard representation, and (of course) the \textbf{zero representation} are subrepresentations of the permutation representation.
    \item \textbf{Zero representation}: The representation $\rho:G\to GL(\{0\})$ sending $g\mapsto 1$ for all $g\in G$. \emph{Denoted by} $\bm{(0)}$.
    \item What about representations that don't have subrepresentations?
    \item \textbf{Simple} (representation): A $G$-representation $V$ that has only two subrerpesentations: $(0)$ and $V$. \emph{Also known as} \textbf{irreducible}, \textbf{irreps}.
    \item Example irreducible representations: Line in $\C^2$, triangle in $\C^2$, $A_5$ and dodecahedron in $\C^3$.
    \item Notion of a direct sum.
    \item \textbf{Direct sum} (of $V_1,V_2$): The $G$-rep with the space $V_1\oplus V_2=\{(v_1,v_2)\mid v_1\in V_1,v_2\in V_2\}$ where $\rho_{V_1\oplus V_2}(g)(v_1,v_2)=(\rho_{V_1}(g)v_1,\rho_{V_2}(g)v_2)$. \emph{Denoted by} $\bm{V_1\oplus V_2}$.
    \begin{itemize}
        \item The matrix of $\rho_{V_1\oplus V_2}(g)$ is the following block matrix.
        \begin{equation*}
            \rho_{V_1\oplus V_2}^{}(g) =
            \begin{bNiceArray}{w{c}{1cm}c|w{c}{1cm}c}[margin]
                \Block{2-2}{\rho_{V_1}^{}(g)} & & \Block{2-2}{0} & \\
                 & & & \\
                \hline
                \Block{2-2}{0} & & \Block{2-2}{\rho_{V_1}^{}(g)} & \\
                 & & & \\
            \end{bNiceArray}
        \end{equation*}
    \end{itemize}
    \item Example: $V_\text{perm}=V_{(3)}\oplus V_{(2,1)}$, with $\C^3=\C\oplus \C^2$ where
    \begin{align*}
        \C &= \langle(1,1,1)\rangle&
        \C^2 &= \langle(x_1,x_2,x_3)\mid x_1+x_2+x_3=0\rangle
    \end{align*}
    \begin{itemize}
        \item The decomposition is into simple representations.
        \item Relate this to the fact that the JCF of any $3\times 3$ permutation matrix has at most a 1-block and a 2-block, if not three 1-blocks. There will always be one 1D subspace on which the permutation matrix is an identity, i.e., $\spn(1,1,1)$, and a 2D orthogonal complement!
        \item As a fun and simple exercise, prove that there is no line fixed under the standard representation.
    \end{itemize}
    \item A simple and important theorem to prove next week.
    \item Theorem: Let $G$ be a finite group and $\F=\C$. Then\dots
    \begin{enumerate}
        \item There are finitely many irreps $V_1,\dots,V_s$ up to isomorphism.
        \begin{itemize}
            \item Later on, we will see that $s$ is equal to the number of conjugacy classes.
        \end{itemize}
        \item For every $G$-rep $V$, there exists a unique $n_1,\dots,n_s\geq 0$ such that $V\cong{V_1}^{n_1}\oplus\cdots\oplus{V_s}^{n_s}$.
    \end{enumerate}
    \item This theorem tells us that if we want to study rep theory, we want to study irreps (which can be kind of complicated) because if we understand them, everything breaks down into them.
    \item Examples.
    \begin{enumerate}
        \item $G=\Z/2\Z=S_2$.
        \begin{itemize}
            \item $V_1=\C e$ with $ge=e$ and $V_{-1}=\C e$ with $ge=-e$.
            \item It follows that $V\cong{V_1}^{n_1}\oplus{V_{-1}}^{n_{-1}}$.
            \item We get a diagonal matrix with only 1s and $-1$s.
        \end{itemize}
        \item $G=S_3$.
        \begin{itemize}
            \item $V_{(3)},V_{(1,1,1)},V_{(2,1)}$.
            \item $GL_5(\F_4)$.
            \item Proven in an elementary way in Section 1.3 of \textcite{bib:FultonHarris}, which we have to read for the HW; will be useful for later in the course's HW.
        \end{itemize}
    \end{enumerate}
    \item Plan: Next time, we'll talk about some more abstract stuff; tensor products of vector spaces.
    \begin{itemize}
        \item Tensor products are something we should read up on now! The definition is hard and abstract.
        \item Then he'll prove the above theorem.
    \end{itemize}
\end{itemize}



\section{S Chapter 1: Generalities on Linear Representations}
\emph{From \textcite{bib:Serre}.}
\begin{itemize}
    \item \marginnote{10/3:}Part I (what we'll be covering) is written for quantum chemists, and thus gives proofs "as elementary as possible, using only the definition of a group and the rudiments of linear algebra" \parencite[v]{bib:Serre}.
    \begin{itemize}
        \item Recall the story about Serre and his wife, the chemist, who needed to explain group theory and rep theory to her students.
    \end{itemize}
    \item Indeed, although the book seemed very fast when I first looked at it two years ago, it reads much more easily now and has enough context for most anyone who is comfortable with group theory and theoretical linear algebra.
\end{itemize}


\subsection*{Section 1.1: Definitions}
\begin{itemize}
    \item Definitions of $\bm{GL(V)}$, \textbf{invertible square matrix}, and \textbf{finite group}.
    \item \textbf{Linear representation}: See class notes. \emph{Also known as} \textbf{group representation}.
    \begin{itemize}
        \item \textcite{bib:Serre} will frequenty write $\rho_s$ for $\rho(s)$.
    \end{itemize}
    \item \textbf{Representation space} (of $G$): The vector space $V$ corresponding to the linear representation $
    \rho:G\to GL(V)$ of $G$. \emph{Also known as} \textbf{representation}.
    \begin{itemize}
        \item The latter term is a self-identified "abuse of language" \parencite[3]{bib:Serre}.
    \end{itemize}
    \item "For most applications, one is interested in dealing with a \emph{finite number of elements} $x_i$ of $V$, and can always find a subrepresentation of $V$\dots of finite dimension, which contains the $x_i$; just take the vector subspace generated by the images $\rho_s(x_i)$ of the $x_i$" \parencite[4]{bib:Serre}.
    \item \textbf{Degree} (of a representation): The dimension of the representation space of this representation.
    \item To give a representation \textbf{in matrix form} is to give a set of invertible matrices that are isomorphic to the group elements.
    \item Important converse: Given invertible matrices satisfying the appropriate homomorphism identities, there is a corresponding group that these matrices represent.
    \item \textbf{Similar} (representations of $G$): Two representations $\rho:G\to GL(V)$ and $\rho':G\to GL(V')$ of $G$ for which there exists a linear isomorphism $\tau:V\to V'$ such that
    \begin{equation*}
        \tau\circ\rho(s) = \rho'(s)\circ\tau
    \end{equation*}
    for all $s\in G$. \emph{Also known as} \textbf{isomorphic}.
    \begin{itemize}
        \item Equivalent definition (in matrix form): There exists $T$ invertible such that $R_s'=TR_sT^{-1}$.
        \item Isomorphic representations have the same degree.
    \end{itemize}
\end{itemize}


\subsection*{Section 1.2: Basic Examples}
\begin{itemize}
    \item Degree 1 representation: A homomorphism $\rho:G\to\C^*$, where $\C^*$ denotes the roots of unity (all $z\in\C$ with $|z|=1$).
    \begin{itemize}
        \item The fact that every $s\in G$ has \emph{finite} order by assumption is what permits this representation.
    \end{itemize}
    \item \textbf{Unit representation}: See class notes. \emph{Also known as} \textbf{trivial representation}.
    \item \textbf{Regular representation}: The representation $\rho:G\to GL(V)$ defined by $s\mapsto[e_t\mapsto e_{st}]$ for all $s\in G$, where $V$ has basis $(e_t)_{t\in G}$.
    \begin{itemize}
        \item $\deg\rho=|G|$.
        \item $e_s=\rho_s(e_1)$.
        \begin{itemize}
            \item Implication: The images of $e_1$ under the $\rho_s$'s form a basis of $V$, i.e., $\{\rho_s(e_1)\mid s\in G\}$ is a basis of $V$.
        \end{itemize}
        \item Converse of above: If $W$ is a representation of $G$ containing a vector $w$ such that $\{\rho_s(w)\mid s\in G\}$ forms a basis of $W$, then $W$ is isomorphic to the regular representation $V$ via $\tau:V\to W$ defined by $\tau(e_s)=\rho_s(w)$.
    \end{itemize}
    \item \textbf{Permutation representation} (associated with $X$): The representation $\rho:G\to GL(V)$ defined by $s\mapsto[e_x\mapsto e_{s\cdot x}]$ for all $s\in G$, where $G\acts X$ a finite set and $V$ has a basis $(e_x)_{x\in X}$.
\end{itemize}


\subsection*{Section 1.3: Subrepresentations}
\begin{itemize}
    \item Definition of \textbf{subrepresentation}.
    \begin{itemize}
        \item Example: Trivial representation $\C(x,\dots,x)$ is a subrepresentation of the regular representation.
    \end{itemize}
    \item Definitions of \textbf{direct sum} of vector spaces and \textbf{kernel}.
    \item \textbf{Complement} (of a subspace): Any $(n-m)$-dimensional subspace $U$ that\dots
    \begin{enumerate}
        \item Satisfies $W\oplus U=V$;
        \item Intersects $W$ trivially;
    \end{enumerate}
    where $\dim V=n$ and $\dim W=m\leq n$.
    \begin{itemize}
        \item This means that a single subspace can have multiple complements!
        \begin{itemize}
            \item Only one \textbf{orthogonal} complement, but many \emph{complements}.
            \item Example: Consider a line through the origin in $\R^2$; any other line through the origin is a complement of it!
        \end{itemize}
        \item It follows that there is a bijection between the complements $W'$ of $W$ in $V$ and the projections $p$ of $V$ onto $W$ (since non-orthogonal complements require non-orthogonal projections).
    \end{itemize}
    \item \textbf{Projection} (of $V$ onto $W$ associated with the decomposition $V=W\oplus W'$): The mapping that sends each $x\in V$ to its component $w\in W$. \emph{Denoted by} $\bm{p}$.
    \begin{itemize}
        \item Consequence: The two properties defining a $p$ are (1) $\im(p)=W$ and (2) $p(x)=x$ for all $x\in W$.
        \item Consequence: These two properties also imply that a map is a projection and $V=W\oplus\ker(p)$.
    \end{itemize}
    \item If a representation has a subrepresentation, then some complement of this subrepresentation is also a subrepresentation.
    \begin{theorem}\label{trm:complements}
        Let $\rho:G\to GL(V)$ be a linear representation of $G$ in $V$ and let $W$ be a vector subspace of $V$ stable under $G$. Then there exists a complement $W^0$ of $W$ in $V$ which is stable under $G$.
        \begin{proof}[Proof 1 (limited conditions)]
            % We need a projection operator that commutes with all $\rho_s$, so we build one called $\rho_0$. This allows us to prove that $\rho_sx$ doesn't vary into $W$ for any $s\in G$ and $x\in W^0$.
            % What is the exact form of the projection operator??

            Let $p$ be the projection of $V$ onto $W$ that corresponds to some arbitrary complement of $W$ in $V$. To begin, we may legally --- albeit with little motivation --- form the average $p^0$ of the conjugates of $p$ by the elements of $G$:
            \begin{equation*}
                p^0 := \frac{1}{|G|}\sum_{t\in G}\rho_t\cdot p\cdot\rho_t^{-1}
            \end{equation*}
            We now seek to prove that $p^0$ is a projection by showing that it satisfies the two properties of a "$p$." First, notice that by assumption, every $\rho_t$ (and thus $\rho_t^{-1}$) preserves $W$. This combined with the fact that $p(V)=W$ implies that $p^0(V)=W$, as desired. Additionally, for any $x\in W$ and $t\in G$, we know by property (2) of a $p$ and the fact that $p_t^{-1}(x)\in W$ that $p\cdot p_t^{-1}(x)=p_t^{-1}(x)$. Applying $p_t$ to both sides of this equation yields $[p_t\cdot p\cdot p_t^{-1}](x)=x$. Hence, $p^0(x)=x$, as desired. Thus, $p^0$ is a projection of $V$ onto $W$, associated with some complement $W^0$ of $W$.\par
            So that we can make a substitution later, we will now prove that
            \begin{equation*}
                \rho_s\cdot p^0 = p^0\cdot\rho_s
            \end{equation*}
            for all $s\in G$. Pick such an $s$. Then
            \begin{equation*}
                \rho_s\cdot p^0\cdot\rho_s^{-1} = \frac{1}{|G|}\sum_{t\in G}\rho_s\cdot\rho_t\cdot p\cdot\rho_t^{-1}\cdot\rho_s^{-1}
                = \frac{1}{|G|}\sum_{t\in G}\rho_{st}\cdot p\cdot\rho_{st}^{-1}
                = p^0
            \end{equation*}
            so we can precompose both sides of the above equation with $\rho_s$ to yield the final result. This line here should make it clear why we needed to form a projection like $p^0$.\par
            We now have all of the tools we need to prove that $W^0$ is stable under $G$. To do so, it will suffice to show that for all $x\in W^0$ and $s\in G$, we have $\rho_s(x)\in W^0$. Let $x\in W^0$ and $s\in G$ be arbitrary. Since $x\in W^0$, $p^0(x)=0$ by definition. This combined with the above commutativity rule implies that $p^0\cdot\rho_s(x)=\rho_s\cdot p^0(x)=\rho_s(0)=0$. But the only way that $p^0$ could map $\rho_s(x)$ to 0 is if $\rho_s(x)\in W^0$, as desired.
        \end{proof}
        \begin{proof}[Proof 2 (orthogonal complement)]
            Let $W^0$ be the orthogonal complement of $W$, and endow $V$ with a \textbf{scalar product} $(x\mid y)$ to turn it into an inner product space. Replace $(x\mid y)$ with the new inner product $\sum_{t\in G}(\rho_tx\mid\rho_ty)$. Now, if it wasn't already, the inner product is invariant under $\rho_s$ for all $s$, i.e., for $s$ arbitrary, we have
            \begin{equation*}
                (\rho_sx\mid\rho_sy) = (x\mid y)
            \end{equation*}
            This means that vectors that were orthogonal before $\rho_s$ is applied to $V$, stay orthogonal after $\rho_s$ is applied to $V$. In particular, since $\rho_s$ preserves $W$ by hypothesis, all vectors orthogonal to $W$ (i.e., all vectors in $W^0$) stay orthogonal to $W$ (i.e., stay in $W^0$) after $\rho_s$ is applied. Thus, $W^0$ is stable under $\rho_s$ as well.
        \end{proof}
    \end{theorem}
    \item Consequence of the second, stronger proof: The representations $W$ and $W^0$ determine the representation $V$.
    \begin{itemize}
        \item This allows us to rigorously say that the representation $V=W\oplus W^0$.
        \item If $W,W^0$ are given in matrix form by $R_s,R_s^0$, then $W\oplus W^0$ is given in matrix form by
        \begin{equation*}
            \begin{pNiceArray}{c|c}[margin]
                R_s & 0\\
                \hline
                0 & R_s^0\\
            \end{pNiceArray}
        \end{equation*}
    \end{itemize}
    \item We can extend this method of directly summing representations to an arbitrary finite number of them.
\end{itemize}



\subsection*{Section 1.4: Irreducible Representations}
\begin{itemize}
    \item Definition of \textbf{irreducible} representation.
    \item Fact: Each nonabelian group possesses at least one irreducible representation with $\deg\geq 2$.
    \begin{itemize}
        \item Proven later.
    \end{itemize}
    \item Irreducible representations construct all representations via the direct sum.
    \begin{theorem}
        Every representation is a direct sum of irreducible representations.
        \begin{proof}
            We induct on $\dim(V)$.\par
            Suppose $\dim(V)=0$. Since 0 is the direct sum of the empty family of irreducible representations, the theorem is vacuously true.\par
            Suppose $\dim(V)\geq 1$. We divide into two cases ($V$ is irreducible and $V$ is reducible). In the first case, we are done. In the second case, $V=V'\oplus V''$ for some $V'\perp V''$ (see Theorem \ref{trm:complements}). Since $\dim(V')<\dim(V)$ and $\dim(V'')<\dim(V)$ by definition, the induction hypothesis implies that $V'$ and $V''$ are direct sums of irreducible representations. Therefore, the same is true of $V$.
        \end{proof}
    \end{theorem}
    \item Fact: The direct-sum decomposition is not necessarily unique.
    \begin{itemize}
        \item Counterexample: If $\rho_s=1$ for all $s\in G$, then there are a plethora of decompositions of a vector space into a direct sum of lines.
    \end{itemize}
    \item Fact: The number of $W_i$ isomorphic to a given irreducible representation \emph{does not} depend on the chosen decomposition.
    \begin{itemize}
        \item Proven later.
    \end{itemize}
\end{itemize}




\end{document}