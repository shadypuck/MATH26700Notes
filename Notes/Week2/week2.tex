\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\stepcounter{chapter}

\begin{document}




\chapter{Constructing Representations}
\section{The Tensor Product}
\begin{itemize}
    \item \marginnote{10/2:}Before lecture, I chatted with a few people about tensor products and the exterior and symmetric powers.
    \begin{itemize}
        \item Patrick: A \textbf{tensor} $v\otimes w$ is just an element of a vector space, indexed differently than in a column.
        \item Raman: There is no canonical way to transform tensors into column vectors.
    \end{itemize}
    \item Course logistics.
    \begin{itemize}
        \item OH: T 5:30-6:30(+) and W 5:30-6:30(+). We can also meet one-on-one.
        \item HW is due Thursdays at midnight.
    \end{itemize}
    \item Today: Constructing new representations from old.
    \begin{itemize}
        \item Rudenko will skim through tensor products really quickly.
    \end{itemize}
    \item Reminder: Last time, we talked about how representation theory is really quite simple. If $G$ is a finite group and $F=\C$, there exist a finite set $V_1,\dots,V_s$ of irreps up to isomorphism, and every finite-dimensional representation $V\cong V_1^{n_1}\oplus\cdots\oplus V_s^{n_s}$.
    \item If $V$ is a representation of $G$, then there are loads of things we can do with it.
    \begin{itemize}
        \item We can construct the dual representation $V^*$.
        \item We can construct the representation $V\otimes V$.
        \item We can construct symmetric powers.
        \item We can construct wedge powers.
        \item There are more, but this is enough for now.
    \end{itemize}
    \item Even when we take a very simple group and representation, there are some very interesting things that can fall out.
    \begin{itemize}
        \item Example: If you take the symmetric powers of $S_3$, as in the homework, you get something really interesting.
    \end{itemize}
    \item Now, we go to linear algebra.
    \item Let $V,W$ be vector spaces over a field $F$. How do we produce a new vector space out of these?
    \item $\Hom_F(V,W)$ is the vector space of linear maps $F:V\to W$!
    \begin{itemize}
        \item $\dim=(\dim V)(\dim W)$.
    \end{itemize}
    \item Can we make $\Hom_F(V,W)$ into a representation of $G$? Yes!
    \begin{figure}[H]
        \centering
        \DisableQuotes
        \begin{tikzcd}
            V \arrow[d,"\rho_V^{}(g)"'] \arrow[r,"L"] & W \arrow[d,"\rho_W^{}(g)"]\\
            V \arrow[r,"gL"'] & W\\
        \end{tikzcd}
        \EnableQuotes
        \vspace{-1.5em}
        \caption{Commutative diagram, linear maps space representation.}
        \label{fig:CDlinMapsRep}
    \end{figure}
    \begin{itemize}
        \item Suppose that $V,W$ are $G$-reps, which gives us $\rho_V:G\to GL(V)$ and $\rho_W:G\to GL(W)$.
        \item Suppose also that we have $L\in\Hom_F(V,W)$.
        \item Now infer from the commutative diagram that it will work to define $gL=\rho_W(g)\circ L\circ\rho_V(g)^{-1}$.
        \item This is pretty standard.
    \end{itemize}
    \item Recall that there is a different space $\Hom_G(V,W)$ of morphisms of $G$-representations (see Figure \ref{fig:CDmorphisms} and the associated discussion).
    \begin{itemize}
        \item This is a very very small subspace of $\Hom_F(V,W)$.
    \end{itemize}
    \item Special case of the above construction: \textbf{Dual representation}.
    \begin{itemize}
        \item Consider $\Hom_F(V,F)$. This the \textbf{dual vector space}.
        \item Basic fact 1: Let $e_1,\dots,e_n$ be a basis of $V$. Then $V^*$ has a corresponding basis $e^1,\dots,e^n$ known as its \textbf{dual basis}.
        \begin{itemize}
            \item Computing coordinates already depends on a basis, and having bases is super nice.
            \item Corollary: $\dim V=\dim V^*$.
            \item This is the first time \textbf{canonical} comes into linear algebra. Canonical (nobody understands what it means) basically means that something doesn't depend on choices.
            \item In particular, $V,V^*$ are isomorphic because they have the same dimension, but for no more natural reason. They can be the same representation, or they can be different.
        \end{itemize}
        \item Basic fact 2: If $V$ is finite-dimensional, then $(V^*)^*\cong V$. The formula for this isomorphism is canonical, because it does not depend on a choice of basis. In particular, choose the map $V\to(V^*)^*$ sending $v$ to the map sending $\varphi\in V^*$ to $\varphi(v)$.
        \item If $V$ is infinite dimensional, none of this is true and you are in the realm of functional analysis.
        \item Ok, so all of this was good information about the dual \emph{space}, but what is the dual \emph{representation}?? Does it matter, and do we need to know for now?
        \begin{itemize}
            \item Defined below in the notes on the reading from \textcite{bib:FultonHarris}.
        \end{itemize}
    \end{itemize}
    \item \textbf{Dual vector space} (of $V$): The vector space defined as follows, given that $V$ is a vector space over $F$. \emph{Denoted by} $\bm{V^*}$. \emph{Given by}
    \begin{equation*}
        V^* = \Hom_F(V,F)
    \end{equation*}
    \item \textbf{Dual basis} (of $V^*$ to $e_1,\dots,e_n$): The basis defined as follows for $i=1,\dots,n$, where $e_1,\dots,e_n$ is a basis of $V$. \emph{Denoted by} $\bm{e^1,\ldots,e^n}$. \emph{Given by}
    \begin{equation*}
        e^i(x_1e_1+\cdots+x_ne_n) = x_i
    \end{equation*}
    \item We now move onto the tensor product.
    \begin{itemize}
        \item The tensor product is very hard to understand. If you learn about it and you feel you don't understand it, that's typical; nobody understands it at first.
        \item For now, we'll discuss two ways of thinking about tensor products that won't bring us any comfort.
    \end{itemize}
    \item Let $V,W$ be two vector spaces over a field $F$.
    \item Abstract definition of the tensor product.
    \begin{itemize}
        \item We have discussed maps from $V\to W$, but there is another related space.
        \item Indeed, we can look at the space of bilinear maps from $V\times W\to F$.
        \begin{itemize}
            \item Example: A map $f:V\times W\to F$ that satisfies the constraints $f(\lambda v,w)=\lambda f(v,w)$, $f(v_1+v_2,w)=f(v_1,w)+f(v_2,w)$, and likewise for the second index. Recall that this is a \textbf{bilinear map}.
        \end{itemize}
        \item Let $V$ have basis $e_1,\dots,e_n$ and $W$ have basis $f_1,\dots,f_m$.
        \item Notice that every bilinear map $f$ can be defined as a linear combination of the $f(e_i,f_j)$. In other words, the $f(e_i,f_j)$ form the basis of a function space.
        \begin{itemize}
            \item This "bilinear maps space" has dimension $nm$.
        \end{itemize}
        \item Now, one way to understand a tensor product: Is this "bilinear maps space" actually some other space? It is! It is $(V\otimes W)^*$.
        \item Bilinear maps are linear maps from where? From $V\otimes W$!
    \end{itemize}
    \item \textbf{Bilinear} (map): A function $f:V\times W\to Z$ that satisfies the following constraints, where $V,W,Z$ are vector spaces over $F$, $v,v_1,v_2\in V$, $w,w_1,w_2\in W$, and $\lambda\in F$. \emph{Constraints}
    \begin{align*}
        f(v_1+v_2,w) &= f(v_1,w)+f(v_2,w)&
            f(\lambda v,w) &= \lambda f(v,w)\\
        f(v,w_1+w_2) &= f(v,w_1)+f(v,w_2)&
            f(v,\lambda w) &= \lambda f(v,w)
    \end{align*}
    \item We now look at a much more elementary definition of the tensor product.
    \item Explicit definition of the tensor product.
    \begin{itemize}
        \item Start off with the hug, easy-to-work-with vector space with basis consisting of pairs of elements $(v,w)\in V\times W$. For example, even if $V,W$ are one dimensional, this is like all pairs of real numbers $(1,0)$, $(2,0)$, $(\pi,e)$, etc. \emph{as basis vectors}; it's huge.
        \begin{itemize}
            \item Then, we quotient it by the space of all elements satisfying the relations $\lambda(v,w)=(\lambda v,w)=(v,\lambda w)$, $(v_1+v_2,w)=(v_1,w)+(v_2,w)$, and the like. These elements will be linear combinations of basis vectors of the following form: $\lambda(v,w)-(\lambda v,w)$, $\lambda(v,w)-(v,\lambda w)$, and $(v_1+v_2,w)-(v_1,w)-(v_2,w)$.
            \item This forces these relationships to be true. For example, in the final quotient space, we can still construct the element $\lambda(v,w)-(\lambda v,w)$. But its inclusion in the quotiented-out subspace will imply that in the quotient space, $\lambda(v,w)-(\lambda v,w)=0$. It follows from here that $\lambda(v,w)=(\lambda v,w)$, as desired.
        \end{itemize}
        \item What do these relations do?
        \begin{itemize}
            \item Essentially, they allows us to treat tensor multiplication much like real multiplication, endowing the operation with distributivity, etc.
            \item For example, the rule $(v_1+v_2,w)=(v_1,w)+(v_2,w)$ becomes, in tensor product notation, $(v_1+v_2)\otimes w=v_1\otimes w+v_2\otimes w$.
        \end{itemize}
        \item Here's an example of this construction.
        \begin{itemize}
            \item Let $V=W$ be the one-dimensional vector space over the finite field $F_2=\Z/2\Z$.
            \item Thus, the elements of $V$ are $\{0,1\}$ (which is, literally, all linear combinations $a0+b1$ where $a,b\in F_2$ as well; this hearkens back to $V$'s definition as an $F_2$-module).
            \item Then the easy-to-work-with vector space we're talking about is the 4-dimensional \textbf{free} vector space $U=\spn(0\otimes 0,0\otimes 1,1\otimes 0,1\otimes 1)$.
            \item Note that in this space, for example, $(0+1)\otimes 0\neq 0\otimes 0+1\otimes 0$; representing the basis as column vectors, this is equivalent the obvious observation that
            \begin{equation*}
                \begin{bmatrix}
                    0\\
                    0\\
                    1\\
                    0\\
                \end{bmatrix}
                \neq
                \begin{bmatrix}
                    1\\
                    0\\
                    0\\
                    0\\
                \end{bmatrix}
                +
                \begin{bmatrix}
                    0\\
                    0\\
                    1\\
                    0\\
                \end{bmatrix}
            \end{equation*}
            \item But we want such relationships to hold true in our conceptual "tensor product space." Thus, we quotient it by the subspace spanning all elements of the form $(a+b)\otimes c-a\otimes c-b\otimes c$.
            \item By direct computation, this subspace is $\spn(0\otimes 0,0\otimes 1)$:
            \begin{align*}
                (0+0)\otimes 0-0\otimes 0-0\otimes 0 &= -0\otimes 0&
                    (0+0)\otimes 1-0\otimes 1-0\otimes 1 &= -0\otimes 1\\
                (0+1)\otimes 0-0\otimes 0-1\otimes 0 &= -0\otimes 0&
                    (0+1)\otimes 1-0\otimes 1-1\otimes 1 &= -0\otimes 1\\
                (1+1)\otimes 0-1\otimes 0-1\otimes 0 &= 0\otimes 0&
                    (1+1)\otimes 1-1\otimes 1-1\otimes 1 &= 0\otimes 1
            \end{align*}
            \begin{itemize}
                \item Note that once we've considered $(a+b)\otimes c$, we don't need to consider $(b+a)\otimes c$ because of the commutativity of addition in $V$. That is, it is axiomatic that $a+b=b+a$ for all $a,b\in V$.
                \item Additionally, in the last line above, we are using the facts that $1+1=2=0$ in $F_2$ and $a\otimes b+a\otimes b=2a\otimes b=0$ in any $F_2$-module to simplify the expressions.
                \item Furthermore, note that since $-1=1\in F_2$, $-0\otimes 0=1(0\otimes 0)=0\otimes 0\in V\otimes V$.
            \end{itemize}
            \item Similarly, the subspace corresponding to $a\otimes(b+c)-a\otimes b-a\otimes c$ is $\spn(0\otimes 0,1\otimes 0)$. Thus, altogether, we quotient out the subspace $X=\spn(0\otimes 0,0\otimes 1,1\otimes 0)$. This leaves us with a 1-dimensional $V\otimes V$, as expected for the tensor product of two one-dimensional vector spaces. It is interesting to note that the one vector we didn't quotient out ($1\otimes 1$) is analogous to $e_1\otimes e_1$ since $e_1\in V$ might as well be defined $e_1:=1$.
            \item Now let's see how well this quotienting worked. First off, a bit of notation: let $\pi:U\to V\otimes V$ be the projection $\pi:v\mapsto v+X$, and denote elements $\pi(v_1\otimes v_2)\in V\otimes V$ by $v_1\otimes_\pi v_2$ for now to differentiate them from elements of $U$.
            \item Let $(0+1)\otimes_\pi 0=(0+1)\otimes 0+X$ be an element of the quotient space $V\otimes V$. Certainly, the elements $0\otimes_\pi 0$ and $1\otimes_\pi 0$ are also elements of this quotient space. Moreover, there is no reason we can't form the linear combination $(0+1)\otimes_\pi 0-0\otimes_\pi 0-1\otimes_\pi 0$. Indeed, when we do, we notice that this element lies in the quotiented-out subspace $X$. Thus,
            \begin{equation*}
                (0+1)\otimes_\pi 0-0\otimes_\pi 0-1\otimes_\pi 0 = [(0+1)\otimes 0-0\otimes 0-1\otimes 0]+X
                = 0+X
                = 0
            \end{equation*}
            \item But
            \begin{equation*}
                (0+1)\otimes_\pi 0-0\otimes_\pi 0-1\otimes_\pi 0 = 0
                \quad\Longrightarrow\quad
                (0+1)\otimes_\pi 0 = 0\otimes_\pi 0+1\otimes_\pi 0
            \end{equation*}
            as desired.
            \item Note that this construction also gives us nice things like $0\otimes_\pi 0=0$, $0\otimes_\pi 1=0$, etc. which were not true in $U$!
            \item It should not be concluded, though, that all we need to quotient out of $U$ for any $V$ is $\spn(0\otimes 0,0\otimes v,v\otimes 0)$ for every $v\in V$; indeed, $V=\R$, for example, will require us to quotient out elements such as $4\otimes 7-2\otimes 7-2\otimes 7$, which can't even be expressed as a single simple tensor.
        \end{itemize}
    \end{itemize}
    \item \textbf{Free} (vector space): A vector space that has a basis consisting of linearly independent elements.
    \begin{itemize}
        \item Example: Think of $V=\C e_1\oplus\C e_2$ as a $\C$-module. A free version $F(V)$ of $V$ is infinite dimensional with every $v\in V$ a linearly independent basis vector. Elements of $F(V)$ are of the form $a_1v_1+\cdots+a_kv_k$ for $a_1,\dots,a_k\in\C$ and $v_1,\dots,v_k\in V$. If $u=v+w$ where $u,v,w\in V$ are all nonzero, then $u\neq v+w$ in $F(V)$ because they are all linearly independent basis vectors.
        \item Example: What we formally start with in the example above is the free $F_2$-module $V\times V$, \emph{not} the Cartesian product vector space $V\times V$.
        \item A terrific explanation of free vector spaces is available \href{https://math.stackexchange.com/questions/18315/free-vector-space-and-vector-space}{here}.
    \end{itemize}
    \item These two definitions constitute a first approximation to what the tensor product is.
    \item Example tensor product space.
    \begin{itemize}
        \item Suppose $V=\C e_1+\C e_2$. We want to look at $V\otimes V$.
        \item A priori\footnote{I.e., it follows from some logic. In particular, it follows from the logic that any element $v\in V$ is of the form $v=ae_1+be_2$, so of course all $v\otimes v$ must be of the given form for choices of $a,b,c,d$.}, it's spanned by $(ae_1+be_2)\otimes(ce_1+de_2)=ace_1\otimes e_1+ade_1\otimes e_2+bce_2\otimes e_1+cde_2\otimes e_2$.
        \item Thus, $V_1\otimes V_2$ has 4-element basis $e_1\otimes e_1,e_1\otimes e_2,e_2\otimes e_1,e_2\otimes e_2$.
    \end{itemize}
    \item Takeaway: What is true in general is that if $V$ has basis $e_1,\dots,e_n$ and $W$ has basis $f_1,\dots,f_m$, then $V\otimes W$ has basis $e_i\otimes f_j$ ($i=1,\dots,n$ and $j=1,\dots,m$).
    \item Having discussed the tensor product of vector spaces, let's think about the tensor product of \emph{representations}.
    \begin{itemize}
        \item Suppose $g:V\to V$ and $g:W\to W$.
        \begin{itemize}
            \item We're starting to make notation sloppy.
        \end{itemize}
        \item How does $g:V\otimes W\to V\otimes W$? Well, we just send $v\otimes w\mapsto(gv)\otimes(gw)$.
        \begin{itemize}
            \item Why is this map well-defined?
            \item We invoke the \textbf{universal property of the tensor product operation}.
            \item This guarantees us that given $g$ --- which is effectively a map from $V\times W\to V\otimes W$ as defined --- there nevertheless exists a complete extension $\tilde{g}:V\otimes W\to V\otimes W$.
        \end{itemize}
        \item As a matrix, this map is pretty strange!
        \begin{itemize}
            \item Example: Let $g:V\to V$ be a $2\times 2$ matrix. What is the matrix of $g:V\otimes V\to V\otimes V$?
            \item If
            \begin{equation*}
                \rho_V(g) = g =
                \begin{pmatrix}
                    a & b\\
                    c & d\\
                \end{pmatrix}
                =: A
            \end{equation*}
            then we have
            \begin{align*}
                g(e_1\otimes e_1) &= ge_1\otimes ge_1\\
                &= (ae_1+ce_2)\otimes(ae_1+ce_2)\\
                &= a^2e_1\otimes e_1+ace_1\otimes e_2+ace_2\otimes e_1+c^2e_2\otimes e_2
            \end{align*}
            \item Evaluating similarly for all basis vectors, we get a very curious block matrix:
            \begin{equation*}
                \begin{bNiceArray}{cccc}[first-row,first-col]
                     & e_1\otimes e_1 & e_1\otimes e_2 & e_2\otimes e_1 & e_2\otimes e_2\\
                    e_1\otimes e_1 & a^2 & ab & ab & b^2\\
                    e_1\otimes e_2 & ac  & ad & bc & bd\\
                    e_2\otimes e_1 & ac  & bc & ad & bd\\
                    e_2\otimes e_2 & c^2 & cd & cd & d^2\\
                \end{bNiceArray}
                =
                % \begin{bNiceArray}{c|c}[margin]
                %     aA & bA\\
                %     \hline
                %     cA & dA\\
                % \end{bNiceArray}
                \begin{bNiceArray}{cc|cc}[margin]
                    \Block{2-2}{aA} & & \Block{2-2}{bA} & \\
                     & & & \\
                    \hline
                    \Block{2-2}{cA} & & \Block{2-2}{dA} & \\
                     & & & \\
                \end{bNiceArray}
            \end{equation*}
            \item Notice how, for example, this takes the tensor $e_1\otimes e_1$, represented as $(1,0,0,0)$, to the tensor $a^2e_1\otimes e_1+ace_1\otimes e_2+ace_2\otimes e_1+c^2e_2\otimes e_2$, represented as $(a^2,ac,ac,c^2)$.
            \item Does this construction imply a canonical way to convert from tensors to column vectors??
        \end{itemize}
        \item Classically, this is called the \textbf{Kronecker product} of two matrices.
        \item People discovered all of this stuff before they unified it as tensor math.
    \end{itemize}
    \item \textbf{Universal property of the tensor product operation}: For every bilinear map $h:V\times W\to Z$, there exists a \emph{unique} linear map $\tilde{h}:V\otimes W\to Z$ such that $h=\tilde{h}\circ\otimes$.
    \begin{figure}[h!]
        \centering
        \DisableQuotes
        \begin{tikzcd}
            V\times W \arrow[r,"\otimes"] \arrow[rd,"h"'] & V\otimes W \arrow[d,"\tilde{h}"]\\
             & Z\\
        \end{tikzcd}
        \EnableQuotes
        \vspace{-1.5em}
        \caption{Universal property, tensor product operation.}
        \label{fig:UPtensorProd}
    \end{figure}
    \begin{proof}
        See the solid explanation \href{https://math.stackexchange.com/a/4248460/340652}{here}. Alternatively, here's my write up.\par\smallskip
        Let $V=\C e_1\oplus\cdots\oplus\C e_n$, $W=\C f_1\oplus\cdots\oplus\C f_m$, $Z$, and $h:V\times W\to Z$ be arbitrary. Define $\tilde{h}:V\otimes W\to Z$ by
        \begin{equation*}
            \tilde{h}(e_i\otimes f_j) := h(e_i,f_j)
        \end{equation*}
        for $i=1,\dots,n$ and $j=1,\dots,m$. Since a linear map is wholly defined by its action on the basis of its domain, this set of equations suffices to define $\tilde{h}$ on all of $V\otimes W$.\par
        \underline{Existence}: To prove that $\tilde{h}$ satisfies the "universal property," it will suffice to show that $h=\tilde{h}\circ\otimes$. Let $(v,w)\in V\times W$ be arbitrary, and suppose $v=\sum_{i=1}^na_ie_i\in V$, and $w=\sum_{i=1}^nb_if_i\in W$. Then
        \begin{align*}
            [\tilde{h}\circ\otimes](v,w) &= \tilde{h}(v\otimes w)\\
            &= \sum_{i=1}^n\sum_{j=1}^ma_ib_i\tilde{h}(e_i\otimes f_i)\\
            &= \sum_{i=1}^n\sum_{j=1}^ma_ib_ih(e_i,f_j)\\
            &= h(v,w)
        \end{align*}
        as desired.\par
        \underline{Uniqueness}: Now suppose $\tilde{g}:V\otimes W\to Z$ also satisfies the "universal property," that is, $h=\tilde{g}\circ\otimes$. Then by definition,
        \begin{equation*}
            \tilde{h}(e_i\otimes f_j) = h(e_i,f_j) = \tilde{g}(e_i\otimes f_j)
        \end{equation*}
        for $i=1,\dots,n$ and $j=1,\dots,m$. But since a linear map is wholly defined by its action on the basis of its domain, it follows that $\tilde{h}=\tilde{g}$, as desired.
    \end{proof}
    \item \textbf{Kronecker product} (of $A,B$): The matrix product defined as follows. \emph{Denoted by} $\bm{A\otimes B}$. \emph{Given by}
    \begin{equation*}
        A\otimes B =\
        \begin{bNiceArray}{c}[first-row,first-col]
              & n\\
            n & A\\
        \end{bNiceArray}
        \otimes\
        \begin{bNiceArray}{c}[first-row,first-col]
              & m\\
            m & B\\
        \end{bNiceArray}
        =\
        \begin{bNiceArray}{ccc}[first-row,first-col]
               &         & nm     &        \\
               & a_{11}B & \cdots & a_{1n}B\\
            nm & \vdots  & \ddots & \vdots \\
               & a_{n1}B & \cdots & a_{nn}B\\
        \end{bNiceArray}
    \end{equation*}
    \begin{itemize}
        \item The Kronecker product is \emph{not} commutative, but the matrices you get are related by conjugacy and by commuting the columns.
    \end{itemize}
    \item Vector spaces of the same dimension are all alike, but vector space representations are very interesting. By the end of this course, we'll understand what's going on.
    \item How we understand tensor stuff: Look at the abstract definition, look at the concrete definition, look at 5 examples, and then go in a circle. Repeat again and again until it makes sense.
    \begin{itemize}
        \item Rudenko is just trying to tell us all relevant words so that they will fit together later.
    \end{itemize}
    \item Fact: If $V,W$ finite-dimensional, $\Hom_F(V,W)\cong V\otimes W^*$.
    \begin{itemize}
        \item Tensor products are very nice spaces from which to construct maps.
        \item Let's construct a reverse map, then.
        \item Take $\alpha\otimes w\in V^*\otimes W$, where $\alpha:V\to F$ by definition. Send $\alpha\otimes w$ to the map $v\mapsto\alpha(v)w$. This is a \emph{canonical} map!! We can show that they span everything.
        \begin{itemize}
            \item For example, if we want to choose $\alpha\otimes w$ mapping to the matrix with a 1 in the upper left-hand corner and zeroes everywhere else, let $w=e_1\in W$ and let $\alpha=e^1\in V^*$.
            \item We can do similarly for all other such matrices, mapping this basis of $\Hom_F(V,W)$ to $e^i\otimes e_j$ ($i=1,\dots,n$ and $j=1,\dots,m$).
            \item Note that this also allows us to define a (noncanonical) inverse map.
        \end{itemize}
        \item This inverse map from $\Hom_F(V,W)\to V^*\otimes W$ is clearly a bit harder to work out.
        \item Hidden in this story is why trace is invariant under matrix conjugation, e.g., why $\tr(SAS^{-1})=\tr(A)$.
        \begin{itemize}
            \item If we take $\Hom_F(V,V)$, then this is isomorphic to $V^*\otimes V$.
            \item There is a very natural map from these isomorphic spaces to $F$.
            \item This map is defined by the trace (which sends $\Hom_F(V,V)\to F$), and $\alpha\otimes v\mapsto\alpha(v)$ (which sends $V^*\otimes V\to F$). We can prove this.
            \item Moreover, this map is canonical, as well.
            \item This is why the main property of the trace is that it's invariant under conjugation, i.e., because $SAS^{-1}$ and $A$ both map to the same element of $V^*\otimes V$. This fact is hidden in the story very nicely.
        \end{itemize}
    \end{itemize}
    \item Tensor products are hard, it will be a pain, we will understand them very well, but it will not be nice for now.
    \item Symmetric products and wedge powers will be discussed briefly next time.
    \begin{itemize}
        \item There is a nice description in \textcite{bib:Serre} that we can use for the homework.
    \end{itemize}
    \item Extra homework: Please read about tensor products in whatever textbook you like, try some examples, and repeat.
\end{itemize}



\section{Office Hours}
\begin{itemize}
    \item \marginnote{10/3:}Problem 2a:
    \begin{itemize}
        \item $\Lambda^2V$ is \emph{exterior powers}.
        \item The exact canonical isomorphism we need is briefly discussed on \textcite[473]{bib:FultonHarris}.
        \item I.e., we have to construct isomorphisms between the structures that don't rely on the choice of any basis. Recall the classic example of $V\cong V^{**}$, as explained in the well-written MSE post "basic difference between canonical isomorphism and isomorphims." Recall that the isomorphism from $V\to V^*$ defined by sending each element of the basis of $V$ to the corresponding element of the dual basis of $V^*$ is \emph{not} canonical because \emph{it involves choosing bases}. Definitions of canonical maps are available in MATH20510Notes, p. 2.
        \item From a quick look at this, it looks like the proof may be analogous to the classic middle-school algebra identity $(v+w)^2=v^2+vw+w^2$.
        \item The second exterior power $\Lambda^2V$ of a finite-dimensional vector space $V$ is the dual space of the vector space of alternating bilinear forms on $V$. Elements of $\Lambda^2V$ are called 2-vectors.
    \end{itemize}
    \item Problem 2b:
    \begin{itemize}
        \item $S^2V$ is \emph{symmetric powers}.
        \item The exact canonical isomorphism we need is briefly discussed on \textcite[473]{bib:FultonHarris}.
    \end{itemize}
    \item Problem 3a:
    \begin{itemize}
        \item This is the determinant of the multiplication table, in relation to that theorem that you showed us at the end of the first class? Yep!
    \end{itemize}
    \item Problem 3b:
    \begin{itemize}
        \item So a circulant matrix is a matrix like the multiplication table from (a)? Yep!
        \item Is $\zeta=e^{2\pi i/n}$? Sort of. It can be any $n^\text{th}$ root of unity.
    \end{itemize}
    \item Problem 4d:
    \begin{itemize}
        \item We'll cover higher symmetric powers in class tomorrow.
        \item However, it basically just means that we're now working with elements of the form $e_1\otimes e_2\otimes e_3\in S^3V$ and on and on.
    \end{itemize}
    \item Problem 5a:
    \begin{itemize}
        \item Is $V^\vee=V^*$? Yes. This is "vee check," and is a notation that some people prefer.
    \end{itemize}
    \item Problem 5b:
    \begin{itemize}
        \item Is "tr" the trace function of the linear map corresponding to $L$? Yes.
        \item What is $L$?
        \begin{itemize}
            \item An element of $V\otimes V^*$ is a linear combination of elements of the form $v\otimes\alpha$, not necessarily just one of these "decomposable" products.
            \item There is an isomorphism $V\otimes V^*\cong\Hom(V)$.
            \item Consider the matrix
            \begin{equation*}
                \begin{pmatrix}
                    1 & 0\\
                    0 & 0\\
                \end{pmatrix}
            \end{equation*}
            It sends $e_1\mapsto e_1$ and $e_2\mapsto 0$. Thus, it is well-matched with $e_1\otimes e^1$, which also grabs $e_1$ (with $e^1$) and sends it to $e_1$.
            \item Consider the matrix
            \begin{equation*}
                \begin{pmatrix}
                    0 & 1\\
                    0 & 0\\
                \end{pmatrix}
            \end{equation*}
            It sends $e_1\mapsto 0$ and $e_2\mapsto e_1$. Thus, it is well-matched with $e_1\otimes e^2$, which also grabs $e_2$ (with $e^2$) and sends it to $e_1$.
            \item In full,
            \begin{equation*}
                \begin{pmatrix}
                    a & b\\
                    c & d\\
                \end{pmatrix}
                \mapsto
                ae_1\otimes e^1+be_1\otimes e^2+ce_2\otimes e^1+de_2\otimes e^2
            \end{equation*}
            \item This map \emph{is} canonical! This is because the bases must be chosen to even begin talking about matrices.
            \item If you change the matrix, the bases change, too??
            \item Takeaway: We have to walk backwards from matrix to linear transformation to representation in $V\otimes V^*$ to a scalar in $F$.
        \end{itemize}
    \end{itemize}
    \item Problem 5c:
    \begin{itemize}
        \item So trace of such a map is equal to the dimension of its image? Yes.
    \end{itemize}
\end{itemize}



\section{Wedge and Symmetric Powers}
\begin{itemize}
    \item \marginnote{10/4:}OH slightly later today at 5:45-6:45 PM.
    \item Recap: Last time, we built new reps from old.
    \begin{itemize}
        \item This stuff can't be learned in 1.5 lectures; he can point us around, but we have to learn it ourselves.
    \end{itemize}
    \item Tensor product review.
    \begin{itemize}
        \item Given $V,W$, make $V\otimes_FW$.
        \item This vector space is hard to describe directly, so we more often talk about its dual $(V\otimes W)^*$ because this is actually easier to describe.
        \item If you want to work with $V\otimes W$ hands-on, you can do the following.
        \begin{itemize}
            \item Start with the following easy-to-work-with vector space: The (probably infinite-dimensional) vector space where each $v\otimes w$ is a basis vector for all $v\in V$ and $w\in W$.
            \item Then quotient it by relations to force them to hold in the final space.
        \end{itemize}
        \item If $V$ has basis $e_1,\dots,e_n$ and $W$ has basis $f_1,\dots,f_m$, then $e_i\otimes f_j$ is a basis of $V\otimes W$.
        \item Interesting fact 1: If $V,W$ are finite dimensional, $V^*\otimes W\cong\Hom(V,W)$.
        \item If we want to work with the tensor product in practice in \emph{rep theory}, the only thing we need to know is the basis of the tensor product space, which can tell us how any map $\rho(g)$ acts on both sides of a $v\otimes w\in V\otimes W$. From here, we recover the Kronecker product of matrices.
        \item So many things are explained by the concept of tensor products!
        \item A tensor in \emph{physics} is something with lots of indices that changes in some way.
        \begin{itemize}
            \item It does come from the math concept.
            \item We'll get a huge basis because we have a massive product like $V\otimes\cdots\otimes V\otimes V^*\otimes\cdots\otimes V^*$.
        \end{itemize}
    \end{itemize}
    \item Last 2 useful notions: Wedge powers and symmetric powers.
    \begin{itemize}
        \item Again, it's much easier to think about the dual space.
    \end{itemize}
    \item Consider the space $V^{\otimes n}$ (dimension $(\dim V)^n$).
    \begin{itemize}
        \item $(V^{\otimes n})^*$ are \textbf{polylinear} maps $f:V^n\to F$.
        \begin{itemize}
            \item Note: By contrast, $(V^n)^*$ is the space of all \emph{linear} maps $f:V^n\to F$.
            \item This distinction is subtle but important. Note, for instance, that $\dim V^{\otimes n}\neq\dim V^n$ and likewise for the duals.
            \item The distinction comes out fully when considering that if, for example, $V=\R^3$, then $V^2\cong\R^6$ and any map in $(V^2)^*$ is determined by its action on $(e_1,0),(e_2,0),(e_3,0),(0,e_1),(0,e_2),(0,e_3)$. By contrast, any map in $(V^{\otimes 2})^*$ is determined by its action on $(e_1,e_1),(e_1,e_2),(e_1,e_3),(e_2,e_1)$, $(e_2,e_2),(e_2,e_3),(e_3,e_1),(e_3,e_2),(e_3,e_3)$.
            \item Important note: What $(V^{\otimes 2})^*$ does is consider these nine elements of $V^2$ as the basis of another space. This is what it truly means when we say "a bilinear map on $V^2$ is a linear map on $V^{\otimes 2}$."
            \item Takeaway: Polylinearity changes the basis upon which a function $f:V^n\to F$ fundamentally acts.
        \end{itemize}
        \item A polylinear map may be \textbf{symmetric}, \textbf{antisymmetric}, or\footnote{This is an exclusive "or."} neither.
        \item These maps form vector spaces and the dimension is actually pretty meaningful.
    \end{itemize}
    \item \textbf{Symmetric} (polylinear map): A polylinear map $f:V^n\to F$ that satisfies the following property. \emph{Constraint}
    \begin{equation*}
        f(v_{\sigma(1)},\dots,v_{\sigma(n)}) = f(v_1,\dots,v_n)
    \end{equation*}
    \item \textbf{Antisymmetric} (polylinear map): A polylinear map $f:V^n\to F$ that satisfies the following property. \emph{Constraint}
    \begin{equation*}
        f(v_{\sigma(1)},\dots,v_{\sigma(n)}) = (-1)^\sigma f(v_1,\dots,v_n)
    \end{equation*}
    \item Suppose you take $V=\C e_1\oplus\C e_2$\footnote{Note that this notation allows you to define a vector space \emph{and} its basis in one go! I.e., the alternative is saying "Let $V$ be a complex vector space with basis $e_1,e_2$."}.
    \begin{itemize}
        \item Consider a symmetric polylinear map $f:V\times V\times V\to\C$.
        \item To compute it, we'll need the action of $f$ on the basis of $V^3$. In particular, we'll need\dots
        \begin{equation*}
            f(x_1e_1+y_1e_2,x_2e_1+y_2e_2,x_3e_1+y_3e_2) = x_1x_2x_3f(e_1,e_1,e_1)+x_1x_2y_3f(e_1,e_1,e_2)+\cdots
        \end{equation*}
        \begin{itemize}
            \item Somewhere in there, you'll also have a $x_1y_2x_3f(e_1,e_2,e_1)$ term as well.
            \item However, because $f$ is symmetric, you know by symmetry that these "bases" are the same, so you don't count them as 2 towards the dimension but as 1.
            \item Thus, $\dim=4$ for symmetric maps.
        \end{itemize}
        \item What about antisymmetric maps?
        \item Suppose $g:V^3\to\C$ is an antisymmetric polylinear map.
        \begin{itemize}
            \item Consider $g(e_1,e_1,e_1)$. Suppose you apply $(12)$. Interchanging the first two indices (for instance) obviously won't do anything, so we'll get
            \begin{align*}
                g(e_1,e_1,e_1) &= (-1)^{(12)}g(e_1,e_1,e_1)\\
                g(e_1,e_1,e_1) &= -g(e_1,e_1,e_1)\\
                2g(e_1,e_1,e_1) &= 0\\
                g(e_1,e_1,e_1) &= 0
            \end{align*}
            \item But what about $g(e_1,e_1,e_2)$? We could apply $(23)$ and get $g(e_1,e_2,e_1)$, right? So it appears that we would just be shrinking two options into one. Technically, this is true, but what's more important is that applying $(12)$ again yields the same thing, meaning that $g(e_1,e_1,e_2)=g(e_1,e_2,e_1)=0$.
            \item And thus, since $V$ has dimension 2 but $g$ takes three vectors, any argument submitted to $g$ will always be linearly dependent. Thus, $g=0$ and, in fact, the space of antisymmetric maps on $V^3$ has dimension 0.
        \end{itemize}
        \item Takeaway: It's not always a rule that $V^{\otimes m}\cong S^mV\oplus\Lambda^mV$.
    \end{itemize}
    \item Mathematically, there's a more natural object to work with than symmetric and antisymmetric maps.
    \begin{itemize}
        \item Wedge powers and symmetric powers!
        \item Given $V$ and $n\in\N$, we can construct $S^nV$ and $\Lambda^nV$. $(S^nV)^*$ is symmetric polylinear maps taking $n$ arguments from $V$. $(\Lambda^nV)^*$ is antisymmetric polylinear maps taking $n$ arguments from $V$.
    \end{itemize}
    \item How about a concrete way to see these? We can relate them to tensor powers.
    \begin{itemize}
        \item Take a tensor power $V^{\otimes n}$, then look at those tensors which are symmetric and antisymmetric under permutation.
        \item Example: Let $V$ be the same as before. Then $V^{\otimes 2}$ has $\dim=4$.
        \begin{itemize}
            \item Take as basis elements for $S^2V$ those that don't change when you change the coordinates.
            \item Take as basis elements for $\Lambda^2V$ those that flip sign when you change the coordinates.
            \item In this case, the basis of $V^{\otimes 2}$ is $e_1\otimes e_1,e_1\otimes e_2,e_2\otimes e_1,e_2\otimes e_2$. The basis of $S^2V$ will be $e_1\otimes e_1,e_1\otimes e_2+e_2\otimes e_1,e_2\otimes e_2$. The basis of $\Lambda^2V$ will be $e_1\otimes e_2-e_2\otimes e_1$. Notice that these bases are identical (up to scaling) with those in \textcite{bib:Serre} and those produced by applying the \href{https://en.wikipedia.org/wiki/Symmetrization}{\textbf{symmetrization}} and \href{https://en.wikipedia.org/wiki/Exterior_algebra#Alternating_tensor_algebra}{\textbf{alternation}} operators to the basis of $V^{\otimes 2}$.
        \end{itemize}
        \item $S^2V$ and $\Lambda^2V$ can form a \emph{direct} sum because the dimensions match and they don't intersect.
        \item Everything we're doing is representations, so $g(v_1\otimes\cdots\otimes v_n)=gv_1\otimes\cdots\otimes gv_n$.
    \end{itemize}
    \item Relating this to something that we've seen but that is a little confusing.
    \begin{itemize}
        \item The product notation is suggestive for symmetric vectors; you can commute $e_1\cdot e_2\in S^2V$, for instance.
        \item This allows us to, for example, shrink $e_1\otimes e_1$ to $2e_1^2$\footnote{Why the 2 coefficient? Because technically, the symmetrization operator takes $e_1\otimes e_1\mapsto e_1\otimes e_1+e_1\otimes e_1=2e_1\cdot e_1=2e_1^2$.}, but $e_1\otimes e_2+e_2\otimes e_1$ only to $e_1\cdot e_2$.
        \item Note that $e_1\wedge e_2=e_1\otimes e_2-e_2\otimes e_1$ by definition.
        \item Fact/exercise: Let $V$ be a vector space of dimension $n$. $V^*$ is the dual space, but it is also a function space. If $V=\R^k$, its a space of \emph{functions from the blackboard}.
        \begin{itemize}
            \item Note that $(\Lambda^kV)^*=\Lambda^kV^*$.
        \end{itemize}
        \item $S^nV^*$ is homogeneous polynomials of degree $n$.
        \item You can take higher degree polynomials and just keep pushing through.
        \begin{itemize}
            \item Ask about this??
        \end{itemize}
        \item Wedge powers now.
        \item By convention, $\Lambda^0V=F$ and $\Lambda^1V=V$. But then you get to $\Lambda^2V$ and $\Lambda^3V$. They grow but then shrink down as the power approaches $\dim V$.
        \item Truth: The dimension of wedge powers $\Lambda^iV$ is $\binom{k}{i}$ for $\dim V=k$. Figuring out why this is the case is another good exercise.
    \end{itemize}
    \item An interesting connection between wedge powers and the determinant.
    \begin{itemize}
        \item Let $V=\C e_1\oplus\cdots\oplus\C e_n$.
        \item Recall that $\Lambda^nV^*$ is the space of antisymmetric polylinear functions $V\times\cdots\times V\to F$ taking $n$ arguments from $V$, and it has a single basis vector $e^1\wedge\cdots\wedge e^n$.
        \item Let $v_1=\sum a_{i1}e_i$, $v_2=\sum a_{i2}e_i$, etc.
        \item Let $f\in\Lambda^nV^*$, so that $f$ is an alternating polylinear map that takes $n$ arguments.
        \item Since $f$ is polylinear, we have that
        \begin{equation*}
            f(v_1,\dots,v_n) = \sum_{i_1,\dots,i_n=1}^na_{i_11}\cdots a_{i_nn}f(e_{i_1},\dots,e_{i_n})
        \end{equation*}
        \item Because of antisymmetry, we need only look at elements where the indices are all different. Thus, the above equals
        \begin{equation*}
            \sum_{\sigma\in S_n}a_{\sigma(1)1}\cdots a_{\sigma(n)n}f(e_{\sigma(1)},\dots,e_{\sigma(n)})
        \end{equation*}
        \item Additionally, $f(e_{\sigma(1)},\dots,e_{\sigma(n)})=(-1)^\sigma f(e_1,\dots,e_n)$ for any $\sigma\in S_n$. Moreover, $f(e_1,\dots,e_n)\in\C$ by definition, so define a constant $\lambda:=f(e_1,\dots,e_n)$. Thus, the above equals
        \begin{equation*}
            \lambda\sum_{\sigma\in S_n}a_{\sigma(1)1}\cdots a_{\sigma(n)n}
        \end{equation*}
        \item But the term following the $\lambda$ is just the determinant of the $n\times n$ matrix $(a_{ij})$. Thus, all said,
        \begin{equation*}
            f(v_1,\dots,v_n) = \lambda\det(v_1\mid\cdots\mid v_n)
        \end{equation*}
        \item Implication: Wedge powers are something like the determinant.
        \begin{itemize}
            \item In particular, because $\Lambda^nV^*$ has only a single basis vector as mentioned above, $f=\lambda e^1\wedge\cdots\wedge e^n$. It follows that $e^1\wedge\cdots\wedge e^n=\det$.
        \end{itemize}
        \item Takeaway: Wedge powers are something interesting; there's a reason to study them.
    \end{itemize}
    \item The basis of the wedge powers consists of wedge monomials $e_{j_1}\wedge\cdots\wedge e_{j_i}$. Moreover, no need to have the same list twice, so choose some way of indexing them, e.g., increasing indexes.
    \begin{itemize}
        \item This is why we do \emph{increasing} bases! There's no particular reason, it's just an arbitrary way of making sure we don't do the same thing twice! We could just as well choose decreasing or any other means of guaranteeing that we don't have duplicates.
    \end{itemize}
    \item Now let's relate all of this exterior and symmetric product stuff back to representation theory.
    \begin{itemize}
        \item Let $V=\C e_1\oplus\cdots\oplus\C e_n$.
        \item Let $G\acts V$ via the homomorphism $G\to GL(V)\cong GL_n(\C)$.
        \item Focusing more on the \emph{matrix} aspect this time, note that under this homomorphism, $g\mapsto A_g$ subject to the homomorphism constraints $A_e=E_n$, etc.
        \item Consider the set $\{A_{g_1},\dots,A_{g_k}\}$ of all matrices in the image of the homomorphism. If we transpose all of them, will they still obey the homomorphism constraints?
        \begin{itemize}
            \item Nope!
            \item Indeed, if we do this, we'll get in trouble. More specifically, transposition is not a representation because $A_{g_1}^TA_{g_2}^T\neq A_{g_1g_2}^T=A_{g_2}^TA_{g_1}^T$.
        \end{itemize}
        \item It's the same story with inverses.
        \item \emph{However}, combining the two operations, we get
        \begin{equation*}
            (A_{g_1g_2}^T)^{-1} = (A_{g_1}^T)^{-1}(A_{g_2}^T)^{-1}
        \end{equation*}
        \begin{itemize}
            \item This is exactly when we take a representation and then go to the dual\footnote{Relation to MATH 20510 when we discussed dual matrices and pullbacks of matrices.}.
        \end{itemize}
        \item This will be on next week's homework!
        \item Takeaway: This is an application of $\Lambda^jV^*$ to representation theory, $j\neq k,n$.
    \end{itemize}
    \item Another relation: An application of $\Lambda^nV^*$ to representation theory.
    \begin{itemize}
        \item Suppose we have a representation $G\acts V$ that we want to flatten into $G\acts\C$. How can we turn a relation between a group of matrices into a relation between a group of numbers?
        \item Use the determinant!
        \item Indeed, we already know that
        \begin{align*}
            \det(A_e) &= 1&
            \det(A_{g_1g_2}) &= (\det A_{g_1})(\det A_{g_2})&
            \det(A_{g^{-1}}) &= \det(A_g)^{-1}
        \end{align*}
        \item In particular, we make formal the transition $G\to GL_j(\C)\to\C$ with the \textbf{top wedge power} $\Lambda^nV^*$.
    \end{itemize}
    \item A last note.
    \begin{itemize}
        \item Don't think that we're limited to top wedge powers.
        \item Recall that we can define tensor products of matrices via the Kronecker product. Well, we can prove that
        \begin{equation*}
            A_{g_1g_2}^{\otimes 2} = A_{g_1}^{\otimes 2}A_{g_2}^{\otimes 2}
        \end{equation*}
        and the like as well!
        \item Similarly, we can define $\Lambda^2$ of a matrix.
        \item We'll get into some weird Kronecker product stuff again, but we can sort through it.
    \end{itemize}
    \item Plan for Friday and next time.
    \begin{itemize}
        \item Prove the theorem that every representation is a sum of irreducible representations.
        \item He will use projectors.
        \item Then a horror story.
        \item Then associative algebra.
    \end{itemize}
\end{itemize}



\section{Office Hours}
\begin{itemize}
    \item \marginnote{10/5:}Problem 2a:
    \begin{itemize}
        \item $(V\oplus W)\otimes(V\oplus W)\stackrel{?}{=}V\otimes V\oplus V\otimes W\oplus W\otimes V\oplus W\otimes W$.
        \item Check linearity in all terms and then with universal property. Check antisymmetric, linear, injective, surjective; dimensions are the same, so no need to check \emph{both} injectivity and surjectivity (surjectivity is easier to check). We can go to basis to check various properties; we can't use a basis to write the map, but we can use bases to check surjectivity and the like.
    \end{itemize}
    \item Problem 3a:
    \begin{itemize}
        \item Bezout and Gauss's lemma is good to learn on my own. Put polynomials in each variable. Throw some stuff about this shit into my answers.
        \item Relearn polynomial division.
        \item $(1,1,1,1)$, $(1,1,-1,-1)$, $(1,-1,1,-1)$, and $(1,-1,-1,1)$.
        \item This is a symmetric matrix.
        \item The upper-left and lower-right blocks of this matrix match; so do the lower-left and upper-right.
        \item When the eigenvalue is equal to zero, the determinant is equal to zero. So look for eigenvectors to calculate eigenvalues, and then just express the determinant as a product of these.
    \end{itemize}
    \item Problem 3b:
    \begin{itemize}
        \item Corresponding eigenvalue is $\sum_{i=1}^nx_iz^{i-1}$.
        \item Can I use representation theory to do this? What group has a multiplication table like this? $\Z/n\Z$. The elements of $\Z/n\Z$ are of the form $\{1,\zeta,\dots,\zeta^{n-1}\}$.
        \item If that's an eigenvector, then it's a subrepresentation; it is a space that is fixed under the action of the matrix.
        \item Other eigenvectors: $(1,1,1)$, $(1,z^2,z)$.
        \item We don't need to do induction or anything fancy like that; we can just do dots. As long as your argument is complete and clear, you're good.
    \end{itemize}
    \item Problem 4a:
    \begin{itemize}
        \item See FH 1.3. Standard rep, not wedge. Treat $\tau,\sigma$ (generators of the action) on the basis vectors.
        \item If both fix, it's the trivial; if one flips, you have alternating; if both flip, you have standard.
        \item $(2,1)\oplus(1,1,1)$. Use problem 2.
        \item See FH Exercise 1.2??
        \item The action of $\tau$ on this basis vector can be computed:
        \begin{equation*}
            \tau(\alpha\wedge\beta) = 1\alpha\wedge\beta
        \end{equation*}
        \item Having obtained an eigenvalue of 1, we can rule out the standard representation. 
    \end{itemize}
    \item Problem 4b:
    \begin{itemize}
        \item $\{\alpha\otimes\alpha\otimes\alpha,\alpha\otimes\alpha\otimes\beta+\alpha\otimes\beta\otimes\alpha+\beta\otimes\alpha\otimes\alpha,\beta\otimes\beta\otimes\beta\}$
    \end{itemize}
    \item Problem 5a:
    \begin{itemize}
        \item Consider an alternate basis $f_1,\dots,f_n$ and dual basis $f^1,\dots,f^n$. Consider the element $f_1\otimes f^1+\cdots+f_n\otimes f^n\in V\otimes V^\vee$. We want to prove that it equals the one asked about in the question.
        \item Under the isomorphism to $\Hom(V,V)$, we send $e_1\otimes e^1$ to $[v\mapsto e^1(v)e_1]$. More generally, we end $e_i\otimes e^i$ to $[v\mapsto e^i(v)e_i]$. Adding all these maps together yields the map $[v\mapsto e^1(v)e_1+\cdots+e^n(v)e_n]$, which is just the identity $1\in\Hom(V,V)$, regardless of basis.
    \end{itemize}
    \item Problem 5b:
    \begin{itemize}
        \item Example:
        \begin{equation*}
            \begin{pmatrix}
                a & b\\
                c & d\\
            \end{pmatrix}
            \to
            ae_1\otimes e^1+be_1\otimes e^2+ce_2\otimes e^1+de_2\otimes e^2
        \end{equation*}
        \item Evaluating this gives
        \begin{equation*}
            e^1(ae_1)+e^2(be_1)+e^1(ce_2)+e^2(de_2) = a+d
        \end{equation*}
        since it's only when the indices match (i.e., along the diagonal) that we get a nonzero value.
    \end{itemize}
    \item Problem 5c:
    \begin{itemize}
        \item $P$ should have a block-diagonal matrix corresponding to the decomposition $V=W\oplus W^0$. $P$ is the identity on $\im(P)$. So if our basis is vectors spanning $W$ and then vectors spanning $W^0$, the matrix should be the identity and then the zero matrix. That should do the trick. How rigorous does this need to be?
        \item Let $e_1,\dots,e_k$ be an orthonormal basis of $\im(P)$. Extend this basis to an orthonormal basis $e_1,\dots,e_n$ of $V$.\par
    \end{itemize}
    \item Problem 5d:
    \begin{itemize}
        \item Trivial representation: All $g\in G$ get mapped to $1\in GL(V)$.
        \item Part (a) gives us the identity in $\Hom(V,V)$.
        \item So we have $\rho:G\to GL(V)$.
        \item Is any line acceptable? Span of the identity function? Rudenko: It depends on $V$. It has \emph{infinitely many} trivial sub representations.
        \item Example: $G\acts \C^2$. with $\rho(g)=I_2$.
        \item Dual representation: Defined analogously to the $\Hom_F(V,W)$ representation. We also need an inverse.
    \end{itemize}
    \item Psets will likely get easier; right now, we have to relearn a lot of old stuff and we are being challenged with harder problems. As the questions become more based on course content and thus will get easier.
    \item He'll do hard PSets, easy exams, and everything is curved; he agrees that this is a hard pset, and probably harder than necessary.
\end{itemize}



\section{Complete Reducibility}
\begin{itemize}
    \item \marginnote{10/6:}Let $G$ be a finite group.
    \item We want to study finite dimensional representations over $\C$.
    \begin{itemize}
        \item Characteristic $F$, $|G|=1$.
        \item What is this stuff about characteristic??
    \end{itemize}
    \item Theorem: Any f.d. representation can be decomposed into a sum of irreps via
    \begin{equation*}
        V = V_1^{n_1}\oplus\cdots\oplus V_k^{n_k}
    \end{equation*}
    Moreover, this decomposition is unique.
    \begin{itemize}
        \item See Proposition \ref{prp:completeReducibility}.
    \end{itemize}
    \item Example: We have already seen $S_2,S_3$ in the homework; now, let $G=\Z/n\Z$.
    \begin{itemize}
        \item Consider $V_0,\dots,V_{n-1}$.
        \item Let $V_i$ be a 1-dimensional rep.
        \begin{itemize}
            \item We have $\rho:G\to C^\times$ defined by $[k]\mapsto(\e[2\pi i/n])^k$.
        \end{itemize}
        \item These are all 1-dimensional representations up to isomorphism.
    \end{itemize}
    \item Example: Let $G=\Z$. What is a representation of $\Z$? We just need to say what happens to 1.
    \begin{itemize}
        \item For example, if the map $G\to GL_n(\C)$ sends $1\mapsto A$, then $2\mapsto A^2$, and on and on.
        \item A place where you run into trouble: $n=2$ and
        \begin{equation*}
            A =
            \begin{pmatrix}
                1 & 1\\
                0 & 1\\
            \end{pmatrix}
        \end{equation*}
        \item The matrix has a fixed subrepresentation (i.e., eigenvector $(1,1)$).
        \begin{itemize}
            \item More specifically, $\C(1,0)\hookrightarrow V$ is a 1D subrepresentation.
        \end{itemize}
        \item The theorem basically tells us that $V=\C(1,0)+\C w$.
        \item This is an example of how things can go wrong. How??
    \end{itemize}
    \item Proving the theorem; we need a miracle!
    \item Existence: We need a lemma.
    \item Lemma: Let $G$ be finite, $F=\C$, and $V$ a $G$-representation. Let $W\leq V$ be a subrepresentation or \textbf{invariant subspace}. Then there exists another invariant subspace $W'\subset V$ such that $V=W\oplus W'$.
    \begin{itemize}
        \item See Theorem \ref{trm:complements} (Rudenko replicates all aspects of the "limited conditions" proof).
        \item This lemma implies existence.
        \item Two proofs: One that only works over complex numbers. He suggests we read about it. Name??
        \item He'll do the slightly less intuitive one, which involves \textbf{projectors}.
    \end{itemize}
    \item \textbf{Projector}: A linear map $P:V\to V$ such that $P^2=P$, that is, is \textbf{idempotent}.
    \begin{itemize}
        \item Example: Consider $W:=\im(P)\leq V$. $P|_W$ does nothing; it's the identity.
        \item A good mental picture: Things are falling from 3D space onto some smaller space.
        \item On the kernel.
        \begin{itemize}
            \item Importantly, $\ker P\cap\im P=0$.
            \item It follows that $V=\im(P)\oplus\ker(P)$.
            \item Within the space, $v=(w,w')=w+w'$. What the projector does is $(w,w')\mapsto(w,0)$.
        \end{itemize}
        \item What else can we say about projectors?
        \begin{itemize}
            \item There is a correspondence between projectors and direct sum decompositions.
        \end{itemize}
    \end{itemize}
    \item So to prove the lemma, we need a projector $P:V\to V$ with image $W$ and certain properties.
    \begin{itemize}
        \item More specifically, the goal is to find a projector $P$\dots
        \begin{enumerate}
            \item With image $W$;
            \item That is a morphism of $G$-reps.
        \end{enumerate}
        \item On the second condition, that is, we want $P(gv)=gP(v)$. In this case, $\ker(P)$ will be a shuffle??
    \end{itemize}
    \item Strategy.
    \begin{itemize}
        \item Take any projector $P_0:V\to W$. And then you can get $g$-projectors $gP=gPg^{-1}$.
        \item So define a new projector
        \begin{equation*}
            P = \frac{1}{|G|}\sum_{g\in G}\underbrace{gP_0g^{-1}}_{gP_0}
        \end{equation*}
        One didn't work, so we hope the average will work, and it will!
        \item For any $w\in W$, we can prove that the sum thing does fix $W$'s, so it is a projector!
        \item $P(hv)$ example.
        \item Note: This computation will be done again later in a different context; this averaging construction is \emph{central} to representation theory.
    \end{itemize}
    \item Constraints we used in the proof: $G$ is finite (or compact), $|G|$ is invertible.
    \begin{itemize}
        \item Only when we get into \textbf{modular representation theory} is where we get into trouble; this theorem actually kills \textbf{extensions}, which are very interesting but are not in finite group rep theory.
    \end{itemize}
    \item Hermitian inner product isn't common here, but it shows up in physics. We will talk a bit more about inner products later, though.
    \item Now for the other part of the original proof: Uniqueness.
    \item Schur's Lemma: Let $G$ be a finite group, let the field $F=\C$, and let $V,W$ be irreps over $F$. Let $f\in\Hom_G(V,W)$, which we may recall is the space of morphisms between $G$-reps $V$ and $W$, that is, all $h:V\to W$ satisfying $h(gv)=gh(v)$. Then\dots
    \begin{enumerate}
        \item $f=0$ if $V\ncong W$. If $V\cong W$, then these maps are isomorphisms.
        \item In particular, if $V$ is an irrep and $f:V\to V$ is such that $f(gv)=gf(v)$, then $f(v)=\lambda v$.
    \end{enumerate}
    Altogether, we have that
    \begin{equation*}
        \Hom_G(V,W) \cong
        \begin{cases}
            0 & V\ncong W\\
            \C & V\cong W
        \end{cases}
    \end{equation*}
    \begin{itemize}
        \item The statement "$\Hom_G(V,W)\cong\C$" reflects the fact that the left space is the space of all scalar isomorphisms $\lambda I$ for scalars $\lambda\in\C$, which happens to be isomorphic to $\C$.
        \item Gist: If you want a certain kind of matrix between certain spaces, in some cases, you'll just fail.
    \end{itemize}
    \item Proof.
    \begin{itemize}
        \item See Lemma \ref{lem:Schur} (Rudenko replicates all aspects of the "limited conditions" proof).
        \item For $f:V\to V$, consider $\ker(f)$ and $\im(f)$. The latter two are subrepresentations of $V,W$, respectively. $\ker(f)=V$ implies $f=0$; symmetric with $\im(f)$. If nonzero, then $\ker=0$ and $\im=W$, implies $f$ is an isomorphism.
    \end{itemize}
    \item Schur's Lemma is the easiest step to learn in the whole story.
    \item Last 2 minutes: Finish the proof of the original theorem.
    \begin{itemize}
        \item Don't worry if we're confused by this last line; it will be repeated later in a much more powerful way.
        \item Analogous to Proposition \ref{prp:completeReducibility}.
        \item I might have missed some stuff here??
    \end{itemize}
    \item Plan for next week.
    \begin{itemize}
        \item Character theory.
        \item \textcite{bib:Serre} is still the best source for tracking lecture content for right now.
    \end{itemize}
    \item This would have been an interesting but wholly nonessential lecture to pay attention to, since I already did all of the readings.
\end{itemize}



\section{S Chapter 1: Generalities on Linear Representations}
\emph{From \textcite{bib:Serre}.}
\subsection*{Section 1.5: Tensor Product of Two Representations}
\begin{itemize}
    \item \marginnote{10/4:}\textbf{Tensor product} (of $V_1,V_2$): The vector space $W$ that (a) is furnished with a map $V_1\times V_2\to W$ sending $(x_1,x_2)\mapsto x_1\cdot x_2$ and (b) satisfies the following two conditions.
    \begin{enumerate}[label={(\roman*)}]
        \item $x_1\cdot x_2$ is bilinear.
        \item If $(e_{i_1})$ is a basis of $V_1$ and $(e_{i_2})$ is a basis of $V_2$, the family of products $e_{i_1}\cdot e_{i_2}$ is a basis of $W$.
    \end{enumerate}
    \emph{Denoted by} $\bm{V_1\otimes V_2}$.
    \begin{itemize}
        \item It can be shown that such a space exists and is unique up to isomorphism (see proof \href{https://www-users.cse.umn.edu/~garrett/m/algebra/notes/27.pdf}{here}).
    \end{itemize}
    \item This definition allows us to say some things quite expediently. For example, (ii) implies that
    \begin{equation*}
        \dim(V_1\otimes V_2) = \dim(V_1)\cdot\dim(V_2)
    \end{equation*}
    \item \textbf{Tensor product} (of $\rho^1,\rho^2$): The representation $\rho:G\to GL(V_1\otimes V_2)$ defined as follows for all $s\in G$, $x_1\in V_1$, and $x_2\in V_2$, where $\rho^1:G\to GL(V_1)$ and $\rho^2:G\to GL(V_2)$ are representations. \emph{Given by}
    \begin{equation*}
        [\rho_s^1\otimes\rho_s^2](x_1\cdot x_2) = \rho_s^1(x_1)\cdot\rho_s^2(x_2)
    \end{equation*}
    \item A more formal write up of the matrix translation of this definition.
    \begin{itemize}
        \item Let $(e_{i_1})$ be a basis for $V_1$, and let $(e_{i_2})$ be a basis for $V_2$.
        \item Let $r_{i_1j_1}(s)$ be the matrix of $\rho_s^1$ with respect to this basis, and let $r_{i_2j_2}(s)$ be the matrix of $\rho_s^2$ with respect to this basis.
        \item It follows that
        \begin{align*}
            \rho_s^1(e_{j_1}) &= \sum_{i_1}r_{i_1j_1}(s)e_{i_1}&
            \rho_s^2(e_{j_2}) &= \sum_{i_2}r_{i_2j_2}(s)e_{i_2}
        \end{align*}
        \item Therefore,
        \begin{equation*}
            [\rho_s^1\otimes\rho_s^2](e_{j_1}\cdot e_{j_2}) = \sum_{i_1,i_2}r_{i_1j_1}(s)r_{i_2j_2}(s)e_{i_1}\cdot e_{i_2}
        \end{equation*}
        and
        \begin{equation*}
            \mathcal{M}(\rho_s^1\otimes\rho_s^2) = (r_{i_1j_1}(s)r_{i_2j_2}(s))
        \end{equation*}
    \end{itemize}
    \item Aside on quantum chemistry to come back to later; I can't quite connect the dots yet.
\end{itemize}


\subsection*{Section 1.6: Symmetric Square and Alternating Square}
\begin{itemize}
    \item Herein, we investigate the tensor product when $V_1=V_2=V$.
    \item Let $(e_i)$ be a basis of $V$.
    \item Define the automorphism $\theta:V\otimes V\to V\otimes V$ by
    \begin{equation*}
        \theta(e_i\cdot e_j) = e_j\cdot e_i
    \end{equation*}
    for all 2-indices $(i,j)$.
    \item Properties of $\theta$.
    \begin{itemize}
        \item Since $\theta$ is linear, it follows that
        \begin{equation*}
            \theta(x\cdot y) = y\cdot x
        \end{equation*}
        for all $x,y\in V$.
        \begin{itemize}
            \item Implication: $\theta$ is independent of the chosen basis $(e_i)$!
        \end{itemize}
        \item $\theta^2=1$, where 1 is the identity map on $V\otimes V$.
    \end{itemize}
    \item Assertion: $V\otimes V$ decomposes into
    \begin{equation*}
        V\otimes V = S^2(V)\oplus\Lambda^2(V)
    \end{equation*}
    \begin{itemize}
        \item Rudenko: We do not have to worry about proving this\dots yet, at least.
    \end{itemize}
    \item \textbf{Symmetric square representation}: The subspace of $V\otimes V$ containing all elements $z$ satisfying $\theta(z)=z$. \emph{Denoted by} $\bm{S^2V}$, $\bm{S^2(V)}$, $\pmb{\mathbb{S}}\bm{{}^2V}$, $\pmb{\textbf{Sym}}\,\bm{{}^2(V)}$.
    \begin{itemize}
        \item Basis: $(e_i\cdot e_j+e_j\cdot e_i)_{i\leq j}$.
        \begin{itemize}
            \item Rudenko: How do we know everything is linearly independent? Well, when we add two linearly independent vectors out of a set, the sum is still linearly independent from everything else!
            \item Example when $\dim V=2$: The basis of $V\otimes V$ is $e_1\otimes e_1,e_1\otimes e_2,e_2\otimes e_1,e_2\otimes e_2$, where all four of these vectors are linearly independent. So naturally, the basis of the corresponding symmetric square representation --- which is $2e_1\otimes e_1,e_1\otimes e_2+e_2\otimes e_1,2e_2\otimes e_2$ --- will still be a linearly independent list of vectors.
        \end{itemize}
        \item Dimension: If $\dim V=n$, then
        \begin{equation*}
            \dim S^2(V) = \frac{n(n+1)}{2}
        \end{equation*}
    \end{itemize}
    \item \textbf{Alternating square representation}: The subspace of $V\otimes V$ containing all elements $z$ satisfying $\theta(z)=-z$. \emph{Denoted by} $\bm{\Lambda^2V}$, $\bm{\Lambda^2(V)}$, $\pmb{\textbf{Alt}}\,\bm{{}^2(V)}$.
    \begin{itemize}
        \item Basis: $(e_i\cdot e_j-e_j\cdot e_i)_{i<j}$.
        \item Dimension: If $\dim V=n$, then
        \begin{equation*}
            \dim\Lambda^2(V) = \frac{n(n-1)}{2}
        \end{equation*}
    \end{itemize}
\end{itemize}



\section{FH Appendix B: On Multilinear Algebra}
\emph{From \textcite{bib:FultonHarris}.}
\subsection*{Section B.1: Tensor Products}
\begin{itemize}
    \item \marginnote{10/5:}\textbf{Tensor product} (of $V,W$ over $F$): A vector space $U$ equipped with a bilinear map $V\times W\to U$ sending $v\times w\to v\otimes w$ that is universal, i.e., for any bilinear map $\beta:V\times W\to Z$, there is a unique linear map from $U\to Z$ that takes $v\otimes w\mapsto\beta(v,w)$. \emph{Denoted by} $\bm{V\otimes W}$, $\bm{V\otimes_FW}$.
    \begin{itemize}
        \item The so-called \emph{universal property} determines the tensor product up to canonical isomorphism.
    \end{itemize}
    \item One construction of $V\otimes W$: From the basis $\{e_i\otimes f_j\}$.
    \begin{itemize}
        \item This construction is \textbf{functorial}, implying that linear maps from $f:V\to V'$ and $g:W\to W'$ determine a linear map $f\otimes g:V\otimes W\to V'\otimes W'$, namely that defined by $f\otimes g:v\otimes w\to fv\otimes gw$.
    \end{itemize}
    \item Definition of the \textbf{$\bm{n}$-fold tensor product}.
    \item \textbf{Multilinear} (map): A map from a Cartesian product $V_1\times\cdots\times V_n$ of vector spaces to a vector space $U$ such that when all but one of the factors $V_i$ are fixed, the resulting map from $V_i\to U$ is linear.
    \item Properties of the tensor product.
    \begin{enumerate}
        \item \emph{Commutativity}:
        \begin{equation*}
            V\otimes W\cong W\otimes V
        \end{equation*}
        by $v\otimes w\mapsto w\otimes v$.
        \item \emph{Distributivity}:
        \begin{equation*}
            (V_1\oplus V_2)\otimes W \cong (V_1\otimes W)\oplus(V_2\otimes W)
        \end{equation*}
        by $(v_1,v_2)\otimes w\mapsto(v_1\otimes w,v_2\otimes w)$.
        \item \emph{Associativity}:
        \begin{equation*}
            (U\otimes V)\otimes W \cong U\otimes(V\otimes W) \cong U\otimes V\otimes W
        \end{equation*}
        by $(u\otimes v)\otimes w\mapsto u\otimes(v\otimes w)\mapsto u\otimes v\otimes w$.
    \end{enumerate}
    \item \textbf{Tensor power} (of $V$ to $n$): The tensor product defined as follows. \emph{Denoted by} $\bm{V^{\otimes n}}$. \emph{Given by}
    \begin{equation*}
        V^{\otimes n} = \underbrace{V\otimes\cdots\otimes V}_{n\text{ times}}
    \end{equation*}
    \begin{itemize}
        \item Convention: $V^{\otimes 0}=F$.
    \end{itemize}
    \item Analogous construction of the tensor product for generalized algebras and modules.
\end{itemize}


\subsection*{Section B.2: Exterior and Symmetric Powers}
\begin{itemize}
    \item \textbf{Alternating} (multilinear map): A multilinear map $\beta$ such that $\beta(v_1,\dots,v_n)=0$ whenever $v_i=v_j$ for some $i,j\in[n]$.
    \begin{itemize}
        \item Implication: $\beta(v_1,\dots,v_n)$ changes sign whenever two of the vectors are interchanged.
        \begin{itemize}
            \item Follows from the definition and the \textbf{standard polarization}.
        \end{itemize}
        \item Implication:
        \begin{equation*}
            \beta(v_{\sigma(1)},\dots,v_{\sigma(n)}) = (-1)^\sigma\beta(v_1,\dots,v_n)
        \end{equation*}
        for all $\sigma\in S_n$.
    \end{itemize}
    \item \textbf{Standard polarization}: The equality
    \begin{equation*}
        \beta(v,w)+\beta(w,v) = \beta(v+w,v+w)-\beta(v,v)-\beta(w,w)
        = 0-0-0
        = 0
    \end{equation*}
    \item \textbf{Exterior powers} (of $V$): The vector space $U$ equipped with an alternating multilinear map $V\times\cdots\times V\to\Lambda^nV$ sending $v_1\times\cdots\times v_n\mapsto v_1\wedge\cdots\wedge v_n$ that is universal, i.e., for any alternating multilinear map $\beta:V^n\to Z$, there is a unique linear map from $U$ to $Z$ that takes $v_1\wedge\cdots\wedge v_n\mapsto\beta(v_1,\dots,v_n)$. \emph{Denoted by} $\bm{\Lambda^nV}$.
    \begin{itemize}
        \item Convention: $\Lambda^0V=F$.
    \end{itemize}
    \item Quotient space construction of the exterior powers.
    \item Projecting from $V^{\otimes n}\to\Lambda^nV$: Define $\pi:V^{\otimes n}\to\Lambda^nV$ by
    \begin{equation*}
        \pi(v_1\otimes\cdots\otimes v_n) = v_1\wedge\cdots\wedge v_n
    \end{equation*}
    \item Basis for the exterior powers.
    \item There is a canonical linear map $\Lambda^aV\otimes\Lambda^bW\to\Lambda^{a+b}(V\oplus W)$, which takes $(v_1\wedge\cdots\wedge v_a)\otimes(w_1\wedge\cdots\wedge w_b)\mapsto v_1\wedge\cdots\wedge v_a\wedge w_1\wedge\cdots\wedge w_b$.
    \begin{itemize}
        \item This determines (how??) an isomorphism
        \begin{equation*}
            \Lambda^n(V\oplus W) \cong \bigoplus_{a=0}^n\Lambda^aV\otimes\Lambda^{n-a}W
        \end{equation*}
        \item This isomorphism plus induction on $n$ can justify (how??) the basis for $\Lambda^nV$ as the increasing indices.
    \end{itemize}
    \item \textbf{Symmetric} (multilinear map): A multilinear map $\beta$ such that $\beta(v_1,\dots,v_n)$ is unchanged when any two factors are interchanged, that is
    \begin{equation*}
        \beta(v_{\sigma(1)},\dots,v_{\sigma(n)}) = \beta(v_1,\dots,v_n)
    \end{equation*}
    for all $\sigma\in S_n$.
    \item \textbf{Symmetric powers} (of $V$): The vector space $U$ equipped with a symmetric multilinear map $V\times\cdots\times V\to S^nV$ sending $v_1\times\cdots\times v_n\mapsto v_1\cdot\ldots\cdot v_n$ that is universal, i.e., for any symmetric multilinear map $\beta:V^n\to Z$, there is a unique linear map from $U$ to $Z$ that takes $v_1\cdot\ldots\cdot v_n\mapsto\beta(v_1,\dots,v_n)$. \emph{Denoted by} $\bm{S^nV}$.
    \begin{itemize}
        \item Convention: $S^0V=F$.
    \end{itemize}
    \item Quotient space construction of the symmetric powers.
    \begin{itemize}
        \item Quotient out all $v_1\otimes\cdots\otimes v_n-v_{\sigma(1)}\otimes\cdots\otimes v_{\sigma(n)}$, that is, those elements of $V^{\otimes n}$ in which $\sigma$ permutes two successive factors. How does this work??
    \end{itemize}
    \item Projecting from $V^{\otimes n}\to S^nV$: Define $\pi:V^{\otimes n}\to S^nV$ by
    \begin{equation*}
        \pi(v_1\otimes\cdots\otimes v_n) = v_1\cdot\ldots\cdot v_n
    \end{equation*}
    \item Basis for the symmetric powers.
    \begin{itemize}
        \item It follows from the basis construction that $S^nV$ can be regarded as the space of homogeneous polynomials of degree $n$ in the variable $e_i$, since each element is of the form $e_{i_1}\cdot\ldots\cdot e_{i_n}$ and we can add them.
    \end{itemize}
    \item Canonical isomorphism:
    \begin{equation*}
        S^n(V\oplus W) \cong \bigoplus_{a=0}^nS^aV\otimes S^{n-a}W
    \end{equation*}
    \item More on $\Lambda^nV,S^nV$ as subspaces of $V^{\otimes n}$.
    \begin{itemize}
        \item We inject $\iota:\Lambda^nV\to V^{\otimes n}$ with
        \begin{equation*}
            \iota(v_1\wedge\cdots\wedge v_n) = \sum_{\sigma\in S_n}(-1)^\sigma v_{\sigma(1)}\otimes\cdots\otimes v_{\sigma(n)}
        \end{equation*}
        \begin{itemize}
            \item This relates to Rudenko's note that $v_1\wedge v_2=v_1\otimes v_2-v_2\otimes v_2$!
            \item There are some more advanced notes on the implications of $\iota$; $[\iota\circ\pi/n!](V^{\otimes n})=V^{\otimes n}$ is brought up.
        \end{itemize}
        \item We inject $\iota:S^nV\to V^{\otimes n}$ with
        \begin{equation*}
            \iota(v_1\cdot\ldots\cdot v_n) = \sum_{\sigma\in S_n}v_{\sigma(1)}\otimes\cdots\otimes v_{\sigma(n)}
        \end{equation*}
        \begin{itemize}
            \item More, related advanced notes; includes the $1/n!$ thing again.
        \end{itemize}
    \end{itemize}
    \item \textbf{Wedge product}: The function $\Lambda^mV\otimes\Lambda^nV\to\Lambda^{m+n}V$ defined as follows. \emph{Denoted by} $\bm{\wedge}$. \emph{Given by}
    \begin{equation*}
        (v_1\wedge\cdots\wedge v_m)\otimes(v_{m+1}\wedge\cdots\wedge v_{m+n}) \mapsto v_1\wedge\cdots\wedge v_m\wedge v_{m+1}\wedge\cdots\wedge v_{m+n}
    \end{equation*}
    \item Properties of the wedge product.
    \begin{enumerate}
        \item \emph{Associativity}:
        \begin{equation*}
            (v_1\wedge v_2)\wedge v_3 = v_1\wedge(v_2\wedge v_3) = v_1\wedge v_2\wedge v_3
        \end{equation*}
        \item \emph{Skew-commutativity}:
        \begin{equation*}
            v_1\wedge v_2 = -v_2\wedge v_2
        \end{equation*}
    \end{enumerate}
    \item Note that both of the above properties hold in higher-dimensional cases as well.
    \item Commutativity of the products.
    \begin{figure}[h!]
        \centering
        \DisableQuotes
        \begin{subfigure}[b]{0.35\linewidth}
            \centering
            \begin{tikzcd}
                \Lambda^mV\otimes\Lambda^nV \arrow[d,hook,"\iota\otimes\iota"'] \arrow[r,"\wedge"] & \Lambda^{m+n}V \arrow[d,hook,"\iota"]\\
                V^{\otimes m}\otimes V^{\otimes n} \arrow[r,"f_1"'] & V^{\otimes(m+n)}\\
            \end{tikzcd}
            \vspace{-1.5em}
            \caption{Wedge product.}
            \label{fig:CDwedgeSyma}
        \end{subfigure}
        \begin{subfigure}[b]{0.35\linewidth}
            \centering
            \begin{tikzcd}
                S^mV\otimes S^nV \arrow[d,hook,"\iota\otimes\iota"'] \arrow[r,"\cdot"] & S^{m+n}V \arrow[d,hook,"\iota"]\\
                V^{\otimes m}\otimes V^{\otimes n} \arrow[r,"f_2"'] & V^{\otimes(m+n)}\\
            \end{tikzcd}
            \vspace{-1.5em}
            \caption{Symmetric product.}
            \label{fig:CDwedgeSymb}
        \end{subfigure}
        \EnableQuotes
        \caption{Commutative diagram, wedge and symmetric products.}
        \label{fig:CDwedgeSym}
    \end{figure}
    \begin{itemize}
        \item $f_1$ is defined by
        \begin{equation*}
            (v_1\otimes\cdots\otimes v_m)\otimes(v_{m+1}\otimes\cdots\otimes v_{m+n}) \mapsto \sum_{\sigma\in G}(-1)^\sigma v_{\sigma(1)}\otimes\cdots\otimes v_{\sigma(m)}\otimes v_{\sigma(m+1)}\otimes\cdots\otimes v_{\sigma(m+n)}
        \end{equation*}
        where $G$ is the subgroup of $S_{m+n}$ preserving the order of the subsets $\{1,\dots,m\}$ and $\{m+1,\dots,m+n\}$.
        \item $f_2$ is defined analogously.
    \end{itemize}
    \item The above mappings all commute with linear maps of vector spaces.
    \begin{itemize}
        \item Example: Our definition $g(v\otimes w)=gv\otimes gw$ could be redrawn as $[g\circ\otimes](v,w)=[\otimes\circ g](v,w)$, where the latter $g:(v,w)\mapsto(gv,gw)$ by abuse of notation.
    \end{itemize}
    \item Tensor, exterior, and symmetric algebras.
\end{itemize}



\section{FH Chapter 1: Representations of Finite Groups}
\emph{From \textcite{bib:FultonHarris}.}
\begin{itemize}
    \item \marginnote{10/1:}Starts with a justification for beginning their investigation of rep theory with finite groups.
\end{itemize}


\subsection*{Section 1.1: Definitions}
\begin{itemize}
    \item Definition of a \textbf{representation}.
    \item $\rho$ "gives $V$ the structure of a $G$-module!" \parencite[3]{bib:FultonHarris}.
    \item When there is little ambiguity about $\rho$, we call $V$ itself a representation of $G$.
    \begin{itemize}
        \item This is what Rudenko has been doing in class!
    \end{itemize}
    \item We also often write $g\cdot v$ for $\rho(g)(v)$, and $g$ for $\rho(g)$.
    \item \textbf{Degree} (of $\rho$): The dimension of $V$.
    \item \textbf{$\bm{G}$-linear} (map): See class notes. \emph{Also known as} \textbf{map}, \textbf{morphism}.
    \item The \textbf{kernel}, \textbf{image}, and \textbf{cokernel} of $\varphi$ are all $G$-submodules.
    \item \textbf{Kernel} (of a map): The vector subspace containing all $v\in V$ for which $\varphi(v)=0$. \emph{Denoted by} $\bm{\ker\varphi}$.
    \item \textbf{Image} (of a map): The vector subspace containing all $w\in W$ for which there exists $v\in V$ such that $\varphi(v)=w$. \emph{Denoted by} $\bm{\im\varphi}$.
    \item \textbf{Cokernel} (of a map): The quotient space $W/\im\varphi$.
    \item Definitions of \textbf{subrepresentation}, \textbf{irreducible} representation, and \textbf{direct sum} of representations.
    \item \textbf{Tensor product} (of $V,W$): The representation with the space $V\otimes W$ where $g(v\otimes w)=gv\otimes gw$.
    \begin{itemize}
        \item \marginnote{10/5:}The $n^\text{th}$ tensor power is also a representation by this rule.
        \item The $n^\text{th}$ exterior and symmetric powers are subrepresentations of the $n^\text{th}$ tensor power.
    \end{itemize}
    \item \textbf{Natural pairing} (between $V^*,V$): The pairing defined as follows for all $v^*\in V^*$ and $v\in V$. \emph{Denoted by} $\bm{\langle\ ,\ \rangle}$. \emph{Given by}
    \begin{equation*}
        \langle v^*,v \rangle = v^*(v) = (v^*)^Tv
    \end{equation*}
    \item \textbf{Dual representation}: The representation from $G\to GL(V^*)$ defined as follows. \emph{Denoted by} $\bm{\rho^*}$. \emph{Given by}
    \begin{equation*}
        \rho^*(g) = \rho(g^{-1})^T
    \end{equation*}
    \begin{itemize}
        \item We should --- and do --- have
        \begin{equation*}
            \langle \rho^*(g)(v^*),\rho(g)(v) \rangle = \langle v^*,v \rangle
        \end{equation*}
        \item Indeed,
        \begin{align*}
            \langle \rho^*(g)(v^*),\rho(g)(v) \rangle &= \rho^*(g)(v^*)^T\rho(g)v\\
            &= [\rho(g^{-1})^T(v^*)]^T\rho(g)v\\
            &= (v^*)^T\rho(g^{-1})T\rho(g)v\\
            &= (v^*)^Tv\\
            &= \langle v^*,v \rangle
        \end{align*}
    \end{itemize}
    \item $\Hom(V,W)$ is a representation.
    \begin{itemize}
        \item Definition via the commutative diagram (from class): $g(L)v=[g\circ L\circ g^{-1}]v$.
        \item Definition via the isomorphic space $V^*\otimes W$ and the dual representation:
        \begin{align*}
            g(v^*\otimes w) &= gv^*\otimes gw\\
            &= (g^{-1})^Tv^*\otimes gw\\
            &= [(g^{-1})^Tv^*](gw)\\
            &= [(g^{-1})^Tv^*]^Tgw\\
            &= (v^*)^Tg^{-1}gw\\
            &= (v^*)^Tw\\
            &= v^*(w)
        \end{align*}
    \end{itemize}
    \item The rules for normal vector spaces hold for representations as well, e.g.,
    \begin{align*}
        V\otimes(U\oplus W) &= (V\otimes U)\oplus(V\otimes W)&
        \Lambda^k(V\oplus W) &= \bigoplus_{a+b=k}\Lambda^aV\oplus\Lambda^bW&
        \Lambda^k(V^*) &= \Lambda^k(V)^*
    \end{align*}
    \item Definition of \textbf{permutation representation} and \textbf{regular representation}.
\end{itemize}


\subsection*{Section 1.2: Complete Reducibility, Schur's Lemma}
\begin{itemize}
    \item \textbf{Indecomposable} (representation): See class notes. \emph{Also known as} \textbf{irreducible}.
    \item Proof of Theorem \ref{trm:complements} as in \textcite{bib:Serre}.
    \begin{itemize}
        \item The method is called "integration over the group (with respect to an invariant measure on the group)" \parencite[6]{bib:FultonHarris}.
    \end{itemize}
    \item \textbf{Complete reducibility}: The property that any representation is a direct sum of irreducible representations. \emph{Also known as} \textbf{semisimplicity}.
    \begin{itemize}
        \item Stated here as a corollary; proven as Theorem \ref{trm:dirSumIrrep} in \textcite{bib:Serre}.
    \end{itemize}
    \item The following lemma has several consequences, among which is that it determines how much a representation's direct-sum decomposition is unique.
    \setcounter{FHchapter}{1}
    \setcounter{FHlemma}{6}
    \begin{FHlemma}[Schur's Lemma]\label{lem:Schur}
        If $V$ and $W$ are irreducible representations of $G$ and $\varphi:V\to W$ is a $G$-module homomorphism, then\dots
        \begin{enumerate}
            \item Either $\varphi$ is an isomorphism, or $\varphi=0$;
            \item If $V=W$, then $\varphi=\lambda I$ for some $\lambda\in\C$, $I$ being the identity.
        \end{enumerate}
        \begin{proof}
            Suppose for the sake of contradiction that $\varphi$ is neither an isomorphism nor zero. Then it has a nontrivial kernel and image, both of which are necessarily invariant under the representation\footnote{See the proof of Schur's Lemma in \textcite{bib:Serre} for an explanation of this fact.}. Therefore, neither $V$ nor $W$ are irreducible representations of $G$, a contradiction.\par
            Since $\C$ is algebraically closed, $\varphi$ must have an eigenvalue $\lambda$. Equivalently, for some $\lambda\in\C$, $\varphi-\lambda I$ has nonzero kernel. But then by part (1), we must have $\varphi-\lambda I=0$, implying that $\varphi=\lambda I$, as desired.
        \end{proof}
    \end{FHlemma}
    \item Direct sum irreducible decomposition.
    \begin{FHproposition}\label{prp:completeReducibility}
        For any representation $V$ of a finite group $G$, there is a decomposition
        \begin{equation*}
            V = V_1^{\oplus a_1}\oplus\cdots\oplus V_k^{\oplus a_k}
        \end{equation*}
        where the $V_i$ are distinct irreducible representations. The decomposition of $V$ into a direct sum of the $k$ factors is unique, as are the $V_i$ that occur and their multiplicities $a_i$.
        \begin{proof}
            Let $W$ be another representation of $G$, possibly of different dimension. Let $\varphi:V\to W$ be a map of representations. Restrict $\varphi$ to $V_i^{\oplus a_i}$, a subrepresentation of $V$. It follows from \hyperref[lem:Schur]{Schur's Lemma} that this restriction either maps into the $W_j^{\oplus b_j}$ satisfying $W_j\cong V_i$, or it does not map it at all.\par
            Uniqueness for the decomposition of $V$ follows by applying \hyperref[lem:Schur]{Schur's Lemma} to the identity map on $V$.
        \end{proof}
    \end{FHproposition}
    \item Goals going forward.
    \begin{enumerate}
        \item Describe all the irreducible representations of $G$.
        \begin{itemize}
            \item We can find all \emph{irreducible} representations of $G$, then describe \emph{any} representation as a linear combination of these.
        \end{itemize}
        \item Find techniques for giving the direct sum decomposition and the multiplicities of an arbitrary representation.
        \item \textbf{Plethysm}: Describe the decompositions, with multiplicities, of representations derived from a given representation $V$, such as $V\otimes V$, $V^*$, $\Lambda^kV$, $S^kV$, and $\Lambda^k(\Lambda^1V)$.
        \begin{itemize}
            \item Note: If $V$ decomposes into two representations, these representations decompose accordingly, e.g., if $V=U\oplus W$, we may invoke the earlier identity to learn that $\Lambda^kV=\oplus_{i+j=k}\Lambda^iU\otimes\Lambda^jW$.
            \item \textbf{Clebsch-Gordon problem}: Decompose $V\otimes W$, given two irreducible representations $V$ and $W$.
        \end{itemize}
    \end{enumerate}
\end{itemize}


\subsection*{Section 1.3: Examples --- Abelian Groups, $\bm{S_3}$}
\begin{itemize}
    \item Classifying the irreducible representations of abelian groups.
    \begin{itemize}
        \item Let $G$ be an arbitrary finite abelian group, and let $V$ be an irreducible representation of it.
        \item Observe that since $gh=hg$ for all $g,h\in G$, we have
        \begin{align*}
            \rho_V(gh) &= \rho_V(hg)\\
            \rho_V(g)\circ\rho_V(h) &= \rho_V(h)\circ\rho_V(g)
        \end{align*}
        \item Thus, each $\rho_V(g)$ is a morphism of $G$-representations.
        \item It follows by Schur's Lemma that each $\rho_V(g)=\lambda_gI$.
        \item Consequently, every subspace of $V$ is invariant under $\rho_V(g)$ for all $v\in V$. Therefore, $V$ must be one dimensional, hence isomorphic to $\C$.
    \end{itemize}
    \item Classifying the irreducible representations of $S_3$.
    \begin{itemize}
        \item There exist two one-dimensional representations of $S_3$ (and of every other nontrivial symmetric group).
        \begin{itemize}
            \item \textbf{Trivial representation} (irreducible).
            \item \textbf{Alternating representation} (irreducible).
        \end{itemize}
        \item Using the fact that $S_3$ is a permutation group, we can locate the\dots
        \begin{itemize}
            \item Permutation representation (reducible);
            \item \textbf{Standard represenatation} (irreducible).
        \end{itemize}
        \item Let $W$ be an arbitrary representation of $S_3$.
        \begin{itemize}
            \item Easily done with \textbf{character theory}, but we'll only get there later.
        \end{itemize}
        \item Since the representation theory of finite abelian groups was just proven to be very simple, we'll start by looking at the action of the finite abelian subgroup $A_3\cong\Z/3\Z\subset S_3$ on $W$.
        \begin{itemize}
            \item Let $\tau$ be a generator of $A_3$. Explicitly, this means $\tau=(1,2,3)$ or $\tau=(1,3,2)$.
            \item By complete reducibility (Proposition \ref{prp:completeReducibility}), $W$ decomposes into a direct sum of irreducible representations:
            \begin{equation*}
                W = \bigoplus V_i
            \end{equation*}
            \item Since $A_3$ is abelian, each $V_i$ is one-dimensional. Thus, we may let $V_i=\spn(v_i)$.
            \item As subrepresentations of $W$, each $V_i$ is stable under $\rho_W(\tau)$. In other words, each $v_i$ is an \emph{eigenvector} of $\rho_W(\tau)$. Thus, the eigenvectors of $\rho_W(\tau)$ span $W$!\footnote{Errata: This implication is very confusingly reversed in \textcite{bib:FultonHarris}.}
            \item What are the corresponding eigenvalues? Let $\omega$ denote the eigenvalue corresponding to $v_i$. Then
            \begin{align*}
                \omega^3v_i &= [\rho_W(\tau)]^3v_i = \rho_W(\tau^3)v_i = \rho_W(e)v_i = 1v_i\\
                \omega^3 &= 1\\
                \omega &= \left( \e[2\pi i/3] \right)^j
            \end{align*}
            where $j=0,1,2$.
        \end{itemize}
        \item Here are two example representations of $A_3$.
        \begin{enumerate}
            \item The permutational representation of $A_3$ is given by
            \begin{align*}
                \rho_{\C^3}(e) &=
                \begin{bmatrix}
                    1 & 0 & 0\\
                    0 & 1 & 0\\
                    0 & 0 & 1\\
                \end{bmatrix}&
                \rho_{\C^3}(\tau) &=
                \begin{bmatrix}
                    0 & 0 & 1\\
                    1 & 0 & 0\\
                    0 & 1 & 0\\
                \end{bmatrix}&
                \rho_{\C^3}(\tau^2) &=
                \begin{bmatrix}
                    0 & 1 & 0\\
                    0 & 0 & 1\\
                    1 & 0 & 0\\
                \end{bmatrix}
            \end{align*}
            \begin{itemize}
                \item Note here that the eigenvalues and corresponding eigenvectors of $\rho_{\C^3}(\tau)$ are
                \begin{align*}
                    \lambda_1 &= 1&
                        \lambda_2 &= \e[2\pi i/3]&
                            \lambda_3 &= \e[4\pi i/3]\\
                    v_1 &=
                    \begin{bmatrix}
                        1\\
                        1\\
                        1\\
                    \end{bmatrix}&
                        v_2 &=
                        \begin{bmatrix}
                            \e[2\pi i/3]\\
                            1\\
                            \e[4\pi i/3]\\
                        \end{bmatrix}&
                            v_3 &=
                            \begin{bmatrix}
                                1\\
                                \e[2\pi i/3]\\
                                \e[4\pi i/3]\\
                            \end{bmatrix}
                \end{align*}
            \end{itemize}
            \item Separately, we have
            \begin{align*}
                \rho_{\C^n}(e) &=
                \begin{bmatrix}
                    1 & \cdots & 0\\
                    \vdots & \ddots & \vdots\\
                    0 & \cdots & 1\\
                \end{bmatrix}&
                \rho_{\C^n}(\tau) &=
                \begin{bmatrix}
                    \e[2\pi i/3] & \cdots & 0\\
                    \vdots & \ddots & \vdots\\
                    0 & \cdots & \e[2\pi i/3]\\
                \end{bmatrix}&
                \rho_{\C^n}(\tau^2) &=
                \begin{bmatrix}
                    \e[4\pi i/3] & \cdots & 0\\
                    \vdots & \ddots & \vdots\\
                    0 & \cdots & \e[4\pi i/3]\\
                \end{bmatrix}
            \end{align*}
            \begin{itemize}
                \item Here, every vector is an eigenvector and there are infinitely many decompositions into direct sums of the one-dimensional representations.
            \end{itemize}
        \end{enumerate}
        \item Now we want to see how the remainder of $S_3$ acts on $W$.
        \begin{itemize}
            \item Let $\sigma$ be an arbitrary transposition in $S_3$.
            \item Note: $\{\sigma,\tau\}$ generates $S_3$.
            \item Recall the relationship $\sigma\tau\sigma=\tau^2$.
            \item The action of $\sigma$ on the eigenvectors of $\tau$: Let $v$ be an arbitrary eigenvector of $\tau$, with corresponding eigenvalue $\omega^j$. Notice that
            \begin{equation*}
                \tau(\sigma(v)) = \sigma(\tau^2(v))
                = \sigma(\omega^{2j}v)
                = \omega^{2j}\sigma(v)
            \end{equation*}
            \item Takeaway: $v$ an eigenvector for $\tau$ with eigenvalue $\omega^j$ implies $\sigma(v)$ an eigenvector for $\tau$ with eigenvalue $\omega^{2j}$.
        \end{itemize}
        \item Exercise 1.10 (A basis for the standard representation of $S_3$): Verify that with $\sigma=(12)$ and $\tau=(123)$, the standard representation has a basis $\alpha=(\omega,1,\omega^2),\beta=(1,\omega,\omega^2)$, with
        \begin{align*}
            \tau\alpha &= \omega\alpha&
            \tau\beta &= \omega^2\beta&
            \sigma\alpha &= \beta&
            \sigma\beta &= \alpha
        \end{align*}
        \begin{itemize}
            \item $1+\omega+\omega^2=0$ in the complex plane.
            \item We do, indeed, get
            \begin{align*}
                \tau\alpha &= (\omega^2,\omega,1)&
                    \tau\beta &= (\omega^2,1,\omega)&
                        \sigma\alpha &= (1,\omega,\omega^2)&
                            \sigma\beta &= (\omega,1,\omega^2)\\
                &= \omega(\omega,1,\omega^2)&
                    &= \omega^2(1,\omega,\omega^2)&
                        &= \beta&
                            &= \alpha\\
                &= \omega\alpha&
                    &= \omega^2\beta
            \end{align*}
            \item We also get --- per the aforementioned rule --- that
            \begin{equation*}
                \tau\alpha = \omega\alpha
            \end{equation*}
            but
            \begin{equation*}
                \tau(\sigma\alpha) = \tau\beta = \omega^2\beta = \omega^2(\sigma\alpha)
            \end{equation*}
            for instance, as predicted.
            \item Note that both $\alpha,\beta$ are orthogonal to $(1,1,1)$, but while they are linearly independent, they are not orthogonal to each other. This is fine, because they're computationally simple, but it is noteworthy.
            \item Also note that we derived $\alpha,\beta$ as eigenvalues $v_2,v_3$ of the permutational representation just above!
        \end{itemize}
        \item If the eigenvalue $\omega$ of $v$ is not equal to 1, then the eigenvalue $\omega^2$ of $\sigma(v)$ is not equal to 1.
        \item Thus, if the eigenvalue of $v$ is not 1, then $v$ and $\sigma(v)$ are linearly independent (two linearly dependent eigenvectors necessarily have the same eigenvalue).
        \begin{itemize}
            \item Indeed, in this case, $v$ and $\sigma(v)$ span a 2D subspace $V'$ that is invariant under $S_3$!! This is because $v=\sigma(\sigma(v))$ as well.
            \item In fact, $V'$ is isomorphic to the standard representation!
            \item Note that $V'$ does not decompose further because any eigenvectors of $\tau$ are not eigenvectors of $\sigma$ and vice versa (see below computation of the eigenvectors of $\sigma$).
        \end{itemize}
        \item What if the eigenvalue of $v$ (under $\tau$) is 1?
        \begin{itemize}
            \item If $\sigma(v)$ is not linearly independent of $v$, then the two span a 1D subrepresentation of $W$, isomorphic to the trivial representation (if $\sigma(v)=v$) and isomorphic to the alternating representation (if $\sigma(v)=-v$).
            \begin{itemize}
                \item How we know that only the trivial and alternating representations are possible: If $\sigma(v)\in\spn(v)$, then $v$ is an eigenvector of $\sigma$. Thus, $\sigma(v)=\lambda v$. Since $\lambda^2v=\sigma^2(v)=e(v)=v$, we know that $\lambda^2=1$, so $\lambda=\pm 1$.
            \end{itemize}
            \item If $\sigma(v)$ is linearly independent of $v$, then $v+\sigma(v)$ spans a 1D subrepresentation of $W$ isomorphic to the trivial representation and $v-\sigma(v)$ spans a 1D subrepresentation of $W$ isomorphic to the alternating representation.
            \begin{itemize}
                \item If the eigenvalue of $v$ under $\tau$ is 1, then the eigenvalue of $\sigma(v)$ under $\tau$ is $1^2=1$. Thus, $\tau$ is the identity map on $\spn[v,\sigma(v)]$.
                \item On the other hand, in the basis $\{v,\sigma(v)\}$, we can represent $\sigma$ with the matrix
                \begin{equation*}
                    \begin{bmatrix}
                        0 & 1\\
                        1 & 0\\
                    \end{bmatrix}
                \end{equation*}
                \item This matrix has eigenvalues and corresponding eigenvectors
                \begin{align*}
                    \lambda_1 &= 1&
                        \lambda_2 &= -1\\
                    v_1 &=
                    \begin{bmatrix}
                        1\\
                        1\\
                    \end{bmatrix}
                    = v+\sigma(v)&
                        v_2 &=
                        \begin{bmatrix}
                            1\\
                            -1\\
                        \end{bmatrix}
                        = v-\sigma(v)
                \end{align*}
                \item Thus, we see that the 2D subrepresentation spanned by $v,\sigma(v)$ decomposes into a trivial ($\lambda_1=1$, $v_1=v+\sigma(v)$) and alternating ($\lambda_2=-1$, $v_2=v-\sigma(v)$) irreducible representation.
            \end{itemize}
        \end{itemize}
        \item It follows that the only three irreps of $S_3$ are the trivial, alternating, and standard ones.
    \end{itemize}
    \item Using the above approach to find the decomposition of the tensor product.
    \begin{itemize}
        \item Let $V$ be the standard two-dimensional representation. Recall that the basis of $V$ is $\{\alpha,\beta\}$.
        \item It follows that the basis of $V\otimes V$ is $\{\alpha\otimes\alpha,\alpha\otimes\beta,\beta\otimes\alpha,\beta\otimes\beta\}$.
        \item These are eigenvectors for $\tau$, and we can find their corresponding eigenvalues via direct computation:
        \begin{align*}
            \tau(\alpha\otimes\alpha) &= \tau\alpha\otimes\tau\alpha&
                \tau(\alpha\otimes\beta) &= \tau\alpha\otimes\tau\beta\\
            &= (\omega\alpha)\otimes(\omega\alpha)&
                &= (\omega\alpha)\otimes(\omega^2\beta)\\
            &= \omega^2\alpha\otimes\alpha&
                &= 1\alpha\otimes\beta
        \end{align*}
        \begin{align*}
            \tau(\beta\otimes\alpha) &= \tau\beta\otimes\tau\alpha&
                \tau(\beta\otimes\beta) &= \tau\beta\otimes\tau\beta\\
            &= (\omega^2\beta)\otimes(\omega\alpha)&
                &= (\omega^2\beta)\otimes(\omega^2\beta)\\
            &= 1\beta\otimes\alpha&
                &= \omega\beta\otimes\beta
        \end{align*}
        \item Similarly, we can calculate the effect of $\sigma$.
        \begin{align*}
            \sigma(\alpha\otimes\alpha) &= \sigma\alpha\otimes\sigma\alpha&
                \sigma(\alpha\otimes\beta) &= \sigma\alpha\otimes\sigma\beta&
                    \sigma(\beta\otimes\alpha) &= \sigma\beta\otimes\sigma\alpha&
                        \sigma(\beta\otimes\beta) &= \sigma\beta\otimes\sigma\beta\\
            &= \beta\otimes\beta&
                &= \beta\otimes\alpha&
                    &= \alpha\otimes\beta&
                        &= \alpha\otimes\alpha
        \end{align*}
        \item Because the transformations for $\alpha\otimes\alpha$ and $\beta\otimes\beta$ are directly analogous to the untensored case of $\alpha$ and $\beta$, these basis vectors span a subrepresentation isomorphic to the standard representation.
        \item Because $\sigma(\alpha\otimes\beta)=\beta\otimes\alpha$ is linearly independent of $\alpha\otimes\beta$ (they are literally different basis vectors), $\alpha\otimes\beta+\beta\otimes\alpha$ spans a trivial representation and $\alpha\otimes\beta-\beta\otimes\alpha$ spans an alternating representation.
        \item Altogether, we get that if $V=(2,1)$, then
        \begin{equation*}
            V\otimes V \cong (2,1)\oplus(3)\oplus(1,1,1)
        \end{equation*}
    \end{itemize}
\end{itemize}




\end{document}