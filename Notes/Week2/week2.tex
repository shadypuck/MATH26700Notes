\documentclass[../notes.tex]{subfiles}

\pagestyle{main}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter\ (#1)}{}}
\stepcounter{chapter}

\begin{document}




\chapter{The Structure of Representations}
\section{The Tensor Product}
\begin{itemize}
    \item \marginnote{10/2:}Before lecture, I chatted with a few people about tensor products and the exterior and symmetric powers.
    \begin{itemize}
        \item Patrick: A \textbf{tensor} $v\otimes w$ is just an element of a vector space, indexed differently than in a column.
        \item Raman: There is no canonical way to transform tensors into column vectors.
    \end{itemize}
    \item Course logistics.
    \begin{itemize}
        \item OH: T 5:30-6:30(+) and W 5:30-6:30(+). We can also meet one-on-one.
        \item HW is due Thursdays at midnight.
    \end{itemize}
    \item Today: Constructing new representations from old.
    \begin{itemize}
        \item Rudenko will skim through tensor products really quickly.
    \end{itemize}
    \item Reminder: Last time, we talked about how representation theory is really quite simple. If $G$ is a finite group and $F=\C$, there exist a finite set $V_1,\dots,V_s$ of irreps up to isomorphism, and every finite-dimensional representation $V\cong V_1^{n_1}\oplus\cdots\oplus V_s^{n_s}$.
    \item If $V$ is a representation of $G$, then there are loads of things we can do with it.
    \begin{itemize}
        \item We can construct the dual representation $V^*$.
        \item We can construct the representation $V\otimes V$.
        \item We can construct symmetric powers.
        \item We can construct wedge powers.
        \item There are more, but this is enough for now.
    \end{itemize}
    \item Even when we take a very simple group and representation, there are some very interesting things that can fall out.
    \begin{itemize}
        \item Example: If you take the symmetric powers of $S_3$, as in the homework, you get something really interesting.
    \end{itemize}
    \item Now, we go to linear algebra.
    \item Let $V,W$ be vector spaces over a field $F$. How do we produce a new vector space out of these?
    \item $\Hom_F(V,W)$ is the vector space of linear maps $F:V\to W$!
    \begin{itemize}
        \item $\dim=(\dim V)(\dim W)$.
    \end{itemize}
    \item Can we make $\Hom_F(V,W)$ into a representation of $G$? Yes!
    \begin{figure}[H]
        \centering
        \DisableQuotes
        \begin{tikzcd}
            V \arrow[d,"\rho_V^{}(g)"'] \arrow[r,"L"] & W \arrow[d,"\rho_W^{}(g)"]\\
            V \arrow[r,"gL"'] & W\\
        \end{tikzcd}
        \EnableQuotes
        \vspace{-1.5em}
        \caption{Commutative diagram, linear maps space representation.}
        \label{fig:CDlinMapsRep}
    \end{figure}
    \begin{itemize}
        \item Suppose that $V,W$ are $G$-reps, which gives us $\rho_V:G\to GL(V)$ and $\rho_W:G\to GL(W)$.
        \item Suppose also that we have $L\in\Hom_F(V,W)$.
        \item Now infer from the commutative diagram that it will work to define $gL=\rho_W(g)\circ L\circ\rho_V(g)^{-1}$.
        \item This is pretty standard.
    \end{itemize}
    \item Recall that there is a different space $\Hom_G(V,W)$ of morphisms of $G$-representations (see Figure \ref{fig:CDmorphisms} and the associated discussion).
    \begin{itemize}
        \item This is a very very small subspace of $\Hom_F(V,W)$.
    \end{itemize}
    \item Special case of the above construction: \textbf{Dual representation}.
    \begin{itemize}
        \item Consider $\Hom_F(V,F)$. This the \textbf{dual vector space}.
        \item Basic fact 1: Let $e_1,\dots,e_n$ be a basis of $V$. Then $V^*$ also has a corresponding basis $e^1,\dots,e^n$, known as its \textbf{dual basis}.
        \begin{itemize}
            \item Computing coordinates already depends on a basis, and having bases is super nice.
            \item Corollary: $\dim V=\dim V^*$.
            \item This is the first time \textbf{canonical} comes into linear algebra. Canonical (nobody understands what it means) basically means that something doesn't depend on choices.
            \item In particular, $V,V^*$ are isomorphic because they have the same dimension, but for no more natural reason. They can be the same representation, or they can be different.
        \end{itemize}
        \item Basic fact 2: If $V$ is finite-dimensional, then $(V^*)^*\cong V$. The formula for this isomorphism is canonical, because it does not depend on a choice of basis. In particular, choose the map $V\to(V^*)^*$ sending $v$ to the map sending $\varphi\in V^*$ to $\varphi(v)$.
        \item If $V$ is infinite dimensional, none of this is true and you are in the realm of functional analysis.
        \item Ok, so all of this was good information about the dual \emph{space}, but what is the dual \emph{representation}?? Does it matter, and do we need to know for now?
    \end{itemize}
    \item \textbf{Dual vector space} (of $V$): The vector space defined as follows, given that $V$ is a vector space over $F$. \emph{Denoted by} $\bm{V^*}$. \emph{Given by}
    \begin{equation*}
        V^* = \Hom_F(V,F)
    \end{equation*}
    \item \textbf{Dual basis} (of $V^*$ to $e_1,\dots,e_n$): The basis defined as follows for $i=1,\dots,n$, where $e_1,\dots,e_n$ is a basis of $V$. \emph{Denoted by} $\bm{e^1,\ldots,e^n}$. \emph{Given by}
    \begin{equation*}
        e^i(x_1e_1+\cdots+x_ne_n) = x_i
    \end{equation*}
    \item We now move onto the tensor product.
    \begin{itemize}
        \item The tensor product is very hard to understand. If you learn about it and you feel you don't understand it, that's typical; nobody understands it at first.
        \item For now, we'll discuss two ways of thinking about tensor products that won't bring us any comfort.
    \end{itemize}
    \item Let $V,W$ be two vector spaces over a field $F$.
    \item Abstract definition of the tensor product.
    \begin{itemize}
        \item We have discussed maps from $V\to W$, but there is another related space.
        \item Indeed, we can look at the space of bilinear maps from $V\times W\to F$.
        \begin{itemize}
            \item Example: A map $f:V\times W\to F$ that satisfies the constraints $f(\lambda v,w)=\lambda f(v,w)$, $f(v_1+v_2,w)=f(v_1,w)+f(v_2,w)$, and likewise for the second index. Recall that this is a \textbf{bilinear map}.
        \end{itemize}
        \item Let $V$ have basis $e_1,\dots,e_n$ and $W$ have basis $f_1,\dots,f_m$.
        \item Notice that every bilinear map $f$ can be defined as a linear combination of the $f(e_i,f_j)$. In other words, the $f(e_i,f_j)$ form the basis of a function space.
        \begin{itemize}
            \item This "bilinear maps space" has dimension $nm$.
        \end{itemize}
        \item Now, one way to understand a tensor product: Is this "bilinear maps space" actually some other space? It is! It is $(V\otimes W)^*$.
        \item Bilinear maps are linear maps from where? From $V\otimes W$!
    \end{itemize}
    \item \textbf{Bilinear} (map): A function $f:V\times W\to Z$ that satisfies the following constraints, where $V,W,Z$ are vector spaces over $F$, $v,v_1,v_2\in V$, $w,w_1,w_2\in W$, and $\lambda\in F$. \emph{Constraints}
    \begin{align*}
        f(v_1+v_2,w) &= f(v_1,w)+f(v_2,w)&
            f(\lambda v,w) &= \lambda f(v,w)\\
        f(v,w_1+w_2) &= f(v,w_1)+f(v,w_2)&
            f(v,\lambda w) &= \lambda f(v,w)
    \end{align*}
    \item We now look at a much more elementary definition of the tensor product.
    \item Explicit definition of the tensor product.
    \begin{itemize}
        \item $V\otimes W$ is equal to a huge vector space with basis consisting of pairs of elements $(v,w)$. Even if $V,W$ are one dimensional, this is like all pairs of real numbers; it's huge. Then, we quotient it by the space of all elements satisfying $\lambda(v,w)=(\lambda v,w)=(v,\lambda w)$, $(v_1+v_2,w)=(v_1,w)+(v_2,w)$, and the like. This forces these relationships to be true.
        \begin{itemize}
            \item Clarify this methodology??
            \item Essentially, this allows us to treat tensor multiplication much like real multiplication, endowing the operation with distributivity, etc.
            \item For example, the rule $(v_1+v_2,w)=(v_1,w)+(v_2,w)$ becomes, in tensor product notation, $(v_1+v_2)\otimes w=v_1\otimes w+v_2\otimes w$.
        \end{itemize}
        \item Example: Suppose $V=\C e_1+\C e_2$. We want to look at $V\otimes V$.
        \begin{itemize}
            \item A priori\footnote{I.e., it follows from some logic. In particular, it follows from the logic that any element $v\in V$ is of the form $v=ae_1+be_2$, so of course all $v\otimes v$ must be of the given form for choices of $a,b,c,d$.}, it's spanned by $(ae_1+be_2)\otimes(ce_1+de_2)=ace_1\otimes e_1+ade_1\otimes e_2+bce_2\otimes e_1+cde_2\otimes e_2$.
            \item Thus, $V_1\otimes V_2$ has 4-element basis $e_1\otimes e_1,e_1\otimes e_2,e_2\otimes e_1,e_2\otimes e_2$.
        \end{itemize}
    \end{itemize}
    \item These two definitions constitute a first approximation to what the tensor product is.
    \item Takeaway: What is true in general is that if $V$ has basis $e_1,\dots,e_n$ and $W$ has basis $f_1,\dots,f_m$, then $V\otimes W$ has basis $e_i\otimes f_j$ ($i=1,\dots,n$ and $j=1,\dots,m$).
    \item Having discussed the tensor product of vector spaces, let's think about the tensor product of \emph{representations}.
    \begin{itemize}
        \item Suppose $g:V\to V$ and $g:W\to W$.
        \begin{itemize}
            \item We're starting to make notation sloppy.
        \end{itemize}
        \item How does $g:V\otimes W\to V\otimes W$? Well, we just send $v\otimes w\mapsto(gv)\otimes(gw)$.
        \begin{itemize}
            \item Why is this map well-defined?
            \item We invoke the \textbf{universal property of the tensor product operation}.
            \item This guarantees us that given $g$ --- which is effectively a map from $V\times W\to V\otimes W$ as defined --- there nevertheless exists a complete extension $\tilde{g}:V\otimes W\to V\otimes W$.
        \end{itemize}
        \item As a matrix, this map is pretty strange!
        \begin{itemize}
            \item Example: Let $g:V\to V$ be a $2\times 2$ matrix. What is the matrix of $g:V\otimes V\to V\otimes V$?
            \item If
            \begin{equation*}
                \rho_V(g) = g =
                \begin{pmatrix}
                    a & b\\
                    c & d\\
                \end{pmatrix}
                =: A
            \end{equation*}
            then we have
            \begin{align*}
                g(e_1\otimes e_1) &= ge_1\otimes ge_1\\
                &= (ae_1+ce_2)\otimes(ae_1+ce_2)\\
                &= a^2e_1\otimes e_1+ace_1\otimes e_2+ace_2\otimes e_1+c^2e_2\otimes e_2
            \end{align*}
            \item Evaluating similarly for all basis vectors, we get a very curious block matrix:
            \begin{equation*}
                \begin{bNiceArray}{cccc}[first-row,first-col]
                     & e_1\otimes e_1 & e_1\otimes e_2 & e_2\otimes e_1 & e_2\otimes e_2\\
                    e_1\otimes e_1 & a^2 & ab & ab & b^2\\
                    e_1\otimes e_2 & ac  & ad & bc & bd\\
                    e_2\otimes e_1 & ac  & bc & ad & bd\\
                    e_2\otimes e_2 & c^2 & cd & cd & d^2\\
                \end{bNiceArray}
                =
                % \begin{bNiceArray}{c|c}[margin]
                %     aA & bA\\
                %     \hline
                %     cA & dA\\
                % \end{bNiceArray}
                \begin{bNiceArray}{cc|cc}[margin]
                    \Block{2-2}{aA} & & \Block{2-2}{bA} & \\
                     & & & \\
                    \hline
                    \Block{2-2}{cA} & & \Block{2-2}{dA} & \\
                     & & & \\
                \end{bNiceArray}
            \end{equation*}
            \item Notice how, for example, this takes the tensor $e_1\otimes e_1$, represented as $(1,0,0,0)$, to the tensor $a^2e_1\otimes e_1+ace_1\otimes e_2+ace_2\otimes e_1+c^2e_2\otimes e_2$, represented as $(a^2,ac,ac,c^2)$.
            \item Does this construction imply a canonical way to convert from tensors to column vectors??
        \end{itemize}
        \item Classically, this is called the \textbf{Kronecker product} of two matrices.
        \item People discovered all of this stuff before they unified it as tensor math.
    \end{itemize}
    \item \textbf{Universal property of the tensor product operation}: For every bilinear map $h:V\times W\to Z$, there exists a \emph{unique} linear map $\tilde{h}:V\otimes W\to Z$ such that $h=\tilde{h}\circ\otimes$.
    \begin{figure}[h!]
        \centering
        \DisableQuotes
        \begin{tikzcd}
            V\times W \arrow[r,"\otimes"] \arrow[rd,"h"'] & V\otimes W \arrow[d,"\tilde{h}"]\\
             & Z\\
        \end{tikzcd}
        \EnableQuotes
        \vspace{-1.5em}
        \caption{Universal property, tensor product operation.}
        \label{fig:UPtensorProd}
    \end{figure}
    \begin{proof}
        See the solid explanation \href{https://math.stackexchange.com/a/4248460/340652}{linked here}. Alternatively, here's my write up.\par\smallskip
        Let $V=\C e_1\oplus\cdots\oplus\C e_n$, $W=\C f_1\oplus\cdots\oplus\C f_m$, $Z$, and $h:V\times W\to Z$ be arbitrary. Define $\tilde{h}:V\otimes W\to Z$ by
        \begin{equation*}
            \tilde{h}(e_i\otimes f_j) := h(e_i,f_j)
        \end{equation*}
        for $i=1,\dots,n$ and $j=1,\dots,m$. Since a linear map is wholly defined by its action on the basis of its domain, this set of equations suffices to define $\tilde{h}$ on all of $V\otimes W$.\par
        \underline{Existence}: To prove that $\tilde{h}$ satisfies the "universal property," it will suffice to show that $h=\tilde{h}\circ\otimes$. Let $(v,w)\in V\times W$ be arbitrary, and suppose $v=\sum_{i=1}^na_ie_i\in V$, and $w=\sum_{i=1}^nb_if_i\in W$. Then
        \begin{align*}
            [\tilde{h}\circ\otimes](v,w) &= \tilde{h}(v\otimes w)\\
            &= \sum_{i=1}^n\sum_{j=1}^ma_ib_i\tilde{h}(e_i\otimes f_i)\\
            &= \sum_{i=1}^n\sum_{j=1}^ma_ib_ih(e_i,f_j)\\
            &= h(v,w)
        \end{align*}
        as desired.\par
        \underline{Uniqueness}: Now suppose $\tilde{g}:V\otimes W\to Z$ also satisfies the "universal property," that is, $h=\tilde{g}\circ\otimes$. Then by definition,
        \begin{equation*}
            \tilde{h}(e_i\otimes f_j) = h(e_i,f_j) = \tilde{g}(e_i\otimes f_j)
        \end{equation*}
        for $i=1,\dots,n$ and $j=1,\dots,m$. But since a linear map is wholly defined by its action on the basis of its domain, it follows that $\tilde{h}=\tilde{g}$, as desired.
    \end{proof}
    \item \textbf{Kronecker product} (of $A,B$): The matrix product defined as follows. \emph{Denoted by} $\bm{A\otimes B}$. \emph{Given by}
    \begin{equation*}
        A\otimes B =\
        \begin{bNiceArray}{c}[first-row,first-col]
              & n\\
            n & A\\
        \end{bNiceArray}
        \otimes\
        \begin{bNiceArray}{c}[first-row,first-col]
              & m\\
            m & B\\
        \end{bNiceArray}
        =\
        \begin{bNiceArray}{ccc}[first-row,first-col]
               &         & nm     &        \\
               & a_{11}B & \cdots & a_{1n}B\\
            nm & \vdots  & \ddots & \vdots \\
               & a_{n1}B & \cdots & a_{nn}B\\
        \end{bNiceArray}
    \end{equation*}
    \begin{itemize}
        \item The Kronecker product is \emph{not} commutative, but the matrices you get are related by conjugacy and by commuting the columns.
    \end{itemize}
    \item Vector spaces of the same dimension are all alike, but vector space representations are very interesting. By the end of this course, we'll understand what's going on.
    \item How we understand tensor stuff: Look at the abstract definition, look at the concrete definition, look at 5 examples, and then go in a circle. Repeat again and again until it makes sense.
    \item He's just trying to tell us all relevant words so that they will fit together later.
    \item Fact: If $V,W$ finite-dimensional, $\Hom_F(V,W)\cong V\otimes W^*$.
    \begin{itemize}
        \item Tensor products are very nice to construct maps from.
        \item Let's construct a reverse map, then.
        \item Take $\alpha\otimes w\in V^*\otimes W$, where $\alpha:V\to F$ by definition. Send $\alpha\otimes w$ to the map $v\mapsto\alpha(v)w$. This is a \emph{canonical} map!! We can show that they span everything.
        \begin{itemize}
            \item For example, if we want to choose $\alpha\otimes w$ mapping to the matrix with a 1 in the upper left-hand corner and zeroes everywhere else, let $w=e_1\in W$ and let $\alpha=e^1\in V^*$.
            \item We can do similarly for all other such matrices, mapping this basis of $\Hom_F(V,W)$ to $e^i\otimes e_j$ ($i=1,\dots,n$ and $j=1,\dots,m$).
            \item Note that this also allows us to define a (noncanonical) inverse map.
        \end{itemize}
        \item This inverse map from $\Hom_F(V,W)\to V^*\otimes W$ is clearly a bit harder to work out.
        \item Hidden in this story is why trace is invariant under conjugation (see below discussion).
    \end{itemize}
    \item If we now take $\Hom_F(V,V)$, then this is isomorphic to $V^*\otimes V$. There is a very natural map from these isomorphic spaces to $F$ defined by the trace, and/or $\alpha\otimes v\mapsto\alpha(v)$. We can prove this. And this is canonical, as well. This is why the main property of the trace is that it's invariant under conjugation. This fact is hidden in the story very nicely.
    \item Tensor products are hard, it will be a pain, we will understand them very well, but it will not be nice for now.
    \item Symmetric products and wedge powers will be discussed briefly next time.
    \begin{itemize}
        \item There is a nice description in \textcite{bib:Serre} that we can use for the homework.
    \end{itemize}
    \item Extra homework: Please read about tensor products in whatever textbook you like, try some examples, and repeat.
\end{itemize}



\section{Office Hours (Rudenko)}
\begin{itemize}
    \item \marginnote{10/3:}Problem 2a:
    \begin{itemize}
        \item $\Lambda^2V$ is \emph{exterior powers}.
        \item The exact canonical isomorphism we need is briefly discussed on \textcite[473]{bib:FultonHarris}.
        \item I.e., we have to construct isomorphisms between the structures that don't rely on the choice of any basis. Recall the classic example of $V\cong V^{**}$, as explained in the well-written MSE post "basic difference between canonical isomorphism and isomorphims." Recall that the isomorphism from $V\to V^*$ defined by sending each element of the basis of $V$ to the corresponding element of the dual basis of $V^*$ is \emph{not} canonical because \emph{it involves choosing bases}. Definitions of canonical maps are available in MATH20510Notes, p. 2.
        \item From a quick look at this, it looks like the proof may be analogous to the classic middle-school algebra identity $(v+w)^2=v^2+vw+w^2$.
        \item The second exterior power $\Lambda^2V$ of a finite-dimensional vector space $V$ is the dual space of the vector space of alternating bilinear forms on $V$. Elements of $\Lambda^2V$ are called 2-vectors.
    \end{itemize}
    \item Problem 2b:
    \begin{itemize}
        \item $S^2V$ is \emph{symmetric powers}.
        \item The exact canonical isomorphism we need is briefly discussed on \textcite[473]{bib:FultonHarris}.
    \end{itemize}
    \item Problem 3a:
    \begin{itemize}
        \item This is the determinant of the multiplication table, in relation to that theorem that you showed us at the end of the first class? Yep!
    \end{itemize}
    \item Problem 3b:
    \begin{itemize}
        \item So a circulant matrix is a matrix like the multiplication table from (a)? Yep!
        \item Is $\zeta=e^{2\pi i/n}$? Sort of. It can be any $n^\text{th}$ root of unity.
    \end{itemize}
    \item Problem 4d:
    \begin{itemize}
        \item We'll cover higher symmetric powers in class tomorrow.
        \item However, it basically just means that we're now working with elements of the form $e_1\otimes e_2\otimes e_3\in S^3V$ and on and on.
    \end{itemize}
    \item Problem 5a:
    \begin{itemize}
        \item Is $V^\vee=V^*$? Yes. This is "vee check," and is a notation that some people prefer.
    \end{itemize}
    \item Problem 5b:
    \begin{itemize}
        \item Is "tr" the trace function of the linear map corresponding to $L$? Yes.
        \item What is $L$?
        \begin{itemize}
            \item An element of $V\otimes V^*$ is a linear combination of elements of the form $v\otimes\alpha$, not necessarily just one of these "decomposable" products.
            \item There is an isomorphism $V\otimes V^*\cong\Hom(V)$.
            \item Consider the matrix
            \begin{equation*}
                \begin{pmatrix}
                    1 & 0\\
                    0 & 0\\
                \end{pmatrix}
            \end{equation*}
            It sends $e_1\mapsto e_1$ and $e_2\mapsto 0$. Thus, it is well-matched with $e_1\otimes e^1$, which also grabs $e_1$ (with $e^1$) and sends it to $e_1$.
            \item Consider the matrix
            \begin{equation*}
                \begin{pmatrix}
                    0 & 1\\
                    0 & 0\\
                \end{pmatrix}
            \end{equation*}
            It sends $e_1\mapsto 0$ and $e_2\mapsto e_1$. Thus, it is well-matched with $e_1\otimes e^2$, which also grabs $e_2$ (with $e^2$) and sends it to $e_1$.
            \item In full,
            \begin{equation*}
                \begin{pmatrix}
                    a & b\\
                    c & d\\
                \end{pmatrix}
                \mapsto
                ae_1\otimes e^1+be_1\otimes e^2+ce_2\otimes e^1+de_2\otimes e^2
            \end{equation*}
            \item This map \emph{is} canonical! This is because the bases must be chosen to even begin talking about matrices.
            \item If you change the matrix, the bases change, too??
            \item Takeaway: We have to walk backwards from matrix to linear transformation to representation in $V\otimes V^*$ to a scalar in $F$.
        \end{itemize}
    \end{itemize}
    \item Problem 5c:
    \begin{itemize}
        \item So trace of such a map is equal to the dimension of its image? Yes.
    \end{itemize}
\end{itemize}



\section{Wedge and Symmetric Powers}
\begin{itemize}
    \item \marginnote{10/4:}OH slightly later today at 5:45-6:45 PM.
    \item Recap: Last time, we built new reps from old.
    \begin{itemize}
        \item This stuff can't be learned in 1.5 lectures; he can point us around, but we have to learn it ourselves.
    \end{itemize}
    \item Tensor product review.
    \begin{itemize}
        \item Given $V,W$, make $V\otimes_FW$.
        \item This vector space is hard to describe directly, so we more often talk about its dual $(V\otimes W)^*$ because this is actually easier to describe.
        \item If you want to work with $V\otimes W$ hands-on, you can do the following.
        \begin{itemize}
            \item Start with the following easy-to-work-with vector space: The (probably infinite-dimensional) vector space where each $v\otimes w$ is a basis vector for all $v\in V$ and $w\in W$.
            \item Then quotient it by relations to force them to hold in the final space.
        \end{itemize}
        \item Here's an example of this construction.
        \begin{itemize}
            \item Let $V=W$ be the one-dimensional vector space over the finite field $F_2=\Z/2\Z$.
            \item Thus, the elements of $V$ are $\{0,1\}$ (which is, literally, all linear combinations $a0+b1$ where $a,b\in F_2$ as well; this hearkens back to $V$'s definition as an $F_2$-module).
            \item Then the easy-to-work-with vector space we're talking about is the 4-dimensional \textbf{free} vector space $U=\spn(0\otimes 0,0\otimes 1,1\otimes 0,1\otimes 1)$.
            \item Note that in this space, for example, $(0+1)\otimes 0\neq 0\otimes 0+1\otimes 0$; representing the basis as column vectors, this is equivalent the obvious observation that
            \begin{equation*}
                \begin{bmatrix}
                    0\\
                    0\\
                    1\\
                    0\\
                \end{bmatrix}
                \neq
                \begin{bmatrix}
                    1\\
                    0\\
                    0\\
                    0\\
                \end{bmatrix}
                +
                \begin{bmatrix}
                    0\\
                    0\\
                    1\\
                    0\\
                \end{bmatrix}
            \end{equation*}
            \item But we want such relationships to hold true in our conceptual "tensor product space." Thus, we quotient it by the subspace spanning all elements of the form $(a+b)\otimes c-a\otimes c-b\otimes c$.
            \item By direct computation, this subspace is $\spn(0\otimes 0,0\otimes 1)$:
            \begin{align*}
                (0+0)\otimes 0-0\otimes 0-0\otimes 0 &= -0\otimes 0&
                    (0+0)\otimes 1-0\otimes 1-0\otimes 1 &= -0\otimes 1\\
                (0+1)\otimes 0-0\otimes 0-1\otimes 0 &= -0\otimes 0&
                    (0+1)\otimes 1-0\otimes 1-1\otimes 1 &= -0\otimes 1\\
                (1+1)\otimes 0-1\otimes 0-1\otimes 0 &= 0\otimes 0&
                    (1+1)\otimes 1-1\otimes 1-1\otimes 1 &= 0\otimes 1
            \end{align*}
            Note that once we've considered $(a+b)\otimes c$, we don't need to consider $(b+a)\otimes c$ because of the commutativity of addition in $V$. That is, it is axiomatic that $a+b=b+a$ for all $a,b\in V$. Additionally, in the last line above, we are using the facts that $1+1=2=0$ in $F_2$ and $a\otimes b+a\otimes b=2a\otimes b=0$ in any $F_2$-module to simplify the expressions.
            \item Similarly, the subspace corresponding to $a\otimes(b+c)-a\otimes b-a\otimes c$ is $\spn(0\otimes 0,1\otimes 0)$. Thus, altogether, we quotient out the subspace $X=\spn(0\otimes 0,0\otimes 1,1\otimes 0)$. This leaves us with a 1-dimensional $V\otimes V$, as expected for the tensor product of two one-dimensional vector spaces. It is interesting to note that the one vector we didn't quotient out ($1\otimes 1$) is analogous to $e_1\otimes e_1$ since $e_1\in V$ might as well be defined $e_1:=1$.
            \item Now let's see how well this quotienting worked. First off, a bit of notation: let $\pi:U\to V\otimes V$ be the projection $\pi:v\mapsto v+X$, and denote elements $\pi(v_1\otimes v_2)\in V\otimes V$ by $v_1\otimes_\pi v_2$ for now to differentiate them from elements of $U$.
            \item Let $(0+1)\otimes_\pi 0=(0+1)\otimes 0+X$ be an element of the quotient space $V\otimes V$. Certainly, the elements $0\otimes_\pi 0$ and $1\otimes_\pi 0$ are also elements of this quotient space. Moreover, we can fairly form the linear combination $(0+1)\otimes_\pi 0-0\otimes_\pi 0-1\otimes_\pi 0$. However, this element lies in the quotiented-out subspace $X$. Thus,
            \begin{equation*}
                (0+1)\otimes_\pi 0-0\otimes_\pi 0-1\otimes_\pi 0 = [(0+1)\otimes 0-0\otimes 0-1\otimes 0]+X
                = 0+X
                = 0
            \end{equation*}
            \item But
            \begin{equation*}
                (0+1)\otimes_\pi 0-0\otimes_\pi 0-1\otimes_\pi 0 = 0
                \quad\Longrightarrow\quad
                (0+1)\otimes_\pi 0 = 0\otimes_\pi 0+1\otimes_\pi 0
            \end{equation*}
            as desired.
            \item Note that this construction also gives us nice things like $0\otimes_\pi 0=0$, $0\otimes_\pi 1=0$, etc. which were not true in $U$! It should not be concluded, though, that all we need to quotient out of $U$ for any $V$ is $\spn(0\otimes 0,0\otimes v,v\otimes 0)$ for every $v\in V$; indeed, $V=\R$, for example, will contain 
        \end{itemize}
        \item If $V$ has basis $e_1,\dots,e_n$ and $W$ has basis $f_1,\dots,f_m$, then $e_i\otimes f_j$ is a basis of $V\otimes W$.
        \item Interesting fact 1: If $V,W$ are finite dimensional, $V^*\otimes W\cong\Hom(V,W)$.
        \item If we want to work with the tensor product in practice in \emph{rep theory}, the only thing we need to know is the basis of the tensor product space, which can tell us how any map $\rho(g)$ acts on both sides of a $v\otimes w\in V\otimes W$. From here, we recover the Kronecker product of matrices.
        \item So many things are explained by the concept of tensor products!
        \item A tensor in \emph{physics} is something with lots of indices that changes in some way.
        \begin{itemize}
            \item It does come from the math concept.
            \item We'll get a huge basis because we have a massive product like $V\otimes\cdots\otimes V\otimes V^*\otimes\cdots\otimes V^*$.
        \end{itemize}
    \end{itemize}
    \item \textbf{Free} (vector space): A vector space that has a basis consisting of linearly independent elements.
    \begin{itemize}
        \item Example: Think of $V=\C e_1\oplus\C e_2$ as a $\C$-module. A free version $F(V)$ of $V$ is infinite dimensional with every $v\in V$ a linearly independent basis vector. Elements of $F(V)$ are of the form $a_1v_1+\cdots+a_kv_k$ for $a_1,\dots,a_k\in\C$ and $v_1,\dots,v_k\in V$. If $u=v+w$ where $u,v,w\in V$ are all nonzero, then $u\neq v+w$ in $F(V)$ because they are all linearly independent basis vectors.
        \item Example: What we formally start with in the example above is $V\times V$, the free $F_2$-module not the Cartesian product vector space $V^2$.
        \item A terrific explanation of free vector spaces is available \href{https://math.stackexchange.com/questions/18315/free-vector-space-and-vector-space}{here}.
    \end{itemize}
    \item Last 2 useful notions: Wedge powers and symmetric powers.
    \begin{itemize}
        \item Again, it's much easier to think about the dual space.
    \end{itemize}
    \item Consider the space $V^{\otimes n}$ (dimension $(\dim V)^n$).
    \begin{itemize}
        \item $(V^{\otimes n})^*$ are \textbf{polylinear} maps $f:V^n\to F$.
        \begin{itemize}
            \item Note: By contrast, $(V^n)^*$ is the space of all \emph{linear} maps $f:V^n\to F$.
            \item This distinction is subtle but important. Note, for instance, that $\dim V^{\otimes n}\neq\dim V^n$ and likewise for the duals.
            \item The distinction comes out fully when considering that if, for example, $V=\R^3$, then $V^2\cong\R^6$ and any map in $(V^2)^*$ is determined by its action on $(e_1,0),(e_2,0),(e_3,0),(0,e_1),(0,e_2),(0,e_3)$. By contrast, any map in $(V^{\otimes 2})^*$ is determined by its action on $(e_1,e_1),(e_1,e_2),(e_1,e_3),(e_2,e_1)$, $(e_2,e_2),(e_2,e_3),(e_3,e_1),(e_3,e_2),(e_3,e_3)$.
            \item Important note: What $(V^{\otimes 2})^*$ does is consider these nine elements of $V^2$ as the basis of another space. This is what it truly means when we say "a bilinear map on $V^2$ is a linear map on $V^{\otimes 2}$."
            \item Takeaway: Polylinearity changes the basis upon which a function $f:V^n\to F$ fundamentally acts.
        \end{itemize}
        \item A polylinear map may be \textbf{symmetric}, \textbf{antisymmetric}, or\footnote{This is an exclusive "or."} neither.
        \item These maps form vector spaces and the dimension is actually pretty meaningful.
    \end{itemize}
    \item \textbf{Symmetric} (polylinear map): A polylinear map $f:V^n\to F$ that satisfies the following property. \emph{Constraint}
    \begin{equation*}
        f(v_{\sigma(1)},\dots,v_{\sigma(n)}) = f(v_1,\dots,v_n)
    \end{equation*}
    \item \textbf{Antisymmetric} (polylinear map): A polylinear map $f:V^n\to F$ that satisfies the following property. \emph{Constraint}
    \begin{equation*}
        f(v_{\sigma(1)},\dots,v_{\sigma(n)}) = (-1)^\sigma f(v_1,\dots,v_n)
    \end{equation*}
    \item Suppose you take $V=\C e_1\oplus\C e_2$\footnote{Note that this notation allows you to define a vector space \emph{and} its basis in one go! I.e., the alternative is saying "Let $V$ be a complex vector space with basis $e_1,e_2$."}.
    \begin{itemize}
        \item Consider a symmetric polylinear map $f:V\times V\times V\to\C$.
        \item To compute it, we'll need the action of $f$ on the basis of $V^3$. In particular, we'll need\dots
        \begin{equation*}
            f(x_1e_1+y_1e_2,x_2e_1+y_2e_2,x_3e_1+y_3e_2) = x_1x_2x_3f(e_1,e_1,e_1)+x_1x_2y_3f(e_1,e_1,e_2)+\cdots
        \end{equation*}
        \begin{itemize}
            \item Somewhere in there, you'll also have a $x_1y_2x_3f(e_1,e_2,e_1)$ term as well.
            \item However, because $f$ is symmetric, you know by symmetry that these "bases" are the same, so you don't count them as 2 towards the dimension but as 1.
            \item Thus, $\dim=4$ for symmetric maps.
        \end{itemize}
        \item What about antisymmetric maps?
        \item Suppose $g:V^3\to\C$ is an antisymmetric polylinear map.
        \begin{itemize}
            \item Consider $g(e_1,e_1,e_1)$. Suppose you apply $(12)$. Interchanging the first two indices (for instance) obviously won't do anything, so we'll get
            \begin{align*}
                g(e_1,e_1,e_1) &= (-1)^{(12)}g(e_1,e_1,e_1)\\
                g(e_1,e_1,e_1) &= -g(e_1,e_1,e_1)\\
                2g(e_1,e_1,e_1) &= 0\\
                g(e_1,e_1,e_1) &= 0
            \end{align*}
            \item But what about $g(e_1,e_1,e_2)$? We could apply $(23)$ and get $g(e_1,e_2,e_1)$, right? So it appears that we would just be shrinking two options into one. Technically, this is true, but what's more important is that applying $(12)$ again yields the same thing, meaning that $g(e_1,e_1,e_2)=g(e_1,e_2,e_1)=0$.
            \item And thus, since $V$ has dimension 2 but $g$ takes three vectors, any argument submitted to $g$ will always be linearly dependent. Thus, $g=0$ and, in fact, the space of antisymmetric maps on $V^3$ has dimension 0.
        \end{itemize}
        \item Note: It's not always a rule that $V^{\otimes m}\cong S^mV\oplus\Lambda^mV$.
    \end{itemize}
    \item Mathematically, there's a more natural object to work with than symmetric and antisymmetric maps.
    \begin{itemize}
        \item Wedge powers and symmetric powers!
        \item Given $V$ and $n\in\N$, we can construct $S^nV$ and $\Lambda^nV$. $(S^nV)^*$ is symmetric polylinear maps taking $n$ arguments from $V$. $(\Lambda^nV)^*$ is antisymmetric polylinear maps taking $n$ arguments from $V$.
    \end{itemize}
    \item How about a concrete way to see these? We can relate them to tensor powers.
    \begin{itemize}
        \item Take a tensor power $V^{\otimes n}$, then look at those tensors which are symmetric and antisymmetric under permutation.
        \item Example: Let $V$ be the same as before. Then $V^{\otimes 2}$ has $\dim=4$.
        \begin{itemize}
            \item Take as basis elements for $S^2V$ those that don't change when you change the coordinates.
            \item Take as basis elements for $\Lambda^2V$ those that flip sign when you change the coordinates.
            \item In this case, the basis of $V^{\otimes 2}$ is $e_1\otimes e_1,e_1\otimes e_2,e_2\otimes e_1,e_2\otimes e_2$. The basis of $S^2V$ will be $e_1\otimes e_1,e_1\otimes e_2+e_2\otimes e_1,e_2\otimes e_2$. The basis of $\Lambda^2V$ will be $e_1\otimes e_2-e_2\otimes e_1$. Notice that these bases are identical (up to scaling) with those in \textcite{bib:Serre} and those produced by applying the \href{https://en.wikipedia.org/wiki/Symmetrization}{\textbf{symmetrization}} and \href{https://en.wikipedia.org/wiki/Exterior_algebra#Alternating_tensor_algebra}{\textbf{alternation}} operators to the basis of $V^{\otimes 2}$.
        \end{itemize}
        \item $S^2V$ and $\Lambda^2V$ direct sum because the dimensions match and they don't intersect, so we're good to go!
        \item Everything we're doing is representations, so $g(v_1\otimes\cdots\otimes v_n)=gv_1\otimes\cdots\otimes gv_n$.
    \end{itemize}
    \item Relating this to something we've seen, but that's a little confusing.
    \begin{itemize}
        \item The product notation is suggestive for symmetric vectors; you can commute $e_1\cdot e_2\in S^2V$, for instance.
        \item This allows us to, for example, shrink $e_1\otimes e_1$ to $2e_1^2$\footnote{Why the 2 coefficient??}, but $e_1\otimes e_2+e_2\otimes e_1$ only to $e_1\cdot e_2$.
        \item Note that $e_1\wedge e_2=e_1\otimes e_2-e_2\otimes e_1$ by definition.
        \item Fact/exercise: Let $V$ be a vector space of dimension $n$. $V^*$ is the dual space, but it is also a function space. If $V=\R^k$, its a space of \emph{functions from the blackboard}.
        \begin{itemize}
            \item Note that $(\Lambda^kV)^*=\Lambda^kV^*$.
        \end{itemize}
        \item $S^nV^*$ is homogeneous polynomials of degree $n$.
        \item You can take higher degree polynomials and just keep pushing through.
        \begin{itemize}
            \item Ask about this??
        \end{itemize}
        \item Wedge powers now.
        \item By convention, $\Lambda^0V=F$ and $\Lambda^1V=V$. But then you get to $\Lambda^2V$ and $\Lambda^3V$. They grow but then shrink down as the power approaches $\dim V$.
        \item Truth: The dimension of wedge powers $\Lambda^iV$ is $\binom{k}{i}$ for $\dim V=k$. Figuring out why this is the case is another good exercise.
    \end{itemize}
    \item An interesting connection between wedge powers and the determinant.
    \begin{itemize}
        \item Let $V=\C e_1\oplus\cdots\oplus\C e_n$.
        \item Recall that $\Lambda^nV^*$ is the space of antisymmetric polylinear functions $V\times\cdots\times V\to F$ taking $n$ arguments from $V$, and it has a single basis vector $e^1\wedge\cdots\wedge e^n$.
        \item Let $v_1=\sum a_{i1}e_i$, $v_2=\sum a_{i2}e_i$, etc.
        \item Let $f\in\Lambda^nV^*$, so that $f$ is an alternating polylinear map that takes $n$ arguments.
        \item Since $f$ is polylinear, we have that
        \begin{equation*}
            f(v_1,\dots,v_n) = \sum_{i_1,\dots,i_n=1}^na_{i_11}\cdots a_{i_nn}f(e_{i_1},\dots,e_{i_n})
        \end{equation*}
        \item Because of antisymmetry, we need only look at elements where the indices are all different. Thus, the above equals
        \begin{equation*}
            \sum_{\sigma\in S_n}a_{\sigma(1)1}\cdots a_{\sigma(n)n}f(e_{\sigma(1)},\dots,e_{\sigma(n)})
        \end{equation*}
        \item Additionally, $f(e_{\sigma(1)},\dots,e_{\sigma(n)})=(-1)^\sigma f(e_1,\dots,e_n)$ for any $\sigma\in S_n$. Moreover, $f(e_1,\dots,e_n)\in\C$ by definition, so define a constant $\lambda:=f(e_1,\dots,e_n)$. Thus, the above equals
        \begin{equation*}
            \lambda\sum_{\sigma\in S_n}a_{\sigma(1)1}\cdots a_{\sigma(n)n}
        \end{equation*}
        \item But the term following the $\lambda$ is just the determinant of the $n\times n$ matrix $(a_{ij})$. Thus, all said,
        \begin{equation*}
            f(v_1,\dots,v_n) = \lambda\det(v_1\mid\cdots\mid v_n)
        \end{equation*}
        \item Implication: Wedge powers are something like the determinant.
        \begin{itemize}
            \item In particular, because $\Lambda^nV^*$ has only a single basis vector as mentioned above, $f=\lambda e^1\wedge\cdots\wedge e^n$. It follows that $e^1\wedge\cdots\wedge e^n=\det$.
        \end{itemize}
        \item Takeaway: Wedge powers are something interesting; there's a reason to study them.
    \end{itemize}
    \item The basis of the wedge powers consists of wedge monomials $e_{j_1}\wedge\cdots\wedge e_{j_i}$. Moreover, no need to have the same list twice, so choose some way of indexing them, e.g., increasing indexes.
    \begin{itemize}
        \item This is why we do \emph{increasing} bases! There's no particular reason, it's just an arbitrary way of making sure we don't do the same thing twice! We could just as well choose decreasing or any other means of guaranteeing that we don't have duplicates.
    \end{itemize}
    \item Now let's relate all of this exterior and symmetric product stuff back to representation theory.
    \begin{itemize}
        \item Let $V=\C e_1\oplus\cdots\oplus\C e_n$.
        \item Let $G\acts V$ via the homomorphism $G\to GL(V)\cong GL_n(\C)$.
        \item Focusing more on the \emph{matrix} aspect this time, note that under this homomorphism, $g\mapsto A_g$ subject to the homomorphism constraints.
        \item Consider the set $\{A_{g_1},\dots,A_{g_k}\}$ of all matrices in the image of the homomorphism. If we transpose all of them, will they still obey the homomorphism constraints?
        \begin{itemize}
            \item Nope!
            \item Indeed, if we do this, we'll get in trouble. More specifically, transposition is not a representation because $A_{g_1}^TA_{g_2}^T\neq A_{g_1g_2}^T=A_{g_2}^TA_{g_1}^T$.
        \end{itemize}
        \item It's the same story with inverses.
        \item \emph{However}, combining the two operations, we get
        \begin{equation*}
            (A_{g_1g_2}^T)^{-1} = (A_{g_1}^T)^{-1}(A_{g_2}^T)^{-1}
        \end{equation*}
        \begin{itemize}
            \item This is exactly when we take a representation and then go to the dual\footnote{Relation to MATH 20510 when we discussed dual matrices and pullbacks of matrices.}.
        \end{itemize}
        \item This will be on next week's homework!
        \item Takeaway: This is an application of $\Lambda^jV^*$ to representation theory, $j\neq k,n$.
    \end{itemize}
    \item Another relation: An application of $\Lambda^nV^*$ to representation theory.
    \begin{itemize}
        \item Suppose we have a representation $G\acts V$ that we want to flatten into $G\acts\C$. How can we turn a relation between a group of matrices into a relation between a group of numbers?
        \item Use the determinant!
        \item Indeed, we already know that
        \begin{align*}
            \det(A_e) &= 1&
            \det(A_{g_1g_2}) &= (\det A_{g_1})(\det A_{g_2})&
            \det(A_{g^{-1}}) &= \det(A_g)^{-1}
        \end{align*}
        \item In particular, we make formal the transition $G\to GL_j(\C)\to\C$ with the \textbf{top wedge power} $\Lambda^nV^*$.
    \end{itemize}
    \item A last note.
    \begin{itemize}
        \item Don't think that we're limited to top wedge powers.
        \item Recall that we can define tensor products of matrices via the Kronecker product. Well, we can prove that
        \begin{equation*}
            A_{g_1g_2}^{\otimes 2} = A_{g_1}^{\otimes 2}A_{g_2}^{\otimes 2}
        \end{equation*}
        and the like as well!
        \item Similarly, we can define $\Lambda^2$ of a matrix.
        \item We'll get into some weird Kronecker product stuff again, but we can sort through it.
    \end{itemize}
    \item Plan for Friday and next time.
    \begin{itemize}
        \item Prove the theorem that every representation is a sum of irreducible representations.
        \item He will use projectors.
        \item Then a horror story.
        \item Then associative algebra.
    \end{itemize}
\end{itemize}



\section{S Chapter 1: Generalities on Linear Representations}
\emph{From \textcite{bib:Serre}.}
\subsection*{Section 1.5: Tensor Product of Two Representations}
\begin{itemize}
    \item \textbf{Tensor product} (of $V_1,V_2$): The vector space $W$ that (a) is furnished with a map $V_1\times V_2\to W$ sending $(x_1,x_2)\mapsto x_1\cdot x_2$ and (b) satisfies the following two conditions.
    \begin{enumerate}[label={(\roman*)}]
        \item $x_1\cdot x_2$ is bilinear.
        \item If $(e_{i_1})$ is a basis of $V_1$ and $(e_{i_2})$ is a basis of $V_2$, the family of products $e_{i_1}\cdot e_{i_2}$ is a basis of $W$.
    \end{enumerate}
    \emph{Denoted by} $\bm{V_1\otimes V_2}$.
    \begin{itemize}
        \item It can be shown that such a space exists and is unique up to isomorphism (see proof \href{https://www-users.cse.umn.edu/~garrett/m/algebra/notes/27.pdf}{here}).
    \end{itemize}
    \item This definition allows us to say some things quite expediently. For example, (ii) implies that
    \begin{equation*}
        \dim(V_1\otimes V_2) = \dim(V_1)\cdot\dim(V_2)
    \end{equation*}
    \item \textbf{Tensor product} (of $\rho^1,\rho^2$): The representation $\rho:G\to GL(V_1\otimes V_2)$ defined as follows for all $s\in G$, $x_1\in V_1$, and $x_2\in V_2$, where $\rho^1:G\to GL(V_1)$ and $\rho^2:G\to GL(V_2)$ are representations. \emph{Given by}
    \begin{equation*}
        [\rho_s^1\otimes\rho_s^2](x_1\cdot x_2) = \rho_s^1(x_1)\cdot\rho_s^2(x_2)
    \end{equation*}
    \item A more formal write up of the matrix translation of this definition.
    \begin{itemize}
        \item Let $(e_{i_1})$ be a basis for $V_1$, and let $(e_{i_2})$ be a basis for $V_2$.
        \item Let $r_{i_1j_1}(s)$ be the matrix of $\rho_s^1$ with respect to this basis, and let $r_{i_2j_2}(s)$ be the matrix of $\rho_s^2$ with respect to this basis.
        \item It follows that
        \begin{align*}
            \rho_s^1(e_{j_1}) &= \sum_{i_1}r_{i_1j_1}(s)e_{i_1}&
            \rho_s^2(e_{j_2}) &= \sum_{i_2}r_{i_2j_2}(s)e_{i_2}
        \end{align*}
        \item Therefore,
        \begin{equation*}
            [\rho_s^1\otimes\rho_s^2](e_{j_1}\cdot e_{j_2}) = \sum_{i_1,i_2}r_{i_1j_1}(s)r_{i_2j_2}(s)e_{i_1}\cdot e_{i_2}
        \end{equation*}
        and
        \begin{equation*}
            \mathcal{M}(\rho_s^1\otimes\rho_s^2) = (r_{i_1j_1}(s)r_{i_2j_2}(s))
        \end{equation*}
    \end{itemize}
    \item Aside on quantum chemistry to come back to later; I can't quite connect the dots yet.
\end{itemize}


\subsection*{Section 1.6: Symmetric Square and Alternating Square}
\begin{itemize}
    \item Herein, we investigate the tensor product when $V_1=V_2=V$.
    \item Let $(e_i)$ be a basis of $V$.
    \item Define the automorphism $\theta:V\otimes V\to V\otimes V$ by
    \begin{equation*}
        \theta(e_i\cdot e_j) = e_j\cdot e_i
    \end{equation*}
    for all 2-indices $(i,j)$.
    \item Properties of $\theta$.
    \begin{itemize}
        \item Since $\theta$ is linear, it follows that
        \begin{equation*}
            \theta(x\cdot y) = y\cdot x
        \end{equation*}
        for all $x,y\in V$.
        \begin{itemize}
            \item Implication: $\theta$ is independent of the chosen basis $(e_i)$!
        \end{itemize}
        \item $\theta^2=1$, where 1 is the identity map on $V\otimes V$.
    \end{itemize}
    \item Assertion: $V\otimes V$ decomposes into
    \begin{equation*}
        V\otimes V = S^2(V)\oplus\Lambda^2(V)
    \end{equation*}
    \begin{itemize}
        \item Rudenko: We do not have to worry about proving this\dots yet, at least.
    \end{itemize}
    \item \textbf{Symmetric square representation}: The subspace of $V\otimes V$ containing all elements $z$ satisfying $\theta(z)=z$. \emph{Denoted by} $\bm{S^2V}$, $\bm{S^2(V)}$, $\pmb{\mathbb{S}}\bm{{}^2V}$, $\pmb{\textbf{Sym}}\,\bm{{}^2(V)}$.
    \begin{itemize}
        \item Basis: $(e_i\cdot e_j+e_j\cdot e_i)_{i\leq j}$.
        \begin{itemize}
            \item Rudenko: How do we know everything is linearly independent? Well, when we add two linearly independent vectors out of a set, the sum is still linearly independent from everything else!
            \item Example when $\dim V=2$: The basis of $V\otimes V$ is $e_1\otimes e_1,e_1\otimes e_2,e_2\otimes e_1,e_2\otimes e_2$, where all four of these vectors are linearly independent. So naturally, the basis of the corresponding symmetric square representation --- which is $2e_1\otimes e_1,e_1\otimes e_2+e_2\otimes e_1,2e_2\otimes e_2$ --- will still be a linearly independent list of vectors.
        \end{itemize}
        \item Dimension: If $\dim V=n$, then
        \begin{equation*}
            \dim S^2(V) = \frac{n(n+1)}{2}
        \end{equation*}
    \end{itemize}
    \item \textbf{Alternating square representation}: The subspace of $V\otimes V$ containing all elements $z$ satisfying $\theta(z)=-z$. \emph{Denoted by} $\bm{\Lambda^2V}$, $\bm{\Lambda^2(V)}$, $\pmb{\textbf{Alt}}\,\bm{{}^2(V)}$.
    \begin{itemize}
        \item Basis: $(e_i\cdot e_j-e_j\cdot e_i)_{i<j}$.
        \item Dimension: If $\dim V=n$, then
        \begin{equation*}
            \dim\Lambda^2(V) = \frac{n(n-1)}{2}
        \end{equation*}
    \end{itemize}
\end{itemize}




\end{document}